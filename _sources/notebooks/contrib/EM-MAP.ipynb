{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization Algorithm and MAP Estimation\n",
    "Prepared by: Ken Sible (ksible@nd.edu, [@kennethsible](https://github.com/kennethsible), 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N \"https://huggingface.co/datasets/papluca/language-identification/resolve/main/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: import nltk\n",
    "except ImportError:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this tutorial notebook, we introduce the **expectation maximization** (EM) algorithm for parameter estimation given partially observed data. We use a **Naive Bayes** classifier for **language identification** as a running example throughout the notebook. In order to train the classifier, we use **maximum a posteriori** (MAP) estimation, a statistical optimization method for estimating the parameters of a distribution given (fully or partially) observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "Let $\\{(x^{(i)},y^{(i)}):x^{(i)}\\in\\{+1,-1\\}^d,y^{(i)}\\in\\{1,2,\\ldots,k\\}\\}_{i=1}^n$ be a training set where $n$ is the number of **documents**, $k$ is the number of **labels** and $d$ is the number of **features**. In language identification, an example of text classification, the labels are languages and the features are tokens, which are meaningful sequences of characters. We define $x^{(i)}_j$ to be $+1$ if document $i$ contains token $j$ and $-1$ otherwise. Hence, each component of the binary vector $x^{(i)}$ represents either the *presence* or *absence* of a particular token in document $i$. Now, we derive the **Naive Bayes** model for classification.\n",
    "\n",
    "Let $Y$ and $X_1,X_2,\\ldots,X_d$ be random variables corresponding to the label $y$ and the vector components $x_1,x_2,\\ldots,x_d$, respectively. The Naive Bayes classifier is a **generative model**, so we need to estimate the parameters of the following joint probablity distribution:\n",
    "$$\\mathbb{P}(Y=y,X_1=x_1,X_2=x_2,\\ldots,X_d=x_d)$$\n",
    "for all labels $y$ paired with attribute values $x_1,x_2,\\ldots,x_d$. Suppose that $X_j$ is independent of $X_{j'}$ for all $j'\\neq j$, called the **Naive Bayes assumption**, which is equivalent to assuming that the positions of tokens are independent in the documents. Then, we observe that\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}(Y=y,X_1=x_1,\\ldots,X_d=x_d)&=\\mathbb{P}(Y=y)\\cdot\\mathbb{P}(X_1=x_1,\\ldots,X_d=x_d\\mid Y=y) \\\\\n",
    "    &=\\mathbb{P}(Y=y)\\cdot\\prod_{j=1}^d\\mathbb{P}(X_j=x_j\\mid X_1=x_1,\\ldots,X_{j-1}=x_{j-1},Y=y) \\\\\n",
    "    &=\\mathbb{P}(Y=y)\\cdot\\prod_{j=1}^d\\mathbb{P}(X_j=x_j\\mid Y=y) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where the final equality follows from the Naive Bayes assumption. The Naive Bayes classifier has two types of **parameters**: $q(y)$, called the **prior distribution**, and $q_j(x\\mid y)$, called the **likelihood distribution**. Here, $q(y)=\\mathbb{P}(Y=y)$ and $q_j(x\\mid y)=\\mathbb{P}(X_j=x\\mid Y=y)$. Then,\n",
    "$$\n",
    "p(y\\mid x_1,\\ldots,x_d)\\propto q(y)\\prod_{j=1}^dq_j(x_j\\mid y)\n",
    "$$\n",
    "where we define $p(y\\mid x_1,\\ldots,x_d)=\\mathbb{P}(Y=y\\mid X_1=x_1,\\ldots,X_d=x_d)$, called the **posterior distribution**. Note, the posterior distribution and the joint distribution $\\mathbb{P}(Y=y,X_1=x_1,\\ldots,X_d=x_d)$ only differs by a constant factor $\\mathbb{P}(X_1=x_1,\\ldots,X_d=x_d)$, called the **marginal distribution**.\n",
    "\n",
    "In order to estimate the parameters of the Naive Bayes classifier, we use **maximum a posteriori** (MAP) estimation, which is equivalent to **maximum likelihood estimation** (MLE) if the prior distribution is uniform.\n",
    "\n",
    "### Maximum A Posteriori Estimation\n",
    "Given a training set $\\{(x^{(i)},y^{(i)}):x^{(i)}\\in\\{+1,-1\\}^d,y^{(i)}\\in\\{1,2,\\ldots,k\\}\\}_{i=1}^n$, the **log-likelihood function** $L(\\theta)$ for the classifier is\n",
    "$$\n",
    "\\begin{align*}\n",
    "    L(\\theta)&=\\sum_{i=1}^n\\log p(x^{(i)},y^{(i)})\\\\\n",
    "    &=\\sum_{i=1}^n\\log \\left(q(y)\\prod_{j=1}^dq_j(x^{(i)}_j\\mid y^{(i)})\\right)\\\\\n",
    "    &=\\sum_{i=1}^n\\log q(y)+\\sum_{i=1}^n\\log\\left(\\prod_{j=1}^dq_j(x^{(i)}_j\\mid y^{(i)})\\right)\\\\\n",
    "    &=\\sum_{i=1}^n\\log q(y)+\\sum_{i=1}^n\\sum_{j=1}^d\\log q_j(x^{(i)}_j\\mid y^{(i)})\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\theta$ is a parameter vector consisting of $q(y)$ and $q_j(x\\mid y)$. The parameters of the classifier are then estimated by maximizing $L(\\theta)$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{max }& \\sum_{i=1}^n\\log q(y)+\\sum_{i=1}^n\\sum_{j=1}^d\\log q_j(x^{(i)}_j\\mid y^{(i)})\\\\\n",
    "    \\text{s.t. }& q(y)\\geq 0\\text{ for all }y\\text{ and }\\textstyle\\sum_{y}q(y)=1,\\\\\n",
    "    \\hphantom{\\text{s.t. }}& q_j(x\\mid y)\\geq 0\\text{ for all }j,x,y\\text{ and }\\textstyle\\sum_{x}q_j(x\\mid y)=1.\n",
    "\\end{align*}\n",
    "$$\n",
    "By using the method of Lagrange multipliers, one can derive the following global solution for the Naive Bayes classifier:\n",
    "$$\n",
    "q(y)=\\frac{\\sum_{i=1}^n[\\![y^{(i)}=y]\\!]}{n}=\\frac{\\text{count}(y)}{n}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "q_j(x\\mid y)=\\frac{\\sum_{i=1}^n[\\![y^{(i)}=y\\text{ and }x^{(i)}_j=x]\\!]}{\\sum_{i=1}^n[\\![y^{(i)}=y]\\!]}=\\frac{\\text{count}_j(x\\mid y)}{\\text{count}(y)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "class BernoulliNB:\n",
    "\n",
    "    def __init__(self, labels: set[str], tokens: set[str]):\n",
    "        self.labels = labels\n",
    "        self.tokens = tokens\n",
    "        self.prior = {}\n",
    "        self.likelihood = {}\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" Initializes model parameters from random numbers \"\"\"\n",
    "\n",
    "        # randomly sample numbers from a uniform distribution\n",
    "        distribution = [random.uniform(0.01, 0.99) for _ in self.labels]\n",
    "\n",
    "        # calculate normalizing constant for prior distribution\n",
    "        denominator = sum(distribution)\n",
    "\n",
    "        # assign normalized probabilities to prior distribution\n",
    "        for label, probability in zip(self.labels, distribution):\n",
    "            self.prior[label] = probability / denominator\n",
    "\n",
    "        for label in self.labels:\n",
    "            if label not in self.likelihood:\n",
    "                self.likelihood[label] = {}\n",
    "\n",
    "            # randomly sample numbers from a uniform distribution\n",
    "            distribution = [random.uniform(0.01, 0.99) for _ in self.tokens]\n",
    "\n",
    "            # assign normalized probabilities to likelihood distribution\n",
    "            for token, probability in zip(self.tokens, distribution):\n",
    "                self.likelihood[label][token] = probability\n",
    "\n",
    "    def train(self, data: list[tuple[str, str]]) -> None:\n",
    "        \"\"\" Trains a Bernoulli Naive Bayes classifier using MAP estimation\n",
    "        \n",
    "            Inputs:\n",
    "                data - list of (document, label) pairs\n",
    "        \"\"\"\n",
    "\n",
    "        # count the number of occurrences of labels and tokens\n",
    "        count, total = {}, Counter()\n",
    "        for document, label in data:\n",
    "            if label not in count:\n",
    "                count[label] = Counter()\n",
    "            for token in set(document):\n",
    "                count[label][token] += 1\n",
    "            total[label] += 1\n",
    "\n",
    "        # update the model parameters from the frequency estimates\n",
    "        for label in self.labels:\n",
    "            probability = total[label] / len(data)\n",
    "            self.prior[label] = probability\n",
    "\n",
    "            self.likelihood[label] = {}\n",
    "            for token in self.tokens:\n",
    "                # apply add-one Laplace smoothing for a Bernoulli distribution\n",
    "                probability = (count[label][token] + 1) / (total[label] + 2)\n",
    "                self.likelihood[label][token] = probability\n",
    "\n",
    "    def calculate_posterior(self, document: str) -> dict[str, float]:\n",
    "        \"\"\" Calculates a probability distribution over labels\n",
    "\n",
    "            Inputs:\n",
    "                document - string of text\n",
    "\n",
    "            Outputs:\n",
    "                posterior - probability distribution over labels\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the posterior distribution from the model parameters\n",
    "        document, posterior = set(document), {}\n",
    "        for label in self.labels:\n",
    "            posterior[label] = math.log(self.prior[label])\n",
    "\n",
    "            for token in self.tokens:\n",
    "                # switch to log space to avoid arithmetic underflow\n",
    "                if token in document:\n",
    "                    posterior[label] += math.log(self.likelihood[label][token])\n",
    "                else:\n",
    "                    posterior[label] += math.log(1 - self.likelihood[label][token])\n",
    "\n",
    "        return posterior\n",
    "\n",
    "    def classify(self, document: str) -> str:\n",
    "        \"\"\" Classifies a document using a Bernoulli Naive Bayes classifier\n",
    "\n",
    "            Inputs:\n",
    "                document - string of text\n",
    "\n",
    "            Outputs:\n",
    "                label - document label\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the posterior distribution over labels\n",
    "        posterior = self.calculate_posterior(document)\n",
    "\n",
    "        # return the label with the highest probability\n",
    "        return max(posterior, key=posterior.get) # argmax(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning (with Labeled Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# import the language identification data\n",
    "data, train_data, valid_data = {}, [], []\n",
    "with open('train.csv') as csv_file:\n",
    "    for i, sample in enumerate(csv.DictReader(csv_file)):\n",
    "        if sample['labels'] in ('en', 'de', 'fr', 'es'):\n",
    "            if sample['labels'] not in data:\n",
    "                data[sample['labels']] = []\n",
    "            document = nltk.word_tokenize(sample['text'])\n",
    "            data[sample['labels']].append(document)\n",
    "\n",
    "# split the data into training and validation\n",
    "train_data = [(x_i, y) for y, x in data.items() for x_i in x[:75]]\n",
    "valid_data = [(x_i, y) for y, x in data.items() for x_i in x[75:100]]\n",
    "\n",
    "# extract the labels and tokens from the data\n",
    "labels, tokens = set(), set()\n",
    "for document, label in train_data:\n",
    "    labels.add(label)\n",
    "    for token in document:\n",
    "        tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.0% Accuracy\n"
     ]
    }
   ],
   "source": [
    "# construct a Bernoulli Naive Bayes classifier\n",
    "model = BernoulliNB(labels, tokens)\n",
    "model.train(train_data)\n",
    "\n",
    "# calculate the accuracy using the validation data\n",
    "count, total = 0, 0\n",
    "for text, label in valid_data:\n",
    "    if model.classify(text) == label:\n",
    "        count += 1\n",
    "    total += 1\n",
    "print(f'{count / total * 100}% Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "In practice, we use subwords of length $n$, called $n$-grams, instead of entire words for language identification. In the Python implementation, change the features from entire words to bigrams ($n=2$) or trigrams ($n=3$), and compare both the accuracy and the number of parameters.\n",
    "\n",
    "Hint: In Cell [4], replace `nltk.word_tokenize(text)` with `list(nltk.bigrams(text))` or `list(nltk.trigrams(text))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "In the Python implementation, we used add-one **Laplace smoothing** (see below). Why do we require smoothing with maximum a posteriori estimation?\n",
    "\n",
    "$$\n",
    "q_j(x\\mid y)=\\frac{\\text{count}_j(x\\mid y)+\\alpha}{\\text{count}(y)+\\alpha d}\n",
    "$$\n",
    "where $\\alpha$ is a hyperparameter and $d$ is the dimension of the multinomial distribution ($2$ for Bernoulli and $|V|$ for categorical where $V$ is the vocabulary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "We implemented a **Bernoulli** Naive Bayes classifier where each likelihood $p(x_j\\mid y)$ is a Bernoulli distribution and each $x_j$ is binary variable representing the presence or absence of feature $j$. However, $p(x_j\\mid y)$ can be any distribution, but Bernoulli (discrete), Multinomial (discrete) and Gaussian (continuous) are the most common. In a **multinomial** Naive Bayes classifier, each likelihood $p(x_j\\mid y)$ is categorical distribution and each $x_j$ is the number of times feature $j$ is observed. In the Python implementation, change the features $\\{x_j\\}_{j=1}^k$ from binary variables to frequency counts and update the likelihoods.\n",
    "\n",
    "Hint: The likelihoods $p(x_j\\mid y)$ are estimated by calculating the fraction of documents for Bernoulli and the fraction of tokens for multinomial (see below).\n",
    "#### Bernoulli\n",
    "$$\n",
    "p(y\\mid x_1,\\ldots,x_d)\\propto q(y)\\prod_{j=1}^d[q_j(x_j\\mid y)]^{x_j}\\cdot[1-q_j(x_j\\mid y)]^{(1-x_j)}\n",
    "$$\n",
    "where $x_j\\in\\{+1,-1\\}$ for all $j$.\n",
    "#### Multinomial\n",
    "$$\n",
    "p(y\\mid x_1,\\ldots,x_d)\\propto q(y)\\prod_{j=1}^d[q_j(x_j\\mid y)]^{x_j}\n",
    "$$\n",
    "where $x_j\\in\\mathbb{N}$ for all $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "\n",
    "    def __init__(self, labels: set[str], tokens: set[str]):\n",
    "        self.labels = labels\n",
    "        self.tokens = tokens\n",
    "        self.prior = {}\n",
    "        self.likelihood = {}\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" Initializes model parameters from random numbers \"\"\"\n",
    "\n",
    "        # randomly sample numbers from a uniform distribution\n",
    "        distribution = [random.uniform(0.01, 0.99) for _ in self.labels]\n",
    "\n",
    "        # calculate normalizing constant for prior distribution\n",
    "        denominator = sum(distribution)\n",
    "\n",
    "        # assign normalized probabilities to prior distribution\n",
    "        for label, probability in zip(self.labels, distribution):\n",
    "            self.prior[label] = probability / denominator\n",
    "\n",
    "        for label in self.labels:\n",
    "            if label not in self.likelihood:\n",
    "                self.likelihood[label] = {}\n",
    "\n",
    "            # randomly sample numbers from a uniform distribution\n",
    "            distribution = [random.uniform(0.01, 0.99) for _ in self.tokens]\n",
    "\n",
    "            # calculate normalizing constant for likelihood distribution\n",
    "            denominator = sum(distribution)\n",
    "\n",
    "            # assign normalized probabilities to likelihood distribution\n",
    "            for token, probability in zip(self.tokens, distribution):\n",
    "                self.likelihood[label][token] = probability / denominator\n",
    "\n",
    "    def train(self, data: list[tuple[str, str]]) -> None:\n",
    "        \"\"\" Trains a multinomial Naive Bayes classifier using MAP estimation\n",
    "        \n",
    "            Inputs:\n",
    "                data - list of (document, label) pairs\n",
    "        \"\"\"\n",
    "\n",
    "        # count the number of occurrences of labels and tokens\n",
    "        count, total = {}, Counter()\n",
    "        for document, label in data:\n",
    "            if label not in count:\n",
    "                count[label] = Counter()\n",
    "            # Add your solution here\n",
    "\n",
    "        # update the model parameters from the frequency estimates\n",
    "        for label in self.labels:\n",
    "            probability = total[label] / len(data)\n",
    "            self.prior[label] = probability\n",
    "\n",
    "            self.likelihood[label] = {}\n",
    "            for token in self.tokens:\n",
    "                # Add your solution here\n",
    "                self.likelihood[label][token] = probability\n",
    "\n",
    "    def calculate_posterior(self, document: str) -> dict[str, float]:\n",
    "        \"\"\" Calculates a probability distribution over labels\n",
    "\n",
    "            Inputs:\n",
    "                document - string of text\n",
    "\n",
    "            Outputs:\n",
    "                posterior - probability distribution over labels\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the posterior distribution from the model parameters\n",
    "        posterior = {}\n",
    "        for label in self.likelihood:\n",
    "            posterior[label] = math.log(self.prior[label])\n",
    "\n",
    "            # Add your solution here\n",
    "\n",
    "        return posterior\n",
    "\n",
    "    def classify(self, document: str) -> str:\n",
    "        \"\"\" Classifies a document using a Bernoulli Naive Bayes classifier\n",
    "\n",
    "            Inputs:\n",
    "                document - string of text\n",
    "\n",
    "            Outputs:\n",
    "                label - document label\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the posterior distribution over labels\n",
    "        posterior = self.calculate_posterior(document)\n",
    "\n",
    "        # return the label with the highest probability\n",
    "        return max(posterior, key=posterior.get) # argmax(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% Accuracy\n"
     ]
    }
   ],
   "source": [
    "# construct a multinomial Naive Bayes classifier\n",
    "model = MultinomialNB(labels, tokens)\n",
    "model.train(train_data)\n",
    "\n",
    "# calculate the accuracy using the validation data\n",
    "count, total = 0, 0\n",
    "for text, label in valid_data:\n",
    "    if model.classify(text) == label:\n",
    "        count += 1\n",
    "    total += 1\n",
    "print(f'{count / total * 100}% Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization Algorithm\n",
    "In statistics, a random variable that is never observed is called a **latent variable**. The **expectation maximization** (EM) algorithm is an iterative method for performing maximum a posteriori (or maximum likelihood) estimation for a latent variable model. First, the initial parameters $\\theta^0$ are chosen at random. Then, new parameters $\\theta^t$ are calculated from the previous posterior distribution $p(y\\mid x^{(i)};\\theta^{t-1})$ at each iteration. The EM algorithm is guaranteed to converge, but only to a local solution. Note, both the initial parameters and the posteriors must be normalized. The parameters for Naive Bayes are\n",
    "$$\n",
    "q^t(y)=\\frac{1}{n}\\sum_{i=1}^n\\delta(y\\mid i)\\quad\\text{ and }\\quad q^t_j(x\\mid y)=\\frac{\\sum_{i:x^{(i)}_j=x}\\delta(y\\mid i)}{\\sum_{i=1}^n\\delta(y\\mid i)}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\delta(y\\mid i)=p(y\\mid x^{(i)};\\theta^{t-1})=\\frac{q^{t-1}(y)\\prod_{j=1}^dq^{t-1}_j(x^{(i)}_j\\mid y)}{\\sum_{y=1}^kq^{t-1}(y)\\prod_{j=1}^dq^{t-1}_j(x^{(i)}_j\\mid y)}.\n",
    "$$\n",
    "Note, these parameter formulas are equivalent to those for fully observed data if we define $\\delta(y\\mid i)=1$ if $y_i=y$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "```\n",
    "Inputs:\n",
    "    data - training examples\n",
    "    n - number of documents\n",
    "    k - number of labels\n",
    "    T - number of iterations\n",
    "\n",
    "Initialization: \n",
    "    Set model parameters (prior and likelihood) to random values\n",
    "\n",
    "Algorithm:\n",
    "    for t = 1:T\n",
    "        for i = 1:n, y = 1:k\n",
    "            calculate posterior distribution from model parameters\n",
    "        update model parameters using normalized posterior distribution\n",
    "Output:\n",
    "    model parameters - prior and likelihood\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(data: list[float]) -> float:\n",
    "    \"\"\" Calculates the log of the sum of exponentials of the input data\n",
    "\n",
    "        Inputs:\n",
    "            data - list of numbers\n",
    "\n",
    "        Outputs:\n",
    "            the log of the sum of exponentials of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate the maximum value of the input data\n",
    "    c = max(data)\n",
    "\n",
    "    # shift the exponential arguments to avoid underflow\n",
    "    return c + math.log(sum(math.exp(x - c) for x in data))\n",
    "\n",
    "def expectation_maximization(model: BernoulliNB, data: list[str], k: int, T: int) -> None:\n",
    "    \"\"\" Expectation Maximization Algorithm for Bernoulli Naive Bayes \"\"\"\n",
    "\n",
    "    for t in range(T):\n",
    "        # estimate posterior distribution\n",
    "        posterior = {}\n",
    "        for i, document in enumerate(data):\n",
    "            posterior[i] = model.calculate_posterior(document)\n",
    "\n",
    "        # normalize posterior distribution\n",
    "        for i in range(len(data)):\n",
    "            denominator = logsumexp([posterior[i][label] for label in model.labels])\n",
    "            for label in model.labels:\n",
    "                posterior[i][label] = math.exp(posterior[i][label] - denominator)\n",
    "\n",
    "        # update parameters (prior and likelihood)\n",
    "        print(f'[{t}] {[model.prior[label] for label in model.labels]}')\n",
    "        for label in model.labels:\n",
    "            marginal = sum(posterior[i][label] for i in range(len(data)))\n",
    "            model.prior[label] = marginal / len(data)\n",
    "            for token in model.tokens:\n",
    "                probability = sum(posterior[i][label] * document.count(token) for i, document in enumerate(data))\n",
    "                model.likelihood[label][token] = (probability + 1) / (marginal + 2)\n",
    "\n",
    "    print(f'[{T}] {[model.prior[label] for label in model.labels]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning (with Unlabeled Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [0.05035805143403056, 0.9496419485659694]\n",
      "[1] [0.5684630939482381, 0.4315369060517618]\n",
      "[2] [0.5966452242425817, 0.40335477575741824]\n",
      "[3] [0.598941474959691, 0.4010585250403089]\n",
      "[4] [0.5991012977476698, 0.40089870225233026]\n",
      "[5] [0.5991127955638306, 0.40088720443616943]\n",
      "[6] [0.5991136736773947, 0.4008863263226054]\n",
      "[7] [0.5991137447735392, 0.4008862552264606]\n",
      "[8] [0.599113750820036, 0.4008862491799639]\n",
      "[9] [0.5991137513539058, 0.40088624864609423]\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = [['A', 'B'], ['C', 'D'], ['C', 'D'], ['A', 'B'], ['A', 'B']]\n",
    "\n",
    "# extract the labels and tokens from the data\n",
    "labels, tokens = set([1, 2]), set()\n",
    "for document in unlabeled_data:\n",
    "    for token in document:\n",
    "        tokens.add(token)\n",
    "\n",
    "# construct a Bernoulli Naive Bayes classifier\n",
    "model = BernoulliNB(labels, tokens)\n",
    "model.initialize()\n",
    "\n",
    "# apply the EM algorithm to the unlabled data\n",
    "expectation_maximization(model, unlabeled_data, 2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label(['A', 'B']) = 1\n",
      "{1: -2.787002126164065, 2: -5.308464141745352}\n"
     ]
    }
   ],
   "source": [
    "print(f\"label({['A', 'B']}) =\", model.classify(['A', 'B']))\n",
    "print(model.calculate_posterior(['A', 'B', 'C', 'C', 'C']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we cannot apply the EM algorithm for unsupervised language identification (see McCallum et al.). Instead, we verified that the EM algorithm converges by providing training data with a known conditional distribution and comparing that against the output distribution from EM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial notebook, we covered the Naive Bayes classifier, maximum a posteriori estimation, and the expectation maximization algorithm. The Naive Bayes classifier is a statistical model for classification based on Bayes' theorem and the Naive Bayes independence assumption. MAP (or MLE) estimation is a statistical technique for estimating the parameters of a posterior (or likelihood) distribution given fully or partially observed data. The EM algorithm is an interative method for calculating new parameters or MAP (or MLE) estimates given unobserved data in a latent model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Deisenroth, M. P., Ong, C. S., &amp; Faisa, A. A. (n.d.). *Mathematics for Machine Learning*. https://mml-book.github.io/book/mml-book.pdf\n",
    "- Collins, M. (n.d.). *The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm*. http://web2.cs.columbia.edu/~mcollins/em.pdf\n",
    "- Jurafsky, D., &amp; Martin, J. H. (n.d.). Speech and Language Processing. https://web.stanford.edu/~jurafsky/slp3/\n",
    "- Papariello, L. (n.d.). *Language Identification Dataset*. https://huggingface.co/datasets/papluca/language-identification\n",
    "- McCallum A., &amp; Nigam K. (n.d.). *A Comparison of Event Models for Naive Bayes Text Classification*. http://www.cs.cmu.edu/~dgovinda/pdf/multinomial-aaaiws98.pdf\n",
    "- Nigam K., McCallum A., &amp; Mitchell T. (n.d.). *Semi-Supervised Text Classification using EM*. https://www.cs.cmu.edu/~tom/pubs/NigamEtAl-bookChapter.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
