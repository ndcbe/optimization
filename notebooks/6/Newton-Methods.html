
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6.4. Newton-type Methods for Unconstrained Optimization &#8212; Optimization for Decision Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/6/Newton-Methods';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6.5. Quasi-Newton Methods for Unconstrained Optimization" href="Quasi-Newton-Methods.html" />
    <link rel="prev" title="6.3. Unconstrained Optimality Conditions" href="Optimality.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/cbe_logo.jpg" class="logo__image only-light" alt="Optimization for Decision Science - Home"/>
    <script>document.write(`<img src="../../_static/cbe_logo.jpg" class="logo__image only-dark" alt="Optimization for Decision Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Optimization for Decision Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Organization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/intro.html">Welcome</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../org/syllabus.html">Syllabus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/calendar.html">Fall 2024 Calendar</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/contribute.html">Contribution Instructions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/workshop.html">Computational Optimization in Python (SÃ£o Paulo, Brazil)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/assignments.html">Assignments</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo1.html">Pyomo Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo2.html">Pyomo Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo3.html">Pyomo Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/project1.html">Project 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms1.html">Algorithms Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms2.html">Algorithms Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/project2.html">Project 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms3.html">Algorithms Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms4.html">Algorithms Homework 4</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/archive.html">Archive</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo-Mini-Project.html">Pyomo Mini-Project: Receding Horizon Stochastic Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/semester_project.html">Semester Project (Spring 2023)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization Modeling in Pyomo</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../1/getting-started.html">1. Getting Started with Pyomo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../1/Local-Install.html">1.1. Local Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Optimization-Modeling.html">1.2. Optimization Modeling with Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Pyomo-Introduction.html">1.3. Your First Optimization Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/LP.html">1.4. Continuous Optimization: Linear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/NLP.html">1.5. Continuous Optimization: Nonlinear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/IP.html">1.6. Integer Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Pyomo-Nuts-and-Bolts.html">1.7. 60 Minutes to Pyomo: An Energy Storage Model Predictive Control Example</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../2/logic.html">2. Logical Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../2/Logical_Modeling_GDP.html">2.1. Logical Modeling and Generalized Disjunctive Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2/Modeling_Disjunctions_Strip_Packing.html">2.2. Modeling Disjunctions through the Strip Packing Problem</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../3/dynamics.html">3. Dynamic Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_car.html">3.1. Pyomo.DAE Example: Race Car</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_TCLab.html">3.2. Pyomo.DAE Example: Temperature Control Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/DAE_background.html">3.3. Differential Algebraic Equations (DAEs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/DAE_numeric_integration.html">3.4. Numeric Integration for DAEs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_theory.html">3.5. Dynamic Optimization with Collocation and Pyomo.DAE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_example.html">3.6. Pyomo.DAE: Racing Example Revisited</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../4/uncertainty.html">4. Optimization Under Uncertainty</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../4/SP.html">4.1. Stochastic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/blocks.html">4.2. Blocks and Other Pyomo Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/AdvancedTopics.html">4.3. Advanced Topics in Stochastic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/RiskMeasures.html">4.4. Risk Measures and Portfolio Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5/data.html">5. Data Science and Applied Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5/Parmest-tutorial.html">5.1. Parameter estimation with <code class="docutils literal notranslate"><span class="pre">parmest</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../5/Parmest-generate-data.html">5.2. Supplementary material: data for parmest tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5/Pyomo_DoE_Tutorial.html">5.3. Optimizing Experiments with <code class="docutils literal notranslate"><span class="pre">Pyomo.DoE</span></code></a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Algorithms and Theory</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="unconstrained.html">6. Unconstrained Nonlinear Optimization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Math-Primer-1.html">6.1. Linear Algebra Review and SciPy Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="Math-Primer-2.html">6.2. Mathematics Primer</a></li>
<li class="toctree-l2"><a class="reference internal" href="Optimality.html">6.3. Unconstrained Optimality Conditions</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">6.4. Newton-type Methods for Unconstrained Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="Quasi-Newton-Methods.html">6.5. Quasi-Newton Methods for Unconstrained Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="Globalization.html">6.6. Descent and Globalization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../7/constrained.html">7. Constrained Nonlinear Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../7/Convexity.html">7.1. Convexity Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Local-Optimality.html">7.2. Local Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/KKT-Multipliers.html">7.3. Analysis of KKT Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Constraint-Qualifications.html">7.4. Constraint Qualifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Second-Order.html">7.5. Second Order Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/degeneracy_hunter.html">7.6. NLP Diagnostics with Degeneracy Hunter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Interior-Point1.html">7.7. Simple Netwon Method for Equality Constrained NLPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Interior-Point2.html">7.8. Inertia-Corrected Netwon Method for Equality Constrained NLPs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../8/special-topics.html">8. Special Topics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../8/MILP.html">8.1. Integer Programming with Simple Branch and Bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../8/MINLP-Algorithms.html">8.2. MINLP Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../8/Global-Opt.html">8.3. Deterministic Global Optimization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Contributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../contrib/pyomo.html">More Pyomo Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../contrib/semiconductor_manufacturing.html">Semiconductor Production Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/student_diet.html">Optimization of Daily Diet Using Pyomo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/blending.html">Blending Under Uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/vehicle_routing.html">Vehicle Routing</a></li>

<li class="toctree-l2"><a class="reference internal" href="../contrib/portfolio_optimization_extended.html">Risk Measures and Portfolio Optimization: Expanded</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/race_car_extended.html">Extended Race Car Optimization Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/hot_air_balloon.html">Hot Air Balloon Dynamic Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/reactor_design.html">Chemical Reactor Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/Disaster_Response_Plan.html">Disaster Response Plan Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/Sudoku_Solver.html">Sudoku Solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/more_circle_packing.html">Circle Packing Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../contrib/modeling.html">Modeling Paradigms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../contrib/multi_objective.html">Multi-Objective Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/advanced_stochastic_programming.html">Advanced Topics in Stochastic Programming</a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../contrib/algorithms.html">Global Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../contrib/Deterministic_Global_Optimization.html">Deterministic Global Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/Bayesian_Optimization1.html">Bayesian Optimization Tutorial 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/Bayesian_Optimization2.html">Bayesian Optimization Tutorial 2</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../contrib/sgd.html">Stochastic Gradient Descent</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../contrib/Stochastic-Gradient-Descent-1.html">Stochastic Gradient Descent Tutorial 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contrib/Stochastic-Gradient-Descent-2.html">Stochastic Gradient Descent Tutorial 2</a></li>






<li class="toctree-l2"><a class="reference internal" href="../contrib/Stochastic-Gradient-Descent-3.html">Stochastic Gradient Descent Tutorial 3</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../contrib/data.html">Machine Learning and Applied Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../contrib/EM-MAP.html">Expectation Maximization Algorithm and MAP Estimation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ndcbe/optimization/blob/master/notebooks/6/Newton-Methods.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ndcbe/optimization" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ndcbe/optimization/issues/new?title=Issue%20on%20page%20%2Fnotebooks/6/Newton-Methods.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/6/Newton-Methods.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Newton-type Methods for Unconstrained Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-method-derivation-and-properties">6.4.1. Newton Method Derivation and Properties</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-system-of-equations">6.4.1.1. Nonlinear System of Equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-optimization">6.4.1.2. Unconstrained Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-issues">6.4.1.3. Important Issues:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-2-1-basic-newton-method">6.4.2. Algorithm 2.1: Basic Newton Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-function">6.4.2.1. Helper Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-1-python-implementation">6.4.2.2. Algorithm 1 Python Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-convergence-proof">6.4.2.3. Quadratic Convergence Proof</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-problem">6.4.3. Test Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#starting-point-near-optimal-solution">6.4.3.1. Starting Point Near Optimal Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-a-different-starting-point">6.4.3.2. Activity: A Different Starting Point</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-let-s-break-it">6.4.3.3. Activity: Letâs break it</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-use-i-in-place-of-hessian">6.4.3.4. Activity: Use <span class="math notranslate nohighlight">\(I\)</span> in place of Hessian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-hessian-with-levenberg-marquardt-correction">6.4.3.5. Adjust Hessian with Levenberg-Marquardt Correction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enhancement-ideas">6.4.4. Enhancement Ideas</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="newton-type-methods-for-unconstrained-optimization">
<h1><span class="section-number">6.4. </span>Newton-type Methods for Unconstrained Optimization<a class="headerlink" href="#newton-type-methods-for-unconstrained-optimization" title="Link to this heading">#</a></h1>
<p><strong>Reference</strong>: Section 2.4.2 in Biegler (2010)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="k">if</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
    <span class="o">!</span>wget<span class="w"> </span><span class="s2">&quot;https://raw.githubusercontent.com/ndcbe/optimization/main/notebooks/helper.py&quot;</span>
    <span class="c1"># We do not need to install Pyomo or Ipopt, but we do want to format our plots</span>
    <span class="c1"># helper.easy_install()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;../&#39;</span><span class="p">)</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">helper</span>
<span class="n">helper</span><span class="o">.</span><span class="n">set_plotting_style</span><span class="p">()</span>

<span class="c1"># Load required Python libraries.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">linalg</span>
</pre></div>
</div>
</div>
</div>
<section id="newton-method-derivation-and-properties">
<h2><span class="section-number">6.4.1. </span>Newton Method Derivation and Properties<a class="headerlink" href="#newton-method-derivation-and-properties" title="Link to this heading">#</a></h2>
<section id="nonlinear-system-of-equations">
<h3><span class="section-number">6.4.1.1. </span>Nonlinear System of Equations<a class="headerlink" href="#nonlinear-system-of-equations" title="Link to this heading">#</a></h3>
<p>Consider the system of nonlinear equations <span class="math notranslate nohighlight">\(f(x): \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span></p>
<div class="math notranslate nohighlight">
\[
f(x) = 0
\]</div>
<p><em>How to solve numerically?</em>
Step 1. Write the Taylor series expansion:</p>
<div class="math notranslate nohighlight">
\[
f(x + p) = f(x) + \nabla f(x)^T p + \mathcal{O}(\|p\|^2)
\]</div>
<p>Step 2. Truncate, set <span class="math notranslate nohighlight">\(f(x + p) = 0\)</span>, and solve for <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[
- f(x) = \nabla f(x)^T p
\]</div>
<div class="math notranslate nohighlight">
\[
p = - (\nabla f(x)^T)^{-1} f(x)
\]</div>
</section>
<section id="unconstrained-optimization">
<h3><span class="section-number">6.4.1.2. </span>Unconstrained Optimization<a class="headerlink" href="#unconstrained-optimization" title="Link to this heading">#</a></h3>
<p>Now, letâs apply Newtonâs method to find a stationary point <span class="math notranslate nohighlight">\(x^*\)</span> where <span class="math notranslate nohighlight">\(\nabla f(x^*) = 0\)</span></p>
<p>Step 1. Start with the Taylor series expansion:</p>
<div class="math notranslate nohighlight">
\[
f(x + p) = f(x) + \nabla f(x)^T p + \frac{1}{2} p^T \nabla^2 f(x)^T p + \mathcal{O}(\|p\|^3)
\]</div>
<p>Step 2. Differentiate with respect to <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p><em>Question:</em> Why <span class="math notranslate nohighlight">\(p\)</span>?</p>
<p><em>Answer:</em> Consider <span class="math notranslate nohighlight">\(x\)</span> as fixed data (i.e., initial points). We want to vary <span class="math notranslate nohighlight">\(p\)</span>, the next step, such that <span class="math notranslate nohighlight">\(\nabla f(x + p) = 0\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\nabla f(x + p) = 0 + \nabla f(x) + \nabla^2 f(x) p + \mathcal{O}(\|p\|^2)
\]</div>
<p>Step 3. Set <span class="math notranslate nohighlight">\(\nabla f(x + p) = 0\)</span> and solve for <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[
0 = \nabla f(x) + \nabla^2 f(x) p \implies p = - (\nabla^2 f(x))^{-1} \nabla f(x)
\]</div>
<p><em>Important Note:</em> When we numerically implement Newtonâs method, we will not explicitly compute <span class="math notranslate nohighlight">\((\nabla^2 f(x))^{-1}\)</span>. Instead, we will solve the <strong>linear system</strong>,</p>
<div class="math notranslate nohighlight">
\[
 \underbrace{\nabla^2 f(x)}_{\mathbf{A}} \underbrace{p}_\mathbf{x} = \underbrace{- \nabla f(x)}_{\mathbf{b}}
\]</div>
<p>to compute <span class="math notranslate nohighlight">\(p\)</span>.</p>
</section>
<section id="important-issues">
<h3><span class="section-number">6.4.1.3. </span>Important Issues:<a class="headerlink" href="#important-issues" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Near a strict local minimizer and with small <span class="math notranslate nohighlight">\(\| p \|\)</span>, truncation error <span class="math notranslate nohighlight">\(\mathcal{O}(\| p \|^3)\)</span> is negligible, and <span class="math notranslate nohighlight">\(f(x)\)</span> behaves like a quadratic function. <em>Weâll revisit convergence rates soon.</em></p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> needs to be nonsingular to compute the Newton step. If <span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> is singular, some corrections can be applied.</p></li>
<li><p>Recall that in the neighborhood <span class="math notranslate nohighlight">\(\mathcal{N}(x^*)\)</span> of a strict local minimizer, <span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> is positive definite. Thus, it is desirable for an approximation to <span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> to also be positive definite (P.D.). <strong>Weâll revisit this later today and in the next lecture.</strong></p></li>
<li><p>To promote convergence, it is important that <span class="math notranslate nohighlight">\(p\)</span> is a descent step, i.e.,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\nabla f(x)^T p &lt; 0
\]</div>
<p>and thus</p>
<div class="math notranslate nohighlight">
\[
f(x^k) &gt; f(x^{k+1})
\]</div>
<p>If <span class="math notranslate nohighlight">\(x^k \in \mathcal{N}(x^*)\)</span> and <span class="math notranslate nohighlight">\(x^{k+1} = x^k + \alpha p\)</span>, where <span class="math notranslate nohighlight">\(\alpha \in (0, 1]\)</span>, then a descent step requires:</p>
<div class="math notranslate nohighlight">
\[
0 &gt; f(x^k + \alpha p) - f(x^k) = \alpha \nabla f(x^k)^T p + \frac{\alpha^2}{2} p^T \nabla^2 f(x + t \alpha p) p
\]</div>
<p>for some <span class="math notranslate nohighlight">\(t \in (0, 1)\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(x^*\)</span> is a strict local minimizer and <span class="math notranslate nohighlight">\(\mathcal{N}(x^*)\)</span> is sufficiently small, then <span class="math notranslate nohighlight">\(\nabla^2 f(x + t p)\)</span> is positive definite, i.e.,</p>
<div class="math notranslate nohighlight">
\[
p^T \nabla^2 f(x + t p) p &gt; 0
\]</div>
<p>This implies <span class="math notranslate nohighlight">\(\nabla f(x)^T p &lt; 0\)</span>.</p>
<p><em>In a few lectures, we will extend this into a line search strategy to update <span class="math notranslate nohighlight">\(\alpha\)</span> and handle cases when <span class="math notranslate nohighlight">\(x^k\)</span> is far from <span class="math notranslate nohighlight">\(x^*\)</span>.</em></p>
</section>
</section>
<section id="algorithm-2-1-basic-newton-method">
<h2><span class="section-number">6.4.2. </span>Algorithm 2.1: Basic Newton Method<a class="headerlink" href="#algorithm-2-1-basic-newton-method" title="Link to this heading">#</a></h2>
<p>Choose a starting point <span class="math notranslate nohighlight">\(x^0\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(k \geq 0\)</span> while <span class="math notranslate nohighlight">\(\| p^k \| &gt; \epsilon_1\)</span> and <span class="math notranslate nohighlight">\(\| \nabla f(x^k) \| &gt; \epsilon_2\)</span>:</p>
<ol class="arabic simple">
<li><p>At <span class="math notranslate nohighlight">\(x^k\)</span>, evaluate <span class="math notranslate nohighlight">\(\nabla f(x^k)\)</span> and <span class="math notranslate nohighlight">\(\nabla^2 f(x^k)\)</span>. If <span class="math notranslate nohighlight">\(\nabla^2 f(x^k)\)</span> is singular, <strong>STOP</strong>.</p></li>
<li><p>Solve the linear system <span class="math notranslate nohighlight">\(\nabla^2 f(x^k) p^k = -\nabla f(x^k)\)</span>.</p></li>
<li><p>Set <span class="math notranslate nohighlight">\(x^{k+1} = x^k + p^k\)</span> and <span class="math notranslate nohighlight">\(k = k + 1\)</span>.</p></li>
</ol>
<p>This basic algorithm has the desirable property of fast convergence, which can be quantified by the following well-known property.</p>
<p>(This algorithm is taken directly from Biegler, 2010)</p>
<section id="helper-function">
<h3><span class="section-number">6.4.2.1. </span>Helper Function<a class="headerlink" href="#helper-function" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Calculate gradient with central finite difference</span>
<span class="k">def</span><span class="w"> </span><span class="nf">my_grad_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Calculate gradient of function my_f using central difference formula</span>
<span class="sd">    </span>
<span class="sd">    Inputs:</span>
<span class="sd">        x - point for which to evaluate gradient</span>
<span class="sd">        f - function to consider</span>
<span class="sd">        eps1 - perturbation size</span>
<span class="sd">        </span>
<span class="sd">    Outputs:</span>
<span class="sd">        grad - gradient (vector)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    
    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;***** my_grad_approx at x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="s2">&quot;*****&quot;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
        
        <span class="c1"># Create vector of zeros except eps in position i</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">e</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps1</span>
        
        <span class="c1"># Finite difference formula</span>
        <span class="n">my_f_plus</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">my_f_minus</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">e</span><span class="p">)</span>
        
        <span class="c1"># Diagnostics</span>
        <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;e[&quot;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s2">&quot;] = &quot;</span><span class="p">,</span><span class="n">e</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x + e[&quot;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s2">&quot;]) = &quot;</span><span class="p">,</span><span class="n">my_f_plus</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x - e[&quot;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s2">&quot;]) = &quot;</span><span class="p">,</span><span class="n">my_f_minus</span><span class="p">)</span>
        
        
        <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">my_f_plus</span> <span class="o">-</span> <span class="n">my_f_minus</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps1</span><span class="p">)</span>
    
    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;***** Done. ***** </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">grad</span>

<span class="c1">## Calculate gradient using central finite difference and my_hes_approx</span>
<span class="k">def</span><span class="w"> </span><span class="nf">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">grad</span><span class="p">,</span><span class="n">eps2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Calculate gradient of function my_f using central difference formula and my_grad</span>
<span class="sd">    </span>
<span class="sd">    Inputs:</span>
<span class="sd">        x - point for which to evaluate gradient</span>
<span class="sd">        grad - function to calculate the gradient</span>
<span class="sd">        eps2 - perturbation size (for Hessian NOT gradient approximation)</span>
<span class="sd">        </span>
<span class="sd">    Outputs:</span>
<span class="sd">        H - Hessian (matrix)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># Create vector of zeros except eps in position i</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">e</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps2</span>
        
        <span class="c1"># Evaluate gradient twice</span>
        <span class="n">grad_plus</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">grad_minus</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">e</span><span class="p">)</span>
        
        <span class="c1"># Notice we are building the Hessian by column (or row)</span>
        <span class="n">H</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_plus</span> <span class="o">-</span> <span class="n">grad_minus</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">H</span>

<span class="k">def</span><span class="w"> </span><span class="nf">check_nan</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>

<span class="c1">## Analyze Hessian.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">analyze_hes</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">check_nan</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Calculate eigenvalues</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvalues: &quot;</span><span class="p">,</span><span class="n">l</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="algorithm-1-python-implementation">
<h3><span class="section-number">6.4.2.2. </span>Algorithm 1 Python Implementation<a class="headerlink" href="#algorithm-1-python-implementation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Implement Alg 1 in a function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">alg1</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">calc_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">calc_hes</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">eps2</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">250</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Arguments:</span>
<span class="sd">        x0 - starting point</span>
<span class="sd">        calc_f - funcation that calculates f(x)</span>
<span class="sd">        calc_grad - function that calculates gradient(x)</span>
<span class="sd">        calc_hes - function that calculates hessian(x)</span>
<span class="sd">    </span>
<span class="sd">    Outputs:</span>
<span class="sd">        x - iteration history of x (decision variables)</span>
<span class="sd">        f - iteration history of f(x) (objective value)</span>
<span class="sd">        p - iteration history of p (steps)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="c1"># Allocate outputs as empty lists</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Store starting point</span>
    <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">flag</span> <span class="o">=</span> <span class="kc">True</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iter. </span><span class="se">\t</span><span class="s2">f(x) </span><span class="se">\t\t</span><span class="s2">||grad(x)|| </span><span class="se">\t</span><span class="s2">||p|| </span><span class="se">\t\t</span><span class="s2">min(lambda)&quot;</span><span class="p">)</span>
    
    <span class="k">while</span> <span class="n">flag</span><span class="p">:</span>
        <span class="c1"># Evaluate f(x) at current iteration</span>
        <span class="n">f</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        
        <span class="c1"># Evaluate gradient</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">calc_grad</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">check_nan</span><span class="p">(</span><span class="n">grad</span><span class="p">)):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: gradiant calculation returned NaN&quot;</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="c1"># Evaluate Hessian</span>
        <span class="n">hes</span> <span class="o">=</span> <span class="n">calc_hes</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">check_nan</span><span class="p">(</span><span class="n">hes</span><span class="p">)):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: Hessian calculation returned NaN&quot;</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;k = &quot;</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;grad = &quot;</span><span class="p">,</span><span class="n">grad</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;hes = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">hes</span><span class="p">)</span>
        
        <span class="c1"># Check if singular via condition number</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">hes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="mf">1E12</span><span class="p">:</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: Hessian is near singular.&quot;</span><span class="p">)</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Calculate step</span>
            
            <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">hes</span><span class="p">,</span><span class="o">-</span><span class="n">grad</span><span class="p">))</span>
            
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p = &quot;</span><span class="p">,</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            
            <span class="c1"># Take step. x[k+1] = x[k] + p[k]</span>
            <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            
            <span class="c1"># Calculate norms</span>
            <span class="n">norm_p</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="n">norm_g</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
            
            <span class="c1"># Calculate eigenvalues (for display only)</span>
            <span class="n">ev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">hes</span><span class="p">))</span>
            
            <span class="c1"># print(&quot;k = &quot;,k,&quot;\t&quot;f[k],&quot;\t&quot;,norm_g,&quot;\t&quot;,norm_p)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="s1">&#39;  </span><span class="se">\t</span><span class="si">{0: 1.4e}</span><span class="s1"> </span><span class="se">\t</span><span class="si">{1:1.4e}</span><span class="s1"> </span><span class="se">\t</span><span class="si">{2:1.4e}</span><span class="s1"> </span><span class="se">\t</span><span class="si">{3: 1.4e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f</span><span class="p">[</span><span class="n">k</span><span class="p">],</span><span class="n">norm_g</span><span class="p">,</span><span class="n">norm_p</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">ev</span><span class="p">)))</span>
            
            <span class="c1"># Check convergence criteria</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">norm_p</span> <span class="o">&gt;</span> <span class="n">eps1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">norm_g</span> <span class="o">&gt;</span> <span class="n">eps2</span><span class="p">)</span>
            
            <span class="c1"># Update iteration counter</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="n">max_iter</span><span class="p">:</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum number of iterations.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x* = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="quadratic-convergence-proof">
<h3><span class="section-number">6.4.2.3. </span>Quadratic Convergence Proof<a class="headerlink" href="#quadratic-convergence-proof" title="Link to this heading">#</a></h3>
<p><strong>Theorem 2.20</strong> (Biegler, 2010)</p>
<p>Assume that <span class="math notranslate nohighlight">\(f(x)\)</span> is twice differentiable and <span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> is Lipschitz continuous in a neighborhood of the solution <span class="math notranslate nohighlight">\(x^*\)</span>, which satisfies the sufficient second-order conditions. Then, by applying Algorithm 2.1 and with a sufficiently close <span class="math notranslate nohighlight">\(x_0\)</span>, there exists a constant <span class="math notranslate nohighlight">\(L\)</span> such that:</p>
<ol class="arabic simple">
<li><p>The convergence rate for <span class="math notranslate nohighlight">\(\{x^k\}\)</span> is quadratic, i.e., <span class="math notranslate nohighlight">\(\| x^{k+1} - x^* \| \leq \hat{L} \| x^k - x^* \|^2\)</span></p></li>
<li><p>The convergence rate for <span class="math notranslate nohighlight">\(\{\nabla f(x^k)\}\)</span> is also quadratic</p></li>
</ol>
<p><strong>Claim 1 Proof:</strong> Convergence rate for <span class="math notranslate nohighlight">\(\{x^k\}\)</span> is quadratic</p>
<p>Step 1: What does the continuity of the second derivative and sufficient second-order conditions tell us?</p>
<p><span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> is positive definite <span class="math notranslate nohighlight">\(\forall x \in \mathcal{N}(x^*)\)</span> which implies</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> is nonsingular</p></li>
<li><p><span class="math notranslate nohighlight">\(\| \left(\nabla^2 f(x)\right)^{-1} \| \leq C\)</span>, i.e., the norm of the inverse of the Hessian is bounded</p></li>
</ul>
<p>Definition of Lipschitz continuity: <span class="math notranslate nohighlight">\(\| \nabla^2 f(x^*) - \nabla^2 f(x) \| \leq L \| x - x^* \|\)</span></p>
<p>Step 2: Consider Newtonâs step <span class="math notranslate nohighlight">\(p = - (\nabla^2 f(x))^{-1} \nabla f(x)\)</span>. Add <span class="math notranslate nohighlight">\(x^k - x^*\)</span> to both sides.</p>
<div class="math notranslate nohighlight">
\[
p^{k} + (x^{k} - x^*) = - (\nabla^2 f(x))^{-1} \nabla f(x) + (x^{k} - x^*)
\]</div>
<p>Step 3: Work through algebra on the RHS to obtain:</p>
<div class="math notranslate nohighlight">
\[
\underbrace{p^{k} + x^{k}}_{x^{k+1}} - x^* = (\nabla^2 f(x^k))^{-1} \left( \nabla^2 f(x^k) (x^k - x^*) - \left( \nabla f(x^k) - \nabla f(x^*) \right) \right)
\]</div>
<p>Step 4: Recall <span class="math notranslate nohighlight">\(\nabla f(x + p) - \nabla f(x) = \int_{0}^{1} \nabla^2 f(x + tp)dt\)</span>, and apply to RHS.</p>
<div class="math notranslate nohighlight">
\[
x^{k+1} - x^* = - (\nabla^2 f(x^k))^{-1} \left( \nabla^2 f(x^*) - \int_0^1 \nabla^2 f(x^* + t(x^k - x^*)) dt \right) (x^k - x^*) 
\]</div>
<p>Next, move the constant <span class="math notranslate nohighlight">\(\nabla^2 f(x^*)\)</span> inside the integral.</p>
<div class="math notranslate nohighlight">
\[
x^{k+1} - x^* = - (\nabla^2 f(x^k))^{-1} \int_0^1 \left( \nabla^2 f(x^*) - \nabla^2 f(x^* + t(x^k - x^*)) dt \right) (x^k - x^*) 
\]</div>
<p>Step 5: Take norms of both sides. Recall the Cauchy-Schwarz inequality <span class="math notranslate nohighlight">\(\langle x, y \rangle \leq \|x\| \|y\|\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\| x^{k+1} - x^* \| \leq \| \nabla^2 f(x^*) \| \cdot \| \int_0^1 \left( \nabla^2 f(x^*) - \nabla^2 f(x^* + t(x^k - x^*)) dt \right) \| \cdot \| x^k - x^* \|
\]</div>
<p>Step 6: Invoke continuity properties (see Step 1).</p>
<div class="math notranslate nohighlight">
\[
\| \int_0^1 \left( \nabla^2 f(x^*) - \nabla^2 f(x^* + t(x^k - x^*)) dt \right) \| \leq L \| x^k - x^* \|
\]</div>
<p>Thus, we conclude that:</p>
<div class="math notranslate nohighlight">
\[
\| x^{k+1} - x^* \| \leq C \cdot L \| x^k - x^* \| \cdot \| x^k - x^* \| = \hat{L} \| x^k - x^* \|^2
\]</div>
<p><strong>Claim 2 Proof:</strong> Convergence rate for <span class="math notranslate nohighlight">\(\{\nabla f(x^k)\}\)</span> is quadratic</p>
</section>
</section>
<section id="test-problem">
<h2><span class="section-number">6.4.3. </span>Test Problem<a class="headerlink" href="#test-problem" title="Link to this heading">#</a></h2>
<p><strong>Example 2.19</strong> Consider the following two-variable unconstrained optimization problem:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ae02900f-8eb4-42a1-b65c-63e0f1d68ff9">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-ae02900f-8eb4-42a1-b65c-63e0f1d68ff9" title="Permalink to this equation">#</a></span>\[\begin{align}
\min f(x) &amp;= \alpha \exp(-\beta) \\
u &amp;= x_1 - 0.8, \\
v &amp;= x_2 - (a_1 + a_2u^2(1-u)^{\frac{1}{2}} + a_3u), \\
\alpha &amp;= -b_1 + b_2u^2(1+u)^{\frac{1}{2}} + b_3u, \\
\beta &amp;= c_1 v^2(1 - c_2v) / (1 + c_3u^2) 
\end{align}\]</div>
<p>with</p>
<div class="amsmath math notranslate nohighlight" id="equation-926a2311-5b05-4f57-aeac-d559e8dba778">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-926a2311-5b05-4f57-aeac-d559e8dba778" title="Permalink to this equation">#</a></span>\[\begin{align}
a^T &amp;= [0.3, 0.6, 0.2], \nonumber \\
b^T &amp;= [5, 26, 3], \nonumber \\
c^T &amp;= [40, 1, 10]. \nonumber
\end{align}\]</div>
<p>The solution to this problem is given by <span class="math notranslate nohighlight">\(x^* = [0.7395, 0.3144]\)</span> with<span class="math notranslate nohighlight">\(f(x^*) = -5.0893\)</span>.</p>
<p>At this solution, <span class="math notranslate nohighlight">\(\nabla f(x^*) = 0\)</span> and the Hessian is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-8a2ba611-bdb6-461d-b5f1-7cc3481f6a4a">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-8a2ba611-bdb6-461d-b5f1-7cc3481f6a4a" title="Permalink to this equation">#</a></span>\[\begin{align}
\nabla^2 f(x^*) &amp;= \begin{bmatrix} 77.012 &amp; 108.334 \\ 108.334 &amp; 392.767 \end{bmatrix},
\end{align}\]</div>
<p>which has eigenvalues <span class="math notranslate nohighlight">\(\lambda = 43.417\)</span> and <span class="math notranslate nohighlight">\(\lambda = 426.362\)</span>.</p>
<p>(The above text is from Bielger, 2010.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Define Python function to calculate objective</span>
<span class="k">def</span><span class="w"> </span><span class="nf">my_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; Evaluate function given above at point x</span>

<span class="sd">    Inputs:</span>
<span class="sd">        x - vector with 2 elements</span>
<span class="sd">        </span>
<span class="sd">    Outputs:</span>
<span class="sd">        f - function value (scalar)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Constants</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
    
    <span class="c1"># Intermediates. Recall Python indicies start at 0</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mf">0.8</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">u</span><span class="p">)</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">u</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">s</span><span class="o">-</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="o">-</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">s2</span><span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">u</span> <span class="c1"># September 5, 2018: changed &#39;s&#39; to &#39;s2&#39;</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##### my_f at x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;#####&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;u = &quot;</span><span class="p">,</span><span class="n">u</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sqrt(1-u) = &quot;</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sqrt(1+u) = &quot;</span><span class="p">,</span><span class="n">s2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;v = &quot;</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;alpha = &quot;</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;beta = &quot;</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x) = &quot;</span><span class="p">,)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;##### Done. #####</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="starting-point-near-optimal-solution">
<h3><span class="section-number">6.4.3.1. </span>Starting Point Near Optimal Solution<a class="headerlink" href="#starting-point-near-optimal-solution" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Test on example</span>

<span class="c1"># Specify convergence criteria</span>
<span class="n">eps1</span> <span class="o">=</span> <span class="mf">1E-8</span>
<span class="n">eps2</span> <span class="o">=</span> <span class="mf">1E-4</span>

<span class="c1"># Create a Lambda (anonymous) function for gradient calculation</span>
<span class="n">calc_grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">my_grad_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">)</span>

<span class="c1"># Create a Lambda (anonymous) function for Hessian calculation</span>
<span class="n">calc_hes</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">)</span>

<span class="c1"># Specify starting point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>

<span class="c1"># Call optimization routine</span>
<span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">alg1</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">calc_hes</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">eps2</span><span class="p">);</span>

<span class="c1"># Actual Hessian</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hessian at x*= &quot;</span><span class="p">)</span>
<span class="n">analyze_hes</span><span class="p">(</span><span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iter. 	f(x) 		||grad(x)|| 	||p|| 		min(lambda)
0   	-4.9246e+00 	1.0874e+01 	4.1946e-02 	 4.9368e+01
1   	-5.0888e+00 	6.2736e-01 	1.9752e-03 	 4.2541e+01
2   	-5.0893e+00 	2.4210e-03 	3.0032e-05 	 4.3424e+01
3   	-5.0893e+00 	2.9199e-07 	3.1620e-09 	 4.3418e+01
Done.
x* =  [0.73950546 0.3143601 ]
Hessian at x*= 
[[ 77.01173033 108.33423048]
 [108.33423048 392.76693009]] 

Eigenvalues:  [ 43.41702924+0.j 426.36163117+0.j] 
</pre></div>
</div>
</div>
</div>
</section>
<section id="activity-a-different-starting-point">
<h3><span class="section-number">6.4.3.2. </span>Activity: A Different Starting Point<a class="headerlink" href="#activity-a-different-starting-point" title="Link to this heading">#</a></h3>
<p>Try with <span class="math notranslate nohighlight">\(x_0 = [0,0]^T\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify starting point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Call optimization routine</span>
<span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">alg1</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">calc_hes</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">eps2</span><span class="p">);</span>

<span class="c1"># Actual Hessian</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hessian at x* = &quot;</span><span class="p">)</span>
<span class="n">analyze_hes</span><span class="p">(</span><span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iter. 	f(x) 		||grad(x)|| 	||p|| 		min(lambda)
0   	 1.6212e-06 	1.5568e-04 	3.8590e-02 	-2.9496e-03
1   	 5.4102e-07 	5.5405e-05 	3.6202e-02 	-1.0900e-03
Done.
x* =  [-0.00124597 -0.07478209]
Hessian at x* = 
[[0.0001531  0.00055527]
 [0.00055527 0.00014766]] 

Eigenvalues:  [ 0.00070565+0.j -0.00040489+0.j] 
</pre></div>
</div>
</div>
</div>
</section>
<section id="activity-let-s-break-it">
<h3><span class="section-number">6.4.3.3. </span>Activity: Letâs break it<a class="headerlink" href="#activity-let-s-break-it" title="Link to this heading">#</a></h3>
<p><strong>Activity</strong>: Try <span class="math notranslate nohighlight">\(x_0 = [-0.2, -0.2]^T\)</span>. Why does the gradient or Hessian return NaN? <em>Hint</em>: redefine <code class="docutils literal notranslate"><span class="pre">calc_grad</span></code> and create <code class="docutils literal notranslate"><span class="pre">my_f_verbose</span></code> using <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify starting point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">])</span>

<span class="c1"># Create a Lambda (anonymous) function for f(x) calculation with verbose output</span>
<span class="n">my_f_verbose</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">my_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>

<span class="c1"># Create a Lambda (anonymous) function for gradient calculation</span>
<span class="n">calc_grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">my_grad_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">my_f_verbose</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>

<span class="c1"># Call optimization routine</span>
<span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">alg1</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">calc_hes</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">eps2</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>

<span class="c1"># Actual Hessian</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hessian at x* = &quot;</span><span class="p">)</span>
<span class="n">analyze_hes</span><span class="p">(</span><span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iter. 	f(x) 		||grad(x)|| 	||p|| 		min(lambda)
***** my_grad_approx at x =  [-0.2 -0.2] *****
##### my_f at x =  [-0.199999 -0.2     ] #####
u =  -0.9999990000000001
sqrt(1-u) =  1.4142132088196604
sqrt(1+u) =  0.0009999999999588667
v =  -1.5485260282367943
alpha =  -7.973997052001044
beta =  22.222565160998283
f(x) = 
##### Done. #####

##### my_f at x =  [-0.200001 -0.2     ] #####
u =  -1.0000010000000001
sqrt(1-u) =  1.4142139159264415
sqrt(1+u) =  nan
v =  -1.5485302466134128
alpha =  nan
beta =  22.222642208927322
f(x) = 
##### Done. #####

e[ 0 ] =  [1.e-06 0.e+00]
f(x + e[ 0 ]) =  -1.780486346234917e-09
f(x - e[ 0 ]) =  nan
##### my_f at x =  [-0.2      -0.199999] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.548527137423857
alpha =  -8.0
beta =  22.222566263573963
f(x) = 
##### Done. #####

##### my_f at x =  [-0.2      -0.200001] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.548529137423857
alpha =  -8.0
beta =  22.22264110629725
f(x) = 
##### Done. #####

e[ 1 ] =  [0.e+00 1.e-06]
f(x + e[ 1 ]) =  -1.786290485440547e-09
f(x - e[ 1 ]) =  -1.7861567995988046e-09
***** Done. ***** 

WARNING: gradiant calculation returned NaN
Done.
x* =  [-0.2 -0.2]
Hessian at x* = 
***** my_grad_approx at x =  [-0.199999 -0.2     ] *****
##### my_f at x =  [-0.199998 -0.2     ] #####
u =  -0.999998
sqrt(1-u) =  1.414212855266137
sqrt(1+u) =  0.0014142135623541761
v =  -1.5485239190522238
alpha =  -7.963224594456855
beta =  22.22252663717692
f(x) = 
##### Done. #####

##### my_f at x =  [-0.2 -0.2] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.5485281374238569
alpha =  -8.0
beta =  22.22260368491507
f(x) = 
##### Done. #####

e[ 0 ] =  [1.e-06 0.e+00]
f(x + e[ 0 ]) =  -1.778149501075135e-09
f(x - e[ 0 ]) =  -1.7862236413056787e-09
##### my_f at x =  [-0.199999 -0.199999] #####
u =  -0.9999990000000001
sqrt(1-u) =  1.4142132088196604
sqrt(1+u) =  0.0009999999999588667
v =  -1.5485250282367944
alpha =  -7.973997052001044
beta =  22.222527739675733
f(x) = 
##### Done. #####

##### my_f at x =  [-0.199999 -0.200001] #####
u =  -0.9999990000000001
sqrt(1-u) =  1.4142132088196604
sqrt(1+u) =  0.0009999999999588667
v =  -1.5485270282367944
alpha =  -7.973997052001044
beta =  22.222602582361898
f(x) = 
##### Done. #####

e[ 1 ] =  [0.e+00 1.e-06]
f(x + e[ 1 ]) =  -1.7805529756354463e-09
f(x - e[ 1 ]) =  -1.7804197192545872e-09
***** Done. ***** 

***** my_grad_approx at x =  [-0.200001 -0.2     ] *****
##### my_f at x =  [-0.2 -0.2] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.5485281374238569
alpha =  -8.0
beta =  22.22260368491507
f(x) = 
##### Done. #####

##### my_f at x =  [-0.200002 -0.2     ] #####
u =  -1.000002
sqrt(1-u) =  1.4142142694796995
sqrt(1+u) =  nan
v =  -1.5485323558054604
alpha =  nan
beta =  22.222680733035
f(x) = 
##### Done. #####

e[ 0 ] =  [1.e-06 0.e+00]
f(x + e[ 0 ]) =  -1.7862236413056787e-09
f(x - e[ 0 ]) =  nan
##### my_f at x =  [-0.200001 -0.199999] #####
u =  -1.0000010000000001
sqrt(1-u) =  1.4142139159264415
sqrt(1+u) =  nan
v =  -1.5485292466134128
alpha =  nan
beta =  22.22260478756765
f(x) = 
##### Done. #####

##### my_f at x =  [-0.200001 -0.200001] #####
u =  -1.0000010000000001
sqrt(1-u) =  1.4142139159264415
sqrt(1+u) =  nan
v =  -1.548531246613413
alpha =  nan
beta =  22.222679630328063
f(x) = 
##### Done. #####

e[ 1 ] =  [0.e+00 1.e-06]
f(x + e[ 1 ]) =  nan
f(x - e[ 1 ]) =  nan
***** Done. ***** 

***** my_grad_approx at x =  [-0.2      -0.199999] *****
##### my_f at x =  [-0.199999 -0.199999] #####
u =  -0.9999990000000001
sqrt(1-u) =  1.4142132088196604
sqrt(1+u) =  0.0009999999999588667
v =  -1.5485250282367944
alpha =  -7.973997052001044
beta =  22.222527739675733
f(x) = 
##### Done. #####

##### my_f at x =  [-0.200001 -0.199999] #####
u =  -1.0000010000000001
sqrt(1-u) =  1.4142139159264415
sqrt(1+u) =  nan
v =  -1.5485292466134128
alpha =  nan
beta =  22.22260478756765
f(x) = 
##### Done. #####

e[ 0 ] =  [1.e-06 0.e+00]
f(x + e[ 0 ]) =  -1.7805529756354463e-09
f(x - e[ 0 ]) =  nan
##### my_f at x =  [-0.2      -0.199998] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.548526137423857
alpha =  -8.0
beta =  22.222528842273906
f(x) = 
##### Done. #####

##### my_f at x =  [-0.2 -0.2] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.5485281374238569
alpha =  -8.0
beta =  22.22260368491507
f(x) = 
##### Done. #####

e[ 1 ] =  [0.e+00 1.e-06]
f(x + e[ 1 ]) =  -1.7863573320035263e-09
f(x - e[ 1 ]) =  -1.7862236413056787e-09
***** Done. ***** 

***** my_grad_approx at x =  [-0.2      -0.200001] *****
##### my_f at x =  [-0.199999 -0.200001] #####
u =  -0.9999990000000001
sqrt(1-u) =  1.4142132088196604
sqrt(1+u) =  0.0009999999999588667
v =  -1.5485270282367944
alpha =  -7.973997052001044
beta =  22.222602582361898
f(x) = 
##### Done. #####

##### my_f at x =  [-0.200001 -0.200001] #####
u =  -1.0000010000000001
sqrt(1-u) =  1.4142139159264415
sqrt(1+u) =  nan
v =  -1.548531246613413
alpha =  nan
beta =  22.222679630328063
f(x) = 
##### Done. #####

e[ 0 ] =  [1.e-06 0.e+00]
f(x + e[ 0 ]) =  -1.7804197192545872e-09
f(x - e[ 0 ]) =  nan
##### my_f at x =  [-0.2 -0.2] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.5485281374238569
alpha =  -8.0
beta =  22.22260368491507
f(x) = 
##### Done. #####

##### my_f at x =  [-0.2      -0.200002] #####
u =  -1.0
sqrt(1-u) =  1.4142135623730951
sqrt(1+u) =  0.0
v =  -1.548530137423857
alpha =  -8.0
beta =  22.222678527720475
f(x) = 
##### Done. #####

e[ 1 ] =  [0.e+00 1.e-06]
f(x + e[ 1 ]) =  -1.7862236413056787e-09
f(x - e[ 1 ]) =  -1.7860899603198772e-09
***** Done. ***** 

[[            nan             nan]
 [            nan -2.42801152e-06]] 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/3w/vr4xmyqs451dg23xk88pqcg00000gq/T/ipykernel_81619/729280440.py:19: RuntimeWarning: invalid value encountered in sqrt
  s2 = np.sqrt(1+u)
</pre></div>
</div>
</div>
</div>
</section>
<section id="activity-use-i-in-place-of-hessian">
<h3><span class="section-number">6.4.3.4. </span>Activity: Use <span class="math notranslate nohighlight">\(I\)</span> in place of Hessian<a class="headerlink" href="#activity-use-i-in-place-of-hessian" title="Link to this heading">#</a></h3>
<p>Create an alternative function that returns the identity matrix for the Hessian.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">eye_hes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">50</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Test with starting point <span class="math notranslate nohighlight">\(x=[0.7,0.3]^T\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify starting point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>

<span class="c1"># Create a Lambda (anonymous) function for gradient calculation</span>
<span class="n">calc_grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">my_grad_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>

<span class="c1"># Call optimization routine</span>
<span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">alg1</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">eye_hes</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">eps2</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>

<span class="c1"># Actual Hessian</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hessian at x* = &quot;</span><span class="p">)</span>
<span class="n">analyze_hes</span><span class="p">(</span><span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iter. 	f(x) 		||grad(x)|| 	||p|| 		min(lambda)
0   	-4.9246e+00 	1.0874e+01 	2.1749e-01 	 5.0000e+01
1   	-1.4795e+00 	1.6883e+01 	3.3765e-01 	 5.0000e+01
2   	-1.8403e+00 	2.6363e+01 	5.2725e-01 	 5.0000e+01
3   	-1.2461e-01 	1.6246e+00 	3.2493e-02 	 5.0000e+01
4   	-1.9169e-01 	2.5753e+00 	5.1507e-02 	 5.0000e+01
5   	-3.8498e-01 	5.2112e+00 	1.0422e-01 	 5.0000e+01
6   	-1.4594e+00 	1.6836e+01 	3.3673e-01 	 5.0000e+01
7   	-1.9249e+00 	2.6467e+01 	5.2934e-01 	 5.0000e+01
8   	-1.1531e-01 	1.5097e+00 	3.0195e-02 	 5.0000e+01
9   	-1.7247e-01 	2.3351e+00 	4.6703e-02 	 5.0000e+01
10   	-3.2703e-01 	4.5030e+00 	9.0059e-02 	 5.0000e+01
11   	-1.0837e+00 	1.3485e+01 	2.6971e-01 	 5.0000e+01
12   	-4.4497e+00 	2.0946e+01 	4.1891e-01 	 5.0000e+01
13   	-1.6236e-01 	2.1013e+00 	4.2026e-02 	 5.0000e+01
14   	-2.8114e-01 	3.6906e+00 	7.3812e-02 	 5.0000e+01
15   	-7.2912e-01 	9.1154e+00 	1.8231e-01 	 5.0000e+01
16   	-4.1506e+00 	2.1547e+01 	4.3094e-01 	 5.0000e+01
17   	-5.4382e-03 	2.2424e-01 	4.4848e-03 	 5.0000e+01
18   	-6.5332e-03 	2.6508e-01 	5.3017e-03 	 5.0000e+01
19   	-8.0846e-03 	3.2179e-01 	6.4357e-03 	 5.0000e+01
20   	-1.0414e-02 	4.0482e-01 	8.0965e-03 	 5.0000e+01
21   	-1.4199e-02 	5.3561e-01 	1.0712e-02 	 5.0000e+01
22   	-2.1098e-02 	7.6415e-01 	1.5283e-02 	 5.0000e+01
23   	-3.6112e-02 	1.2322e+00 	2.4644e-02 	 5.0000e+01
24   	-8.0386e-02 	2.4796e+00 	4.9592e-02 	 5.0000e+01
25   	-3.1729e-01 	7.8096e+00 	1.5619e-01 	 5.0000e+01
Maximum number of iterations.
Done.
x* =  [0.71010106 0.23551614]
Hessian at x* = 
[[ 45.2508031    7.93765054]
 [  7.93765054 147.99461656]] 

Eigenvalues:  [ 44.64118334+0.j 148.60423633+0.j] 
</pre></div>
</div>
</div>
</div>
<p>Test with starting point <span class="math notranslate nohighlight">\(x=[0.0,0.0]^T\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify starting point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>

<span class="c1"># Call optimization routine</span>
<span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">alg1</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">calc_hes</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">eps2</span><span class="p">);</span>

<span class="c1"># Actual Hessian</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hessian at x*= &quot;</span><span class="p">)</span>
<span class="n">analyze_hes</span><span class="p">(</span><span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iter. 	f(x) 		||grad(x)|| 	||p|| 		min(lambda)
0   	 1.6212e-06 	1.5568e-04 	3.8590e-02 	-2.9496e-03
1   	 5.4102e-07 	5.5405e-05 	3.6202e-02 	-1.0900e-03
Done.
x* =  [-0.00124597 -0.07478209]
Hessian at x*= 
[[0.0001531  0.00055527]
 [0.00055527 0.00014766]] 

Eigenvalues:  [ 0.00070565+0.j -0.00040489+0.j] 
</pre></div>
</div>
</div>
</div>
</section>
<section id="adjust-hessian-with-levenberg-marquardt-correction">
<h3><span class="section-number">6.4.3.5. </span>Adjust Hessian with Levenberg-Marquardt Correction<a class="headerlink" href="#adjust-hessian-with-levenberg-marquardt-correction" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">adjusted_hes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">grad</span><span class="p">,</span><span class="n">eps2</span><span class="p">,</span><span class="n">eps3</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="c1"># Estimate Hessian with finite difference</span>
    <span class="n">hes</span> <span class="o">=</span> <span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">grad</span><span class="p">,</span><span class="n">eps2</span><span class="p">)</span>
    
    <span class="c1"># Calculate eigenvalues</span>
    <span class="n">l</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">hes</span><span class="p">)</span>
    <span class="n">smallest_ev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
    
    <span class="c1"># Calculate modification</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span><span class="p">(</span><span class="n">smallest_ev</span> <span class="o">-</span> <span class="n">eps3</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="o">-</span><span class="n">smallest_ev</span> <span class="o">+</span> <span class="n">eps3</span>
    
    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Added (</span><span class="si">{0:1.4e}</span><span class="s2">)*I in LM correction.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">delta</span><span class="p">))</span>
    
    <span class="c1"># Adjust hessian with Levenberg-Marquardt Correction</span>
    <span class="k">return</span> <span class="n">V</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">l</span><span class="p">))</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify starting point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">])</span>

<span class="c1"># Create a Lambda (anonymous) function for Hessian calculation</span>
<span class="n">calc_hes</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">adjusted_hes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">,</span><span class="mf">1E-3</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Call optimization routine</span>
<span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">alg1</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">my_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">calc_hes</span><span class="p">,</span><span class="n">eps1</span><span class="p">,</span><span class="n">eps2</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>

<span class="c1"># Actual Hessian</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hessian at x*= &quot;</span><span class="p">)</span>
<span class="n">analyze_hes</span><span class="p">(</span><span class="n">my_hes_approx</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">calc_grad</span><span class="p">,</span><span class="mf">1E-6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iter. 	f(x) 		||grad(x)|| 	||p|| 		min(lambda)
Added (1.2030e-03)*I in LM correction.
0   	-2.0284e-07 	6.5946e-06 	6.5923e-03 	 1.0000e-03
Done.
x* =  [-0.09849776 -0.0935812 ]
Hessian at x*= 
[[ 4.20491243e-05 -5.31304857e-05]
 [-5.31304857e-05 -2.35437850e-04]] 

Eigenvalues:  [ 5.18741524e-05+0.j -2.45262878e-04+0.j] 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="enhancement-ideas">
<h2><span class="section-number">6.4.4. </span>Enhancement Ideas<a class="headerlink" href="#enhancement-ideas" title="Link to this heading">#</a></h2>
<p>Here are some ideas to improve this notebook:</p>
<ul class="simple">
<li><p>Plot the test function</p></li>
<li><p>On the plot of the test function, visualize the solution path</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/6"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Optimality.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">6.3. </span>Unconstrained Optimality Conditions</p>
      </div>
    </a>
    <a class="right-next"
       href="Quasi-Newton-Methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.5. </span>Quasi-Newton Methods for Unconstrained Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-method-derivation-and-properties">6.4.1. Newton Method Derivation and Properties</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-system-of-equations">6.4.1.1. Nonlinear System of Equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-optimization">6.4.1.2. Unconstrained Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-issues">6.4.1.3. Important Issues:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-2-1-basic-newton-method">6.4.2. Algorithm 2.1: Basic Newton Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-function">6.4.2.1. Helper Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-1-python-implementation">6.4.2.2. Algorithm 1 Python Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-convergence-proof">6.4.2.3. Quadratic Convergence Proof</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-problem">6.4.3. Test Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#starting-point-near-optimal-solution">6.4.3.1. Starting Point Near Optimal Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-a-different-starting-point">6.4.3.2. Activity: A Different Starting Point</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-let-s-break-it">6.4.3.3. Activity: Letâs break it</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-use-i-in-place-of-hessian">6.4.3.4. Activity: Use <span class="math notranslate nohighlight">\(I\)</span> in place of Hessian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-hessian-with-levenberg-marquardt-correction">6.4.3.5. Adjust Hessian with Levenberg-Marquardt Correction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enhancement-ideas">6.4.4. Enhancement Ideas</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander Dowling, et al.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>