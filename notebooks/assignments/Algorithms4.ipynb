{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e9a52a",
   "metadata": {},
   "source": [
    "# Algorithms Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab48994",
   "metadata": {},
   "source": [
    "Assigment Overview: This assignment implements a basic interior point method for nonlinear programs with equality constraints and bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74c034",
   "metadata": {},
   "source": [
    "## Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8715f98",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa637b7e",
   "metadata": {},
   "source": [
    "Recall, the step for a primal-dual interior point method is defined by the following system of linear equations (Eq. 6.56 in Biegler, 2010):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "W^k & \\nabla c(x^k)^T & -I \\\\\n",
    "\\nabla c(x^k) & 0 & 0 \\\\\n",
    "U^k & 0 & X^k\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "d_x^k \\\\\n",
    "d_v^k \\\\\n",
    "d_u^k\n",
    "\\end{bmatrix}\n",
    "=\n",
    "-\\begin{bmatrix}\n",
    "\\nabla f(x^k) + \\nabla c(x^k) v^k - u^k \\\\\n",
    "c(x^k) \\\\\n",
    "X^k u^k - \\mu_l e\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72caae8f",
   "metadata": {},
   "source": [
    "Alternatively, the step for the dual variables for the bounds can be recommed for this system (Eq. 6.57 in Biegler, 2010):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "W^k + \\Sigma^k & \\nabla c(x^k)^T \\\\\n",
    "\\nabla c(x^k) & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "d_x^k \\\\\n",
    "d_v^k\n",
    "\\end{bmatrix}\n",
    "=\n",
    "-\\begin{bmatrix}\n",
    "\\nabla \\varphi_\\mu(x^k) + \\nabla c(x^k) v^k \\\\\n",
    "c(x^k)\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "\n",
    "And then computed using the solution to the linear system (Eq. 6.58 in Biegler, 2010):\n",
    "\n",
    "\\begin{equation*}\n",
    "d_u^k = \\mu_l (X^k)^{-1} e - u^k - \\Sigma^k d_x^k.\n",
    "\\end{equation*}\n",
    "\n",
    "Finally, inertia correction and regularization can be applied to ensure reliable calculation of the Newton step (Eq. 6.59 in Biegler, 2010):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "W^k + \\Sigma^k + \\delta_W I & \\nabla c(x^k)^T \\\\\n",
    "\\nabla c(x^k) & -\\delta_A I\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "d_x^k \\\\\n",
    "d_v^k\n",
    "\\end{bmatrix}\n",
    "=\n",
    "-\\begin{bmatrix}\n",
    "\\nabla \\varphi_\\mu(x^k) + \\nabla c(x^k) v^k \\\\\n",
    "c(x^k)\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a91e88",
   "metadata": {},
   "source": [
    "### Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bb509",
   "metadata": {},
   "source": [
    "Consider the following nonlinear program:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\min_{x} \\quad & f(x) \\\\\n",
    "\t\\mathrm{s.t.} \\quad & c(x) = 0 \\\\\n",
    "\t& x_i \\geq 0, \\quad i \\in \\mathcal{I}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^{n}$, $f(x): \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, $c(x): \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ and $|\\mathcal{I} |$ = $r$ (i.e., there are $r$ variables with a lower bound and $r \\leq n$). This is an extension of (6.48) in Biegler (2010).\n",
    "\n",
    "This has the corresponding log-barrier approximation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\min_{x} \\quad & \\phi_{\\mu_l}(x) := f(x) - \\mu_l \\sum_{i \\in \\mathcal{I}} \\log(x_i) \\\\\n",
    "\t\\mathrm{s.t.} \\quad & c(x) = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is an extension of (6.49) in Biegler (2010).\n",
    "\n",
    "Let the $n \\times r$ matrix $G$ encode which variables are bounded. If variable $i$ corresponds to the $j$th bound, then $G_{i,j} = 1$ and otherwise $G_{i,j} = 0$.\n",
    "\n",
    "$G$ is assembled as follows:\n",
    "1. Initialize $G$ as the zero matrix\n",
    "2. Loop over $k=1$ to $k=|\\mathcal{I}$\n",
    "   1. Extract the index $i$ corresponding to the element $\\mathcal{I}_k$\n",
    "   2. Set $G_{i,k} = 1$\n",
    "\n",
    "Notice that $G$ is the gradient of $x \\geq 0$. $G^T G = I$ but the converse does not hold unless $r = n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fed8b8",
   "metadata": {},
   "source": [
    "### Reformulation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae61884",
   "metadata": {},
   "source": [
    "Start with:\n",
    "\n",
    "$$\n",
    "\t\\begin{align}\n",
    "\t\t\\min_{x} \\quad & x_1^2 + x_2 \\\\\n",
    "\t\t\\mathrm{s.t.} \\quad & x_1 + x_2 = 1 \\\\\n",
    "\t\t& x_1 + 1 \\geq 0\n",
    "\t\\end{align}\n",
    "$$\n",
    "\n",
    "Add slack variable $x_3$ and convert the inequality constraint to an equality constraint and bound:\n",
    "\n",
    "$$\n",
    "\t\\begin{align}\n",
    "\t\t\\min_{x} \\quad & x_1^2 + x_2 \\\\\n",
    "\t\t\\mathrm{s.t.} \\quad & x_1 + x_2 = 1 \\\\\n",
    "\t\t& x_1 + 1 - x_3 = 0 \\\\\n",
    "\t\t& x_3 \\geq 0\n",
    "\t\\end{align}\n",
    "$$\n",
    "\n",
    "Now assemble $G$:\n",
    "\n",
    "$$\n",
    "\tG = \\begin{bmatrix}\n",
    "\t\t0 \\\\\n",
    "\t\t0 \\\\\n",
    "\t\t1\n",
    "\t\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e568fa",
   "metadata": {},
   "source": [
    "### Primal Dual Optimality Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923aa13",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\dims}[3]{\\underbrace{#1}_{#2 \\times #3}}\n",
    "\\newcommand{\\I}[0]{\\mathcal{I}}\n",
    "\\newcommand{\\dfx}[0]{\\dims{\\nabla f(x^k)}{n}{1}}\n",
    "\\newcommand{\\cx}[0]{\\dims{c(x^k)}{m}{1}}\n",
    "\\newcommand{\\dcx}[0]{\\dims{\\nabla c(x^k)}{n}{m}}\n",
    "\\newcommand{\\dcxT}[0]{\\dims{\\nabla_x c(x^k)^T}{m}{n}}\n",
    "\\newcommand{\\W}[0]{\\dims{W^k}{n}{n}}\n",
    "\\newcommand{\\G}[0]{\\dims{G}{n}{r}}\n",
    "\\newcommand{\\GT}[0]{\\dims{G^T}{r}{n}}\n",
    "\\newcommand{\\vk}[0]{\\dims{v^k}{m}{1}}\n",
    "\\newcommand{\\Xinv}[0]{\\dims{(\\hat{X}^k)^{-1}}{r}{r}}\n",
    "\\newcommand{\\X}[0]{\\dims{\\hat{X}^k}{r}{r}}\n",
    "\\newcommand{\\e}[0]{\\dims{e}{r}{1}}\n",
    "\\newcommand{\\uk}[0]{\\dims{u^k}{r}{1}}\n",
    "\\newcommand{\\dx}[0]{\\dims{d_x^k}{n}{1}}\n",
    "\\newcommand{\\dv}[0]{\\dims{d_v^k}{m}{1}}\n",
    "\\newcommand{\\du}[0]{\\dims{d_u^k}{r}{1}}\n",
    "\\newcommand{\\Uk}[0]{\\dims{U^k}{r}{r}}\n",
    "\\newcommand{\\Sk}[0]{\\dims{\\Sigma^k}{n}{n}}\n",
    "\\newcommand{\\dphi}[0]{\\dims{\\nabla \\phi_{\\mu_l}}{n}{1}}\n",
    "$$\n",
    "\n",
    "Next we extend (6.51) in Biegler (2010):\n",
    "    \n",
    "\\begin{gather*}\n",
    "\t\\dfx ~+~ \\dcx \\vk ~-~ \\G \\uk = \\dims{0}{n}{1} \\\\\n",
    "\t\\X \\uk = \\mu \\e \\\\\n",
    "\t\\cx = \\dims{0}{m}{1}\n",
    "\\end{gather*}\n",
    "\n",
    "We now extend (6.56) in Biegler (2010):\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\W & \\dcx & \\G \\\\\n",
    "\t\t\\dcxT & \\dims{0}{m}{m} & \\dims{0}{m}{r} \\\\\n",
    "\t\t\\Uk \\GT & \\dims{0}{r}{m} & \\X\n",
    "\t\\end{bmatrix} \n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\dx \\\\\n",
    "\t\t\\dv \\\\\n",
    "\t\t\\du\n",
    "\t\\end{bmatrix} = -\n",
    "\t\\begin{bmatrix}\n",
    "\t\\dfx ~+~ \\dcx \\vk ~-~ \\G \\uk \\\\\n",
    "\t\\cx \\\\\n",
    "\t\\X \\uk ~-~ \\mu_l \\e\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "where $U^k = \\mathrm{diag}\\{u^k\\}$ and $W^k = \\nabla_{xx} L(x^k,v^k)$. Notice that $W^k$ does NOT include a contribution from the barrier term:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\W = \\dims{\\nabla^2 f(x^k)}{n}{n} + \\sum_{j=1}^{m} \\left( \\dims{\\nabla^2 c_{j}(x^k)}{n}{n} \\underbrace{v^k_{j}}_{\\mathrm{scalar}} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "We can verify that $W^k$ does not include $\\hat{X}$ by showing that the KKT system above is a Newton step to solve the nonlinear system for the primal dual conditions.\n",
    "\n",
    "\n",
    "The Newton step can be simplified, similar to (6.57) and (6.58) in Biegler (2010):\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\W + \\Sk & \\dcx \\\\\n",
    "\t\t\\dcxT & 0\n",
    "\t\\end{bmatrix} \n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\dx \\\\\n",
    "\t\t\\dv\n",
    "\t\\end{bmatrix} = -\n",
    "\t\\begin{bmatrix}\n",
    "\t\\dphi ~+~ \\dcx \\vk \\\\\n",
    "\t\\cx\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\du = \\mu_l \\Xinv \\e ~-~ \\uk ~-~ \\GT \\Sk \\dx\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\Sk = \\G \\Xinv \\Uk \\GT\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\dphi = \\dfx - \\mu_l \\G \\Xinv \\dims{e}{r}{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Notice that the equation for $\\du$ simplifies by substituting $G^T G = I$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\du = \\mu_l \\Xinv \\e ~-~ \\uk ~-~ \\Xinv \\Uk \\GT \\dx\n",
    "\\end{equation*}\n",
    "\n",
    "Finally, inertia correction can be applied to simplified KKT step similar to (6.59) in Biegler (2010).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1ed21",
   "metadata": {},
   "source": [
    "## Basic Interior Point Method for Inequality and Equality Constraint NLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd841f9",
   "metadata": {},
   "source": [
    "Implement a basic interior point method for inequality and equality constrained nonlinear programs. See pg. 154 â€“ 155 in Biegler (2010). You may skip the line search, i.e., always take a full step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a244c5",
   "metadata": {},
   "source": [
    "### Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e86ba7",
   "metadata": {},
   "source": [
    "Write detailed pseudocode on paper or a whiteboard. Scan/take a photo and turn in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec363f",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad37a24",
   "metadata": {},
   "source": [
    "Implement in Python. Hints: Reuse code from Algorithm 5.2 example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec182a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Python libraries\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "### Define helper functions\n",
    "## Check is element of array is NaN\n",
    "def check_nan(A):\n",
    "    return np.sum(np.isnan(A))\n",
    "\n",
    "## Calculate gradient with central finite difference\n",
    "def my_grad_approx(x,f,eps1,verbose=False):\n",
    "    '''\n",
    "    Calculate gradient of function f using central difference formula\n",
    "    \n",
    "    Inputs:\n",
    "        x - point for which to evaluate gradient\n",
    "        f - function to consider\n",
    "        eps1 - perturbation size\n",
    "        \n",
    "    Outputs:\n",
    "        grad - gradient (vector)\n",
    "    '''\n",
    "    \n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"***** my_grad_approx at x = \",x,\"*****\")\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        \n",
    "        # Create vector of zeros except eps in position i\n",
    "        e = np.zeros(n)\n",
    "        e[i] = eps1\n",
    "        \n",
    "        # Finite difference formula\n",
    "        my_f_plus = f(x + e)\n",
    "        my_f_minus = f(x - e)\n",
    "        \n",
    "        # Diagnostics\n",
    "        if(verbose):\n",
    "            print(\"e[\",i,\"] = \",e)\n",
    "            print(\"f(x + e[\",i,\"]) = \",my_f_plus)\n",
    "            print(\"f(x - e[\",i,\"]) = \",my_f_minus)\n",
    "        \n",
    "        \n",
    "        grad[i] = (my_f_plus - my_f_minus)/(2*eps1)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"***** Done. ***** \\n\")\n",
    "    \n",
    "    return grad\n",
    "\n",
    "## Calculate gradient with central finite difference\n",
    "def my_jac_approx(x,h,eps1,verbose=False):\n",
    "    '''\n",
    "    Calculate Jacobian of function h(x) using central difference formula\n",
    "    \n",
    "    Inputs:\n",
    "        x - point for which to evaluate gradient\n",
    "        h - vector-valued function to consider. h(x): R^n --> R^m\n",
    "        eps1 - perturbation size\n",
    "        \n",
    "    Outputs:\n",
    "        A - Jacobian (n x m matrix)\n",
    "    '''\n",
    "    \n",
    "    # Check h(x) at x\n",
    "    h_x0 = h(x)\n",
    "    \n",
    "    # Extract dimensions\n",
    "    n = len(x)\n",
    "    m = len(h_x0)\n",
    "    \n",
    "    # Initialize Jacobian matrix\n",
    "    A = np.zeros((n,m))\n",
    "    \n",
    "    # Calculate Jacobian by row\n",
    "    for i in range(0,n):\n",
    "        \n",
    "        # Create vector of zeros except eps in position i\n",
    "        e = np.zeros(n)\n",
    "        e[i] = eps1\n",
    "        \n",
    "        # Finite difference formula\n",
    "        my_h_plus = h(x + e)\n",
    "        my_h_minus = h(x - e)\n",
    "        \n",
    "        # Diagnostics\n",
    "        if(verbose):\n",
    "            print(\"e[\",i,\"] = \",e)\n",
    "            print(\"h(x + e[\",i,\"]) = \",my_h_plus)\n",
    "            print(\"h(x - e[\",i,\"]) = \",my_h_minus)\n",
    "        \n",
    "        \n",
    "        A[i,:] = (my_h_plus - my_h_minus)/(2*eps1)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"***** Done. ***** \\n\")\n",
    "    \n",
    "    return A\n",
    "    \n",
    "## Calculate Hessian using central finite difference\n",
    "def my_hes_approx(x,grad,eps2):\n",
    "    '''\n",
    "    Calculate gradient of function my_f using central difference formula and my_grad\n",
    "    \n",
    "    Inputs:\n",
    "        x - point for which to evaluate gradient\n",
    "        grad - function to calculate the gradient\n",
    "        eps2 - perturbation size (for Hessian NOT gradient approximation)\n",
    "        \n",
    "    Outputs:\n",
    "        H - Hessian (matrix)\n",
    "    '''\n",
    "    \n",
    "    n = len(x)\n",
    "    H = np.zeros([n,n])\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        # Create vector of zeros except eps in position i\n",
    "        e = np.zeros(n)\n",
    "        e[i] = eps2\n",
    "        \n",
    "        # Evaluate gradient twice\n",
    "        grad_plus = grad(x + e)\n",
    "        grad_minus = grad(x - e)\n",
    "        \n",
    "        # Notice we are building the Hessian by column (or row)\n",
    "        H[:,i] = (grad_plus - grad_minus)/(2*eps2)\n",
    "\n",
    "    return H\n",
    "\n",
    "## Linear algebra calculation\n",
    "def xxT(u):\n",
    "    '''\n",
    "    Calculates u*u.T to circumvent limitation with SciPy\n",
    "    \n",
    "    Arguments:\n",
    "    u - numpy 1D array\n",
    "    \n",
    "    Returns:\n",
    "    u*u.T\n",
    "    \n",
    "    Assume u is a nx1 vector.\n",
    "    Recall: NumPy does not distinguish between row or column vectors\n",
    "    \n",
    "    u.dot(u) returns a scalar. This functon returns an nxn matrix.\n",
    "    '''\n",
    "    \n",
    "    n = len(u)\n",
    "    A = np.zeros([n,n])\n",
    "    for i in range(0,n):\n",
    "        for j in range(0,n):\n",
    "            A[i,j] = u[i]*u[j]\n",
    "    \n",
    "    return A\n",
    "\n",
    "## Analyze Hessian\n",
    "def analyze_hes(B):\n",
    "    print(B,\"\\n\")\n",
    "    \n",
    "    l = linalg.eigvals(B)\n",
    "    print(\"Eigenvalues: \",l,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble KKT matrix (equality constrained only)\n",
    "def assemble_check_KKT(W,Sk,A,deltaA,deltaW,verbose):\n",
    "    \n",
    "    # Add your solution here\n",
    "    \n",
    "    return KKT,inertia_correct,pos_ev,neg_ev,zero_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9be074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barrier_subproblem(x0,v0,u0,calc_f,calc_c,var_bounds,mu,eps,max_iter=100,verbose=False):\n",
    "    '''\n",
    "    Basic Full Space Newton Method for Solving Barrier Subproblem Constrained NLP\n",
    "    \n",
    "    Input:\n",
    "        x0 - starting point (vector)\n",
    "        calc_f - function to calculate objective (returns scalar)\n",
    "        calc_c - function to calculate constraints (returns vector)\n",
    "        var_bounds - list of indicies for variables with lower bound\n",
    "        mu - barrier penalty\n",
    "        eps - tolerance for termination\n",
    "        \n",
    "    Histories (stored for debugging):\n",
    "        x - history of steps (primal variables)\n",
    "        v - history of steps (duals for constraints)\n",
    "        u - history of steps (duals for bounds)\n",
    "        f - history of objective evaluations\n",
    "        c - history of constraint evaluations\n",
    "        df - history of objective gradients\n",
    "        dL - history of Lagrange function gradients\n",
    "        A - history of constraint Jacobians\n",
    "        W - history of Lagrange Hessians\n",
    "        S - history of sigma matrix\n",
    "    \n",
    "    Outputs:\n",
    "        x - final value for primal variable\n",
    "        v - final value for constraint duals\n",
    "        u - final value for bound duals\n",
    "    \n",
    "    Notes:\n",
    "        1. For simplicity, central finite difference is used \n",
    "           for all gradient calculations.\n",
    "    '''    \n",
    "    \n",
    "    ### Specifics for Algorithm 5.2\n",
    "    # Tuning parameters\n",
    "    delta_bar_W_min = 1E-20\n",
    "    delta_bar_W_0 = 1E-4\n",
    "    delta_bar_W_max = 1E40\n",
    "    delta_bar_A = 1E-8\n",
    "    kappa_u = 8\n",
    "    kappa_l = 1/3\n",
    "    \n",
    "    # Declare iteration histories as empty lists\n",
    "    x = []\n",
    "    v = []\n",
    "    u = []\n",
    "    f = []\n",
    "    L = []\n",
    "    c = []\n",
    "    df = []\n",
    "    dL = []\n",
    "    A = []\n",
    "    W = []\n",
    "\n",
    "    # Add your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interior_point(x0,calc_f,calc_c,var_bounds,max_iter=20):\n",
    "    \n",
    "    \n",
    "    # Add your solution here\n",
    "    \n",
    "    return x, v, u, mu, E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb21995",
   "metadata": {},
   "source": [
    "## Test Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ab8ef",
   "metadata": {},
   "source": [
    "### Problem 1: Convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c290f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x[0] + 2*x[1]\n",
    "c = lambda x: (x[0] + x[1] - 1)*np.ones(1)\n",
    "\n",
    "# Indices of variables with lower bound of zero\n",
    "vb = [1]\n",
    "\n",
    "x0 = np.ones(2)\n",
    "u0 = np.ones(1)\n",
    "v0 = np.ones(1)\n",
    "\n",
    "x_, v_, u_, E_ = barrier_subproblem(x0,v0,u0,f,c,vb,1E-1,1E-10,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44a1a9",
   "metadata": {},
   "source": [
    "### Problem 2: Convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e17352",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x[0] + 2*x[1]\n",
    "c = lambda x: (x[0] + x[1] - 1)*np.ones(1)\n",
    "\n",
    "# Indices of variables with lower bound of zero\n",
    "vb = [1]\n",
    "\n",
    "x0 = np.ones(2)\n",
    "#u0 = np.ones(1)\n",
    "#v0 = np.ones(1)\n",
    "\n",
    "# x_, v_, u_, E_ = barrier_subproblem(x0,v0,u0,f,c,vb,1E-1,1E-10,verbose=False)\n",
    "\n",
    "x, v, u, mu, E = interior_point(x0,f,c,vb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of iterations\n",
    "N = len(x)\n",
    "\n",
    "# iteration\n",
    "iters = range(0,N)\n",
    "\n",
    "plt.figure\n",
    "for i in range(0,len(x0)):\n",
    "    plt.plot(iters,[x[j][i] for j in range(0,N)],label=\"x\"+str(i))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Primal Variables\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "for i in range(0,len(u[0])):\n",
    "    plt.plot(iters,[u[j][i] for j in range(0,N)],label=\"u\"+str(i))\n",
    "for i in range(0,len(v[0])):\n",
    "    plt.plot(iters,[v[j][i] for j in range(0,N)],label=\"v\"+str(i))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Dual Variables\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "plt.semilogy(iters,mu)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Barrier Penalty\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "plt.semilogy(range(1,N),E)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1653ce",
   "metadata": {},
   "source": [
    "### Problem 3: Nonconvex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x[0] + 2*x[1] + x[2]**2\n",
    "\n",
    "def c(x):\n",
    "    rhs = np.zeros(2)\n",
    "    rhs[0] = x[0] + x[1] - 1\n",
    "    rhs[1] = x[2] - x[1] - 3\n",
    "    return rhs\n",
    "\n",
    "# Indices of variables with lower bound of zero\n",
    "vb = [1,2]\n",
    "\n",
    "x0 = np.ones(3)\n",
    "u0 = np.ones(2)\n",
    "v0 = np.ones(2)\n",
    "\n",
    "## Test barrier subproblem\n",
    "# xtest, vtest, utest, Etest = barrier_subproblem(x0,v0,u0,f,c,vb,1E-1,1E-10,max_iter=10,verbose=True)\n",
    "\n",
    "##\n",
    "x, v, u, mu, E = interior_point(x0,f,c,vb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of iterations\n",
    "N = len(x)\n",
    "\n",
    "# iteration\n",
    "iters = range(0,N)\n",
    "\n",
    "plt.figure\n",
    "for i in range(0,len(x0)):\n",
    "    plt.plot(iters,[x[j][i] for j in range(0,N)],label=\"x\"+str(i))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Primal Variables\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "for i in range(0,len(u[0])):\n",
    "    plt.plot(iters,[u[j][i] for j in range(0,N)],label=\"u\"+str(i))\n",
    "for i in range(0,len(v[0])):\n",
    "    plt.plot(iters,[v[j][i] for j in range(0,N)],label=\"v\"+str(i))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Dual Variables\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "plt.semilogy(iters,mu)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Barrier Penalty\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure\n",
    "plt.semilogy(range(1,N),E)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
