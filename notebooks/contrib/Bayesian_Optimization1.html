
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bayesian Optimization Tutorial 1 &#8212; Optimization for Decision Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/contrib/Bayesian_Optimization1';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Bayesian Optimization Tutorial 2" href="Bayesian_Optimization2.html" />
    <link rel="prev" title="Deterministic Global Optimization" href="Deterministic_Global_Optimization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/cbe_logo.jpg" class="logo__image only-light" alt="Optimization for Decision Science - Home"/>
    <script>document.write(`<img src="../../_static/cbe_logo.jpg" class="logo__image only-dark" alt="Optimization for Decision Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Optimization for Decision Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Organization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/intro.html">Welcome</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../org/syllabus.html">Syllabus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/calendar.html">Fall 2024 Calendar</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/contribute.html">Contribution Instructions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/workshop.html">Computational Optimization in Python (SÃ£o Paulo, Brazil)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/assignments.html">Assignments</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo1.html">Pyomo Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo2.html">Pyomo Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo3.html">Pyomo Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/project1.html">Project 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms1.html">Algorithms Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms2.html">Algorithms Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/project2.html">Project 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms3.html">Algorithms Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms4.html">Algorithms Homework 4</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/archive.html">Archive</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo-Mini-Project.html">Pyomo Mini-Project: Receding Horizon Stochastic Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/semester_project.html">Semester Project (Spring 2023)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization Modeling in Pyomo</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../1/getting-started.html">1. Getting Started with Pyomo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../1/Local-Install.html">1.1. Local Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Optimization-Modeling.html">1.2. Optimization Modeling with Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Pyomo-Introduction.html">1.3. Your First Optimization Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/LP.html">1.4. Continuous Optimization: Linear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/NLP.html">1.5. Continuous Optimization: Nonlinear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/IP.html">1.6. Integer Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Pyomo-Nuts-and-Bolts.html">1.7. 60 Minutes to Pyomo: An Energy Storage Model Predictive Control Example</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../2/logic.html">2. Logical Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../2/Logical_Modeling_GDP.html">2.1. Logical Modeling and Generalized Disjunctive Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2/Modeling_Disjunctions_Strip_Packing.html">2.2. Modeling Disjunctions through the Strip Packing Problem</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../3/dynamics.html">3. Dynamic Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_car.html">3.1. Pyomo.DAE Example: Race Car</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_TCLab.html">3.2. Pyomo.DAE Example: Temperature Control Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/DAE_background.html">3.3. Differential Algebraic Equations (DAEs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/DAE_numeric_integration.html">3.4. Numeric Integration for DAEs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_theory.html">3.5. Dynamic Optimization with Collocation and Pyomo.DAE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_example.html">3.6. Pyomo.DAE: Racing Example Revisited</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../4/uncertainty.html">4. Optimization Under Uncertainty</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../4/SP.html">4.1. Stochastic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/blocks.html">4.2. Blocks and Other Pyomo Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/AdvancedTopics.html">4.3. Advanced Topics in Stochastic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/RiskMeasures.html">4.4. Risk Measures and Portfolio Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5/data.html">5. Data Science and Applied Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5/Parmest-tutorial.html">5.1. Parameter estimation with <code class="docutils literal notranslate"><span class="pre">parmest</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../5/Parmest-generate-data.html">5.2. Supplementary material: data for parmest tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5/Pyomo_DoE_Tutorial.html">5.3. Optimizing Experiments with <code class="docutils literal notranslate"><span class="pre">Pyomo.DoE</span></code></a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Algorithms and Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../6/unconstrained.html">6. Unconstrained Nonlinear Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../6/Math-Primer-1.html">6.1. Linear Algebra Review and SciPy Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Math-Primer-2.html">6.2. Mathematics Primer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Optimality.html">6.3. Unconstrained Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Newton-Methods.html">6.4. Newton-type Methods for Unconstrained Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Quasi-Newton-Methods.html">6.5. Quasi-Newton Methods for Unconstrained Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Globalization.html">6.6. Descent and Globalization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../7/constrained.html">7. Constrained Nonlinear Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../7/Convexity.html">7.1. Convexity Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Local-Optimality.html">7.2. Local Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/KKT-Multipliers.html">7.3. Analysis of KKT Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Constraint-Qualifications.html">7.4. Constraint Qualifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Second-Order.html">7.5. Second Order Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/degeneracy_hunter.html">7.6. NLP Diagnostics with Degeneracy Hunter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Interior-Point1.html">7.7. Simple Netwon Method for Equality Constrained NLPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Interior-Point2.html">7.8. Inertia-Corrected Netwon Method for Equality Constrained NLPs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../8/special-topics.html">8. Special Topics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../8/MILP.html">8.1. Integer Programming with Simple Branch and Bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../8/MINLP-Algorithms.html">8.2. MINLP Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../8/Global-Opt.html">8.3. Deterministic Global Optimization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Contributions</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="pyomo.html">More Pyomo Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="semiconductor_manufacturing.html">Semiconductor Production Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="student_diet.html">Optimization of Daily Diet Using Pyomo</a></li>
<li class="toctree-l2"><a class="reference internal" href="blending.html">Blending Under Uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="vehicle_routing.html">Vehicle Routing</a></li>

<li class="toctree-l2"><a class="reference internal" href="portfolio_optimization_extended.html">Risk Measures and Portfolio Optimization: Expanded</a></li>
<li class="toctree-l2"><a class="reference internal" href="race_car_extended.html">Extended Race Car Optimization Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="hot_air_balloon.html">Hot Air Balloon Dynamic Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="reactor_design.html">Chemical Reactor Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="Disaster_Response_Plan.html">Disaster Response Plan Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="Sudoku_Solver.html">Sudoku Solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="more_circle_packing.html">Circle Packing Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="modeling.html">Modeling Paradigms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="multi_objective.html">Multi-Objective Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced_stochastic_programming.html">Advanced Topics in Stochastic Programming</a></li>






</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="algorithms.html">Global Optimization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Deterministic_Global_Optimization.html">Deterministic Global Optimization</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Bayesian Optimization Tutorial 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Optimization2.html">Bayesian Optimization Tutorial 2</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="sgd.html">Stochastic Gradient Descent</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Stochastic-Gradient-Descent-1.html">Stochastic Gradient Descent Tutorial 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="Stochastic-Gradient-Descent-2.html">Stochastic Gradient Descent Tutorial 2</a></li>






<li class="toctree-l2"><a class="reference internal" href="Stochastic-Gradient-Descent-3.html">Stochastic Gradient Descent Tutorial 3</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="data.html">Machine Learning and Applied Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="EM-MAP.html">Expectation Maximization Algorithm and MAP Estimation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ndcbe/optimization/blob/master/notebooks/contrib/Bayesian_Optimization1.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ndcbe/optimization" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ndcbe/optimization/issues/new?title=Issue%20on%20page%20%2Fnotebooks/contrib/Bayesian_Optimization1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/contrib/Bayesian_Optimization1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian Optimization Tutorial 1</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1.	Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-we-need-bayesian-optimization">1.1.    When We Need Bayesian Optimization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-vs-nonparametric-in-bayesian-optimization">1.2.  Parametric vs. Nonparametric in Bayesian Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-bayesian-optimization-pbo">1.2.1.	Parametric Bayesian Optimization (PBO)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonparametric-bayesian-optimization-nbo">1.2.2.	Nonparametric Bayesian Optimization (NBO)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#method">2.	Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonparametric-bayesian-optimization">2.1. Nonparametric Bayesian Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-process">2.2. Gaussian Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-mean-function">2.2.1.	The prior mean function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-choice-of-kernel-in-gaussian-process">2.2.2.	The Choice of Kernel in Gaussian Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acquisition-function">2.3. Acquisition Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-improvement-pi">2.3.1.	Probability of Improvement (PI)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-improvement-ei">2.3.2.	Expected Improvement (EI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-the-bayesian-optimization-from-scratch">3.	Implementing the Bayesian Optimization from scratch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries-to-perform-bayesian-optimization">4.	Libraries to Perform Bayesian Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpyopt">4.1. GpyOpt</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-optimize">4.2. Scikit-Optimize</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-between-the-results-of-the-custom-bo-model-gpyopt-and-scikit-opt">4.3. Comparison Between the Results of the Custom BO Model, GpyOPT, and Scikit-Opt</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Discussion:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-usage-of-bayesian-optimization">5. Advanced  Usage of Bayesian Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyper-parameter-tuning">5.1. Hyper-Parameter Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-search">5.1.1 Manual Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">5.1.2 Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-search">5.1.3 Random Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-optimization">5.1.4 Bayesian Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-problem-tuning-hyperparameter-in-ml-based-building-response-estimation-model-under-an-earthquake">5.1.5 Real-World Problem: Tuning HyperParameter in ML-Based Building Response Estimation Model Under an Earthquake</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-required-libraries"><strong>Step 1:</strong> Import required libraries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-the-dataset"><strong>Step 2:</strong> Prepare the dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-define-a-model"><strong>Step 3:</strong> Define a model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-data-visualization"><strong>Step 4:</strong> Data visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-preparing-data"><strong>Step 5:</strong> Preparing Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-defining-the-baseline-model"><strong>Step 6:</strong> Defining the baseline model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-tuning-hyper-parameters"><strong>Step 7:</strong> Tuning Hyper-Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-8-results-and-discussion"><strong>Step 8:</strong> Results and discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Discussion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-studies-bayesian-multi-objective-optimization">6. Further Studies (Bayesian Multi-Objective Optimization)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">7. Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-a-required-packages-and-installation-tips">Appendix A. Required Packages and Installation Tips</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-optimization-tutorial-1">
<h1>Bayesian Optimization Tutorial 1<a class="headerlink" href="#bayesian-optimization-tutorial-1" title="Link to this heading">#</a></h1>
<p><strong>Prepared by:</strong> <a class="reference external" href="https://github.com/ParisaToofani">Parisa Toofani Movaghar</a> (<a class="reference external" href="mailto:ptoofani&#37;&#52;&#48;nd&#46;edu">ptoofani<span>&#64;</span>nd<span>&#46;</span>edu</a>, 2023)</p>
<section id="introduction">
<h2>1.	Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Bayesian optimization is an approach to optimize the objective function that usually takes a long time (minutes or hours) to evaluate like tuning hyperparameters of a deep neural network. Also, many optimization problems in machine learning are black-box optimization problems where the objective function is unknown (black-box function) or we have no information about its derivatives. Thanks to the Bayes theorem, such problems can be handled by developing a Bayesian optimization algorithm. There are many areas where Bayesian optimization (BO) can be a handy tool. To make an example BO has been widely used to design engineering systems (system identification) and choose laboratory experiments in materials (model selection). But this is not the only place that BO can be helpful. In computer science they are widely used in A/B testing, recommender system, robotics and reinforcement learning, environmental monitoring and sensor networks, preference learning and interactive interfaces, and tuning the hyperparameters in deep neural networks. As can be seen, BO is a class of machine-learning-based optimization algorithms that focuses on solving problems by inferring the data. But as an engineer, we should know where the best place is to use BO for our problems.</p>
<section id="when-we-need-bayesian-optimization">
<h3>1.1.    When We Need Bayesian Optimization?<a class="headerlink" href="#when-we-need-bayesian-optimization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The BO algorithm works well on the problem where the input dimension is not too large. It is recommended to use this algorithm for problems with dimensions less than 20.</p></li>
<li><p>The objective function is continuous. We will see later that this is an assumption for the implementation of the Gaussian process (GP).</p></li>
<li><p>Cost matters! The function that we need to optimize is expensive to evaluate. For instance, it requires lots of iterations while our system is limited in performance (computationally cost). On the other hand, the implementation requires purchasing cloud computing environments while our budget is limited (budget limitation).</p></li>
<li><p>The objective function has one of the following problems: 1) the function is unknown (black box), 2) the derivatives unknown or difficult to calculate, and 3) the function is non-convex.</p></li>
<li><p>We are looking for the global optimum, not the local one.</p></li>
</ul>
</section>
<section id="parametric-vs-nonparametric-in-bayesian-optimization">
<h3>1.2.  Parametric vs. Nonparametric in Bayesian Optimization<a class="headerlink" href="#parametric-vs-nonparametric-in-bayesian-optimization" title="Link to this heading">#</a></h3>
<p>A Bayesian Optimization is an approach that uses Bayes Theorem to direct the search in order to find the minimum or maximum of an objective function. The most efficacy of the Bayesian optimization is in the black box system in which the parameter of the model is unknown (nonparametric Bayesian optimization (NBO)). But imagine that I have a beam whose characteristics are defined by stiffness, elasticity, and the applied load (more simply imagine we have a model with a, b, and c variables). Further, the goal is to find these variables in a way that the beam deflection is minimum. If we consider our model uncertain, this is another place that BO comes to help. In other words, due to the uncertainty that exists in our model, we infer our model to find the variables that describe our model well. This case is defined as the parametric Bayesian optimization (PBO).</p>
</section>
<section id="parametric-bayesian-optimization-pbo">
<h3>1.2.1.	Parametric Bayesian Optimization (PBO)<a class="headerlink" href="#parametric-bayesian-optimization-pbo" title="Link to this heading">#</a></h3>
<p>In order to define the Bayesian optimization with parametric models, we consider a generic family of models parametrized by w. Imagine we have a set of data <span class="math notranslate nohighlight">\((D)\)</span>. Since w is an unobserved quantity, we consider it as a latent random variable that has a prior distribution named <span class="math notranslate nohighlight">\(p(w)\)</span>. Given a data D and defining the likelihood model as <span class="math notranslate nohighlight">\(p(D|w)\)</span>, we can then infer a posterior distribution <span class="math notranslate nohighlight">\(p(w|D)\)</span> using the Bayes theorem.</p>
<div class="math notranslate nohighlight">
\[p(wâD)=\frac{(p(Dâw)p(w))}{(p(D))}	\quad \quad(1)\]</div>
<p>The posterior represents the updated beliefs about the model parameters after observing the data <span class="math notranslate nohighlight">\((D)\)</span>. The denominator correspondent with the marginal likelihood (evidence in literature) which is usually difficult to evaluate. This problem is considered a fully Bayesian approach.</p>
</section>
<section id="nonparametric-bayesian-optimization-nbo">
<h3>1.2.2.	Nonparametric Bayesian Optimization (NBO)<a class="headerlink" href="#nonparametric-bayesian-optimization-nbo" title="Link to this heading">#</a></h3>
<p>The NBO is rooted in two components: 1) a Bayesian statistical model for modeling the objective function (surrogate) and 2) an acquisition function for deciding where to sample next. The mathematics here is going to be a little bit complex. We will use a space-filling experimental design to evaluate the objective function. Then we will perform the iterative calculations to allocate the remainder of a budget of N functions evaluations. Here, the surrogate model is defined as a Gaussian process (GP) that provides the Bayesian posterior probability distribution that describes the potential value for <span class="math notranslate nohighlight">\(f(x)\)</span> at the determined point x. Following this, the acquisition function measures the x value at the next step based on the current posterior evaluation.</p>
</section>
</section>
<section id="method">
<h2>2.	Method<a class="headerlink" href="#method" title="Link to this heading">#</a></h2>
<section id="nonparametric-bayesian-optimization">
<h3>2.1. Nonparametric Bayesian Optimization<a class="headerlink" href="#nonparametric-bayesian-optimization" title="Link to this heading">#</a></h3>
<p>In this part, we will see how we can generalize the concept of PBO to NBO in order to optimize our favorite black box. This can be done by marginalizing away the weights in Bayesian linear regression and applying the kernel trick to construct a Bayesian nonparametric regression model. By kernelizing a marginalized version of Bayesian linear regression what we have already done is construct an object called a Gaussian process (GP) which we will discuss in the next part.</p>
<p><strong>___________________________________________</strong></p>
<p><strong>Algorithm:</strong> Bayesian Optimization</p>
<p><strong>___________________________________________</strong></p>
<ol class="arabic simple">
<li><p>Define the search space of the objective function</p></li>
<li><p>Initialize the set of sampled points <span class="math notranslate nohighlight">\(D = []\)</span></p></li>
<li><p>Specify the number of iterations <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p>for i = 1 to t do</p></li>
<li><p>Fit a Gaussian process model to the data in D</p></li>
<li><p>Use an acquisition function (e.g., expected improvement) to propose a new point <span class="math notranslate nohighlight">\(x^*\)</span></p></li>
<li><p>Evaluate the objective function at <span class="math notranslate nohighlight">\(x^*\)</span></p></li>
<li><p>Add the new point and its objective value to <span class="math notranslate nohighlight">\(D\)</span></p></li>
<li><p>end for</p></li>
<li><p>Return the best-observed value in <span class="math notranslate nohighlight">\(D\)</span></p></li>
</ol>
<p><strong>___________________________________________</strong></p>
</section>
<section id="gaussian-process">
<h3>2.2. Gaussian Process<a class="headerlink" href="#gaussian-process" title="Link to this heading">#</a></h3>
<p>The Gaussian process is a nonparametric model that is characterized by its prior mean function (<span class="math notranslate nohighlight">\(Î¼_0\)</span>) and its positive definite kernel (covariance) function (<span class="math notranslate nohighlight">\(Î£_0\)</span>) over the function space. The interesting thing about the  <span class="math notranslate nohighlight">\(Î£_0\)</span> is that it is constructed at each pair of points <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span>. Therefore, if the points <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span> are close to each other in the input space, they would have a larger positive correlation which put emphasis on the belief that they have more similar function values.
The prior distribution over the functions <span class="math notranslate nohighlight">\([f(x_1 ),f(x_2 ),â¦,f(x_n )]\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}fâ\mathcal{N}\left(\left[\begin{matrix}\mu(x_1)\\\vdots\\\mu\left(x_n\right)\\\end{matrix}\right],\ \left[\begin{matrix}\Sigma_0(x_1,x_1)&amp;\ldots&amp;\Sigma_0(x_1,x_n)\\\vdots&amp;\ddots&amp;\vdots\\\Sigma_0(x_n,x_1)&amp;\ldots&amp;\Sigma_0(x_n,x_n)\\\end{matrix}\right]\right)âGP\left(\underset{{Mean}}{{\underbrace{\mu\left(x_{1:n}\right)}}}\ ,\underset{Kernel}{\underbrace{\Sigma_0(x_{1:n},x_{1:n})}}\ \right) \quad\quad	(2)\end{split}\]</div>
<p>Letâs go one step further and include some data in the problem. Imagine the observation is defined as <span class="math notranslate nohighlight">\(D_n={[(x_i,f(x_i)]}_{i=1}^n\)</span>. As defined previously, the random variable <span class="math notranslate nohighlight">\(f(x)\)</span> conditioned on observations <span class="math notranslate nohighlight">\(D_n\)</span> is also normally distributed when kernelizing linear regression. Such a conditioned function is correspondent to the posterior which has the mean of <span class="math notranslate nohighlight">\(Î¼_n\)</span> and <span class="math notranslate nohighlight">\(Ï_n^2\)</span> which is defined as follows:</p>
<div class="math notranslate nohighlight">
\[Î¼_n (x)=Î£_0 (x,x_{1:n} ) [[Î£_0(x_{1:n},x_{1:n} )+Ï^2 I]]^{-1} (f(x_{1:n} )-Î¼(x_{1:n} ))+Î¼(x)  \quad\quad	(3)\]</div>
<div class="math notranslate nohighlight">
\[Ï_n^2 (x)=Î£_0 (x,x)-Î£_0 (x,x_{1:n} ) [[Î£_0 (x_{1:n},x_{1:n} )+Ï^2 I]]^{-1} Î£_0 (x_{1:n},x)  \quad\quad	(4)\]</div>
<p>Where <span class="math notranslate nohighlight">\(Î£_0 (x,x_{1:n} )\)</span> is a vector of covariance terms between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x_{1:n}\)</span>. The posterior mean and variance evaluated at any point x represent the model prediction and uncertainty, respectively, in the objective function at the point <span class="math notranslate nohighlight">\(x\)</span>. The posterior functions are used to select the next query point <span class="math notranslate nohighlight">\(x_{n+1}\)</span>.</p>
</section>
<section id="the-prior-mean-function">
<h3>2.2.1.	The prior mean function<a class="headerlink" href="#the-prior-mean-function" title="Link to this heading">#</a></h3>
<p>The prior mean function provides a possibles a possible offset. In most cases, this function is set to be a constant (<span class="math notranslate nohighlight">\(Î¼(x)â¡Î¼\)</span>). However, when <span class="math notranslate nohighlight">\(f\)</span> is believed to have a trend or some application specific parametric structure, we could define the mean function to be:</p>
<div class="math notranslate nohighlight">
\[Î¼(x)=Î¼+â_{i=1}^pÎ²_i Ï_i (x)	\quad \quad(5)\]</div>
<p>Where each <span class="math notranslate nohighlight">\(\psi_i(x)\)</span> is a parametric function, and often a low-order polynomial in <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="the-choice-of-kernel-in-gaussian-process">
<h3>2.2.2.	The Choice of Kernel in Gaussian Process<a class="headerlink" href="#the-choice-of-kernel-in-gaussian-process" title="Link to this heading">#</a></h3>
<p>One of the most important components of BO is the covariance (kernel) function which dictates the structure of the response functions that we can fit. To make an example, if we expect the response function to be periodic, we can use periodic kernels. Kernels usually are defined as the points closer in the input space that are more strongly correlated. In addition, kernels should be positive-semi definite. Different types of kernels can be defined as follows:</p>
<p>a)	Laplacian function: this function provides a continuous and non-differentiable kernel function. In this case, if you average over your samples, you will get a straight line called Brownian bridges.</p>
<div class="math notranslate nohighlight">
\[\Sigma_0=\sigma_f^2\exp{\left(-\frac{1}{2l^2}\left|\left|x_i-x_j\right|\right|\right)} \quad \quad (6)\]</div>
<p>b)	Power exponential or Gaussian kernel:</p>
<div class="math notranslate nohighlight">
\[\Sigma_0=\sigma_f^2\exp{\left(-\frac{1}{2l^2}\left|\left|x_i-x_j\right|\right|^2\right)} \quad \quad (7)\]</div>
<p>c)	Rational Quadratic:</p>
<div class="math notranslate nohighlight">
\[\Sigma_0=\sigma_f^2\exp{\left(1+\frac{1}{2\alpha l^2}\left|\left|x_i-x_j\right|\right|^{2}\right)^{-\alpha}} \quad \quad (8)\]</div>
<p>d)	Periodic function:</p>
<div class="math notranslate nohighlight">
\[\Sigma_0=\sigma_f^2\exp \left( -2 \sin^2 \left( \frac{\pi \| x_i - x_j \|}{\omega} \right) / l^2 \right) \quad \quad (9)\]</div>
<p>e)	Matern kernel: Matern kernels are a flexible class of stationary kernels. The main parameter to characterize these kernels is <span class="math notranslate nohighlight">\(\nu&gt;0\)</span> which defines the smoothness. The following shows the formulation of famous Matern kernels.</p>
<div class="math notranslate nohighlight">
\[\Sigma_{0_{Mattern1}}=\sigma_f^2\exp{\left(-\sqrt{\left(x_i-x_j\right)^Tl\left(x_i-x_j\right)}\right)} \quad \quad (10)\]</div>
<div class="math notranslate nohighlight">
\[\Sigma_{0_{Mattern3}}=\sigma_f^2\exp{\left(-\sqrt3\sqrt{\left(x_i-x_j\right)^Tl\left(x_i-x_j\right)}\right)}\left(1+\sqrt3\sqrt{\left(x_i-x_j\right)^Tl\left(x_i-x_j\right)}\right) \quad \quad (11)\]</div>
<div class="math notranslate nohighlight">
\[\Sigma_{0_{Mattern5}}=\sigma_f^2\exp{\left(-\sqrt5\sqrt{\left(x_i-x_j\right)^Tl\left(x_i-x_j\right)}\right)}\left(1+\sqrt5\sqrt{\left(x_i-x_j\right)^Tl\left(x_i-x_j\right)}+\frac{5}{3}\left(x_i-x_j\right)^Tl\left(x_i-x_j\right)\right) \quad \quad (12)\]</div>
<div class="math notranslate nohighlight">
\[\Sigma_{0_{Mattern_{exp}}}=\sigma_f^2\exp{\left(-\frac{1}{\left(x_i-x_j\right)^Tl\left(x_i-x_j\right)}\right)} \quad \quad (13)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Importing all required libraries</span>
<span class="c1"># =============================================</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.gridspec</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gridspec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.gaussian_process</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.axes_grid1</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_axes_locatable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warnings</span><span class="w"> </span><span class="kn">import</span> <span class="n">catch_warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">warnings</span><span class="w"> </span><span class="kn">import</span> <span class="n">simplefilter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Creating an object-oriented model to calculate </span>
<span class="c1"># the various kernel functions</span>
<span class="c1"># =============================================</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GpKernels</span><span class="p">:</span>
  <span class="c1"># Model initialization</span>
  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xa</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">sigmaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define basic model parameters that are the same in the whole model</span>
<span class="sd">    Inputs:</span>
<span class="sd">        xa: sample observation (point a)</span>
<span class="sd">        xb: sample observation (point b)</span>
<span class="sd">        sigmaf: (model hyperparameter) vertical scale (the overall variance) -&gt; default value = 1</span>
<span class="sd">        l: (model hyperparameter) horizontal scale (the lengthscale) -&gt; default value = 1</span>
<span class="sd">    Outputs:</span>
<span class="sd">        Nothing   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">xa</span> <span class="o">=</span> <span class="n">xa</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">xb</span> <span class="o">=</span> <span class="n">xb</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sigmaf</span> <span class="o">=</span> <span class="n">sigmaf</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="n">l</span>
    
  <span class="c1"># Define the Laplacian function </span>
  <span class="k">def</span><span class="w"> </span><span class="nf">laplaciankernel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the Kernel using the Laplacian function</span>
<span class="sd">    Inputs:</span>
<span class="sd">        self: include all model parameters in __init__ function</span>
<span class="sd">    Outputs:</span>
<span class="sd">        kernel values   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">diiff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xa</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xb</span><span class="p">,</span> <span class="s1">&#39;sqeuclidean&#39;</span><span class="p">))</span>
    <span class="n">sq_norm</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diiff</span><span class="p">)</span> <span class="c1"># L1 distance</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigmaf</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sq_norm</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

  <span class="c1"># Define the power exponential or Gaussian kernel </span>
  <span class="k">def</span><span class="w"> </span><span class="nf">gausskernel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the Kernel using the power exponential or Gaussian function</span>
<span class="sd">    Inputs:</span>
<span class="sd">        self: include all model parameters in __init__ function</span>
<span class="sd">    Outputs:</span>
<span class="sd">        kernel values   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sq_norm</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">sp</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xa</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xb</span><span class="p">,</span> <span class="s1">&#39;sqeuclidean&#39;</span><span class="p">)</span> <span class="c1"># L2 distance (Squared Euclidian)</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigmaf</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sq_norm</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

  <span class="c1"># Define the Rational Quadratic function</span>
  <span class="k">def</span><span class="w"> </span><span class="nf">rationalquadkernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the Kernel using the Rational Quadratic function</span>
<span class="sd">    Inputs:</span>
<span class="sd">        self: include all model parameters in __init__ function</span>
<span class="sd">        alpha: (model hyperparameter) the scale-mixture&gt;0</span>
<span class="sd">    Outputs:</span>
<span class="sd">        kernel values   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sq_norm</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xa</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xb</span><span class="p">,</span> <span class="s1">&#39;sqeuclidean&#39;</span><span class="p">)))</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigmaf</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">sq_norm</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))),</span> <span class="o">-</span><span class="n">alpha</span><span class="p">)</span>

  <span class="c1"># Define the Periodic function </span>
  <span class="k">def</span><span class="w"> </span><span class="nf">periodickernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">freq</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the Kernel using the Periodic function</span>
<span class="sd">    Inputs:</span>
<span class="sd">        self: include all model parameters in __init__ function</span>
<span class="sd">        freq: the period or the distance between repetitions</span>
<span class="sd">    Outputs:</span>
<span class="sd">        kernel values   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">diiff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xa</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xb</span><span class="p">,</span> <span class="s1">&#39;sqeuclidean&#39;</span><span class="p">))</span>
    <span class="n">sq_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diiff</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigmaf</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">sq_norm</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="n">freq</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># =============================================</span>
<span class="c1"># Creating an object-oriented model to implement </span>
<span class="c1"># the Gaussian Process model</span>
<span class="c1"># =============================================</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GaussianProcess</span><span class="p">:</span>
  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define basic model parameters that are the same in the whole model</span>
<span class="sd">    Inputs:</span>
<span class="sd">        mu: prior mean function</span>
<span class="sd">        sigma: prior positive definite kernel (covariance) function</span>
<span class="sd">    Outputs:</span>
<span class="sd">        Nothing   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

  <span class="k">def</span><span class="w"> </span><span class="nf">gpprior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">realization</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the samples from the prior</span>
<span class="sd">    Inputs:</span>
<span class="sd">        self: include all model parameters in __init__ function</span>
<span class="sd">        realization: number of the samples to generate (number of functions to sample)</span>
<span class="sd">    Outputs:</span>
<span class="sd">        samples from the prior at data points   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">realization</span><span class="p">)</span>

  <span class="k">def</span><span class="w"> </span><span class="nf">gpposterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">kernel_func</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the posterior mean and covariance</span>
<span class="sd">    Inputs:</span>
<span class="sd">        self: include all model parameters in __init__ function</span>
<span class="sd">        X1: sample observation (randomly generated)</span>
<span class="sd">        X2: predicted points (randomly generated)</span>
<span class="sd">        y1: sample observation values (calculated using the objective function and X1)</span>
<span class="sd">        kernel_func: pre-defined object-oriented class (to define the kernel function)</span>
<span class="sd">    Outputs:</span>
<span class="sd">        muPOST: posterior mean</span>
<span class="sd">        sigmaPOST: posterior covariance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># !!! Important: here we assume that the prior mean is equal to zero</span>
    <span class="c1"># Kernel of the noisy observations</span>
    <span class="c1"># The kernel type is defined to be Gaussian (all other types can be used)</span>
    <span class="c1"># Kernel between observations and observations</span>
    <span class="n">sigma11</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X1</span><span class="p">)</span><span class="o">.</span><span class="n">gausskernel</span><span class="p">()</span>
    <span class="c1"># Kernel between observations and predictions</span>
    <span class="n">sigma12</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span><span class="o">.</span><span class="n">gausskernel</span><span class="p">()</span>
    <span class="c1"># Kernel between predictions and predictions</span>
    <span class="n">sigma22</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span><span class="o">.</span><span class="n">gausskernel</span><span class="p">()</span>
    <span class="c1"># Solve</span>
    <span class="c1"># pos --&gt; positive definite</span>
    <span class="n">solved</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">sigma11</span><span class="p">,</span> <span class="n">sigma12</span><span class="p">,</span> <span class="n">assume_a</span><span class="o">=</span><span class="s1">&#39;pos&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="c1"># Compute the posterior mean</span>
    <span class="n">muPOST</span> <span class="o">=</span> <span class="n">solved</span> <span class="o">@</span> <span class="n">y1</span>
    <span class="c1"># Compute the posterior covariance</span>
    <span class="n">sigmaPOST</span> <span class="o">=</span> <span class="n">sigma22</span> <span class="o">-</span> <span class="p">(</span><span class="n">solved</span> <span class="o">@</span> <span class="n">sigma12</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">muPOST</span><span class="p">,</span> <span class="n">sigmaPOST</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Creating an illustration for the covariance matrix </span>
<span class="c1"># =============================================</span>
<span class="c1"># Start plotting</span>
<span class="c1"># The size is set to be (8,4) for capturing a better illustration</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="c1"># Define a limitation for x values</span>
<span class="n">xlim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Expand the shape of an array</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Define a covariance matrix using gaussian kernel</span>
<span class="n">Sigma</span> <span class="o">=</span> <span class="n">GpKernels</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">gausskernel</span><span class="p">()</span>
<span class="c1"># Plot covariance matrix</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Sigma</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="mf">0.045</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="c1"># Set plots attributes</span>
<span class="c1"># title </span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">((</span><span class="s1">&#39;Gaussian covariance </span><span class="se">\n</span><span class="s1">&#39;</span>
              <span class="s1">&#39;matrix contours&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># labels and ticks</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span> 
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$k(x,x)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ticks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ticks</span><span class="p">)))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ticks</span><span class="p">)))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">ticks</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">ticks</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="c1"># Grid</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Plot covariance with X=0 and X -&gt; marginalizing</span>
<span class="c1"># Define a limitation for x values</span>
<span class="n">xlim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1"># Expand the shape of an array</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Define array of zero (X=0)</span>
<span class="n">zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">]])</span>
<span class="c1"># Define a covariance matrix using gaussian kernel</span>
<span class="n">Sigma0</span> <span class="o">=</span> <span class="n">GpKernels</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">zero</span><span class="p">)</span><span class="o">.</span><span class="n">gausskernel</span><span class="p">()</span>
<span class="c1"># Plot marginal</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Sigma0</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$k(x,0)$&#39;</span><span class="p">)</span>
<span class="c1"># Set plots attributes</span>
<span class="c1"># title </span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">((</span>
    <span class="s1">&#39;Gaussian covariance</span><span class="se">\n</span><span class="s1">&#39;</span>
    <span class="s1">&#39;between $x$ and $0$&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># labels and ticks</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;covariance&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">)</span>
<span class="c1"># legends</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2594208eee8f40ce433ac56c0de6bf55d4cbcf72ebad9fbae212a775a2359af0.png" src="../../_images/2594208eee8f40ce433ac56c0de6bf55d4cbcf72ebad9fbae212a775a2359af0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Creating an illustration for drawn samples</span>
<span class="c1"># from the Gaussian process distribution</span>
<span class="c1"># =============================================</span>
<span class="c1"># Number of points in each function</span>
<span class="n">nb_of_samples</span> <span class="o">=</span> <span class="mi">100</span>  
<span class="c1"># Number of functions to sample</span>
<span class="n">number_of_functions</span> <span class="o">=</span> <span class="mi">100</span>  
<span class="c1"># Independent variable samples with specific limitations</span>
<span class="c1"># Expand the shape of an array</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">nb_of_samples</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Generate the Gaussian kernel (covariance matrix)</span>
<span class="n">Î£</span> <span class="o">=</span> <span class="n">GpKernels</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">gausskernel</span><span class="p">()</span>  
<span class="c1"># Start drawing samples from the prior</span>
<span class="c1"># !!! The mean is set to be zero</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_of_samples</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">Î£</span><span class="p">)</span><span class="o">.</span><span class="n">gpprior</span><span class="p">(</span><span class="n">number_of_functions</span><span class="p">)</span>
<span class="c1"># Plot the sampled functions (realizations) -&gt; prior</span>
<span class="c1"># Define a plot size</span>
<span class="n">priorfig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="c1"># title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;100 function realizations</span><span class="se">\n</span><span class="s1">&#39;</span> 
              <span class="s1">&#39;at 100 points&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># generating the line plots using all iterations results</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_functions</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># define a and y axis labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y = f(x)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># define a limit on xaxis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/22b40c7754d9cb397c39fc7e52fd7d92e460138609eec72786fb970942791842.png" src="../../_images/22b40c7754d9cb397c39fc7e52fd7d92e460138609eec72786fb970942791842.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Generating samples from the posterior</span>
<span class="c1"># =============================================</span>
<span class="c1"># Define the actual function (here, a periodic function (cosine) is used)</span>
<span class="n">f_cos</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">n1</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># Training points (used in conditioning)</span>
<span class="n">n2</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Test points in the posterior</span>
<span class="n">ny</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Number of functions that will be sampled from the posterior</span>
<span class="n">domain</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="c1"># Define a specific domain</span>
<span class="c1"># Sample observations (randomly generated)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">domain</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="n">domain</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># Sample observations value (using the objective function)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">f_cos</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="c1"># Predicted points (randomly generated)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">domain</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">domain</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Finding the mean and covariance of the posterior using the Gaussian process</span>
<span class="c1"># !!! It is important to note that prior kernel is set to be a Gaussian kernel</span>
<span class="n">mu2</span><span class="p">,</span> <span class="n">Sigma2</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">()</span><span class="o">.</span><span class="n">gpposterior</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">GpKernels</span><span class="p">)</span>
<span class="c1"># Compute the standard deviation at the test points</span>
<span class="c1"># -&gt; used in defining intervals</span>
<span class="n">Ï2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Sigma2</span><span class="p">))</span>
<span class="c1"># Randomly generated samples for posterior using </span>
<span class="c1">#multivariate Normal distribution</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu2</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">Sigma2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Creating an illustration for drawn samples</span>
<span class="c1"># from the posterior</span>
<span class="c1"># =============================================</span>
<span class="c1"># Define a main plot size and attributes</span>
<span class="n">fig2</span><span class="p">,</span> <span class="p">(</span><span class="n">ax12</span><span class="p">,</span> <span class="n">ax22</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
    <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># set plot properties once so they can be easily changed later</span>
<span class="n">mrk_siz</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># set markersize</span>
<span class="n">lin_wdth</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># set linewidth</span>
<span class="n">shade_intensity</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># set shade intensity</span>

<span class="c1"># title (subplot1)</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">set_title</span><span class="p">((</span><span class="s1">&#39;Posterior and prior</span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="s1">&#39; distribution&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># plot objective function (using predicted points)</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">f_cos</span><span class="p">(</span><span class="n">X2</span><span class="p">),</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lin_wdth</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$cos(x)$&#39;</span><span class="p">)</span>
<span class="c1">#plot the intervals</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X2</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">mu2</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">Ï2</span><span class="p">,</span> <span class="n">mu2</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">Ï2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> 
                 <span class="n">alpha</span><span class="o">=</span><span class="n">shade_intensity</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$2 \sigma_{2|1}$&#39;</span><span class="p">)</span>
<span class="c1"># plot the mean</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lin_wdth</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mu_{2|1}$&#39;</span><span class="p">)</span>
<span class="c1"># plot the randomly generated sample points (training)</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s1">&#39;ks&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">mrk_siz</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$(x_1, y_1)$&#39;</span><span class="p">)</span>
<span class="c1"># set label attributes</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="n">domain</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">domain</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax12</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;lower center&#39;</span><span class="p">,</span><span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plot samples from this function</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">set_title</span><span class="p">((</span><span class="s1">&#39;100 function realizations</span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="s1">&#39;from posterior&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># plot posterior realizations</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># set label attributes</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># define domain</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="n">domain</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">domain</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">ax22</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="c1"># adjust subplots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span> <span class="o">=</span> <span class="mf">0.92</span><span class="p">,</span> <span class="n">wspace</span> <span class="o">=</span> <span class="mf">0.12</span><span class="p">,</span> <span class="n">hspace</span> <span class="o">=</span> <span class="mf">0.08</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7488b18abdaa2a132f1a45125ddf468e987e83a1353143fb37e7b746301c3036.png" src="../../_images/7488b18abdaa2a132f1a45125ddf468e987e83a1353143fb37e7b746301c3036.png" />
</div>
</div>
</section>
<section id="acquisition-function">
<h3>2.3. Acquisition Function<a class="headerlink" href="#acquisition-function" title="Link to this heading">#</a></h3>
<p>Proposing sampling points in the search space is done by acquisition functions. They trade off exploitation and exploration. Exploitation means sampling where the surrogate model predicts a high objective and exploration means sampling at locations where the prediction uncertainty is high. Both correspond to high acquisition function values and the goal is to maximize the acquisition function to determine the next sampling point. There are two famous approaches to implementing the acquisition function named âprobability of improvement (PI)â and âexpected improvement (EI)â. But before getting to each algorithm letâs define what is the improvement. Imagine the objective function that we defined is <span class="math notranslate nohighlight">\(f(x)\)</span> and the current observe point is defined as <span class="math notranslate nohighlight">\(x^*\)</span>, then the improvement function <span class="math notranslate nohighlight">\(I(x)\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight">
\[I(x)=maxâ¡(f(x)-f(x^* ),0) \quad \quad (14)\]</div>
</section>
<section id="probability-of-improvement-pi">
<h3>2.3.1.	Probability of Improvement (PI)<a class="headerlink" href="#probability-of-improvement-pi" title="Link to this heading">#</a></h3>
<p>As we are using the GP, we know that at every point of observation, we apply a Gaussian distribution. Therefore, at point x the value of the function f(x) is sampled from the normal distribution with mean Î¼(x) and variance of the Ï^2 (x). Finally, the probability of improvement is defined as:</p>
<div class="math notranslate nohighlight">
\[PI(x)=1-Ï(z_0 )=Ï(-z_0 )=Ï((Î¼(x)-f(x^* ))/Ï(x) ) \quad \quad (15)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the function of the probability improvement</span>
<span class="k">def</span><span class="w"> </span><span class="nf">probability_improvement</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Xsamples</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the probability improvement</span>
<span class="sd">    Inputs:</span>
<span class="sd">        model: pre-defined surrogate model </span>
<span class="sd">        X: observation data</span>
<span class="sd">        Xsamples: randomly generated samples using random search</span>
<span class="sd">    Outputs:</span>
<span class="sd">        pi: probability of improvement</span>
<span class="sd">        </span>
<span class="sd">    !!! Important note: </span>
<span class="sd">        1- the surrogate model should be defined </span>
<span class="sd">        using the models provided by the Scikit-learn </span>
<span class="sd">        (like GaussianProcessRegressor), otherwise, it will not</span>
<span class="sd">        work.</span>
<span class="sd">        2- 1E-9 added in pi formulation is called jitter, and</span>
<span class="sd">        it is used to avoid numerical problems.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># calculate mean and stdev via surrogate function</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xsamples</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># start prediction using surrogate model</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># find the best score</span>
    <span class="n">best</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
    <span class="c1"># calculate the probability of improvement</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">mu</span> <span class="o">-</span> <span class="n">best</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span><span class="o">+</span><span class="mf">1E-9</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pi</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="expected-improvement-ei">
<h3>2.3.2.	Expected Improvement (EI)<a class="headerlink" href="#expected-improvement-ei" title="Link to this heading">#</a></h3>
<p>PI only considers the probability of the point at the current step. However, the effect of the higher moments can affect the improvement (magnitude of improvement). To solve such a problem, the expected improvement is defined as follows:</p>
<div class="math notranslate nohighlight">
\[EI(x)â¡E[I(x)]=â«I(x)Ï(z)dzâÏ(z)=1/â{2Ï}  expâ¡(-z^2/2)   \quad \quad (16)\]</div>
<div class="math notranslate nohighlight">
\[EI\left(x\right)=\int_{z_0}^{\infty\ \ }{max\left(f\left(x\right)-f\left(x^\ast\right),0\right)\phi\left(z\right)dz=}\int_{z_0}^{\infty\ \ }\left(\mu+\sigma z-f\left(x^\ast\right)\right)\phi\left(z\right)dz=\int_{z_0}^{\infty\ \ }\left(\mu-f\left(x^\ast\right)\right)\phi\left(z\right)dz+\int_{z_0}^{\infty\ \ }{\sigma\ z\ \frac{1}{\sqrt{2\pi}}\ e^{\left(-\frac{z^2}{2}\right)}dz}=\left(\mu-f\left(x^\ast\right)\right)\int_{z_0}^{\infty\ \ }\phi\left(z\right)dz+\sigma\frac{1}{\sqrt{2\pi}}\int_{z_0}^{\infty\ \ }{z\ e^{\left(-\frac{z^2}{2}\right)}dz}=\left(\mu-f\left(x^\ast\right)\right)\left(1-\Phi\left(z_0\right)\right)+\sigma\phi\left(z_0\right)=\left(\mu-f\left(x^\ast\right)\right)\Phi\left(\frac{\mu-f\left(x^\ast\right)}{\sigma}\right)+\sigma\phi\left(\frac{\mu-f\left(x^\ast\right)}{\sigma}\right) \quad \quad (17)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the function of the expected improvement</span>
<span class="k">def</span><span class="w"> </span><span class="nf">expected_improvement</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finding the expectation of improvement</span>
<span class="sd">    Inputs:</span>
<span class="sd">        model: pre-defined surrogate model </span>
<span class="sd">        X: observation data</span>
<span class="sd">        Xsamples: randomly generated samples using random search</span>
<span class="sd">        xi: exploitation-exploration trade-off parameter (default = 0.01)</span>
<span class="sd">    Outputs:</span>
<span class="sd">        ei: expectation of improvement</span>
<span class="sd">        </span>
<span class="sd">    !!! Important note: </span>
<span class="sd">        1- the surrogate model should be defined </span>
<span class="sd">        using the models provided by the Scikit-learn </span>
<span class="sd">        (like GaussianProcessRegressor), otherwise, it will not</span>
<span class="sd">        work.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># calculate mean and stdev via surrogate function</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># start prediction using surrogate model</span>
    <span class="n">mu_sample</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># find the best score</span>
    <span class="n">mu_sample_opt</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">mu_sample</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">):</span>
        <span class="n">imp</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">mu_sample_opt</span> <span class="o">-</span> <span class="n">xi</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">imp</span> <span class="o">/</span> <span class="n">sigma</span>
        <span class="c1"># calculate the expectation of improvement</span>
        <span class="n">ei</span> <span class="o">=</span> <span class="n">imp</span> <span class="o">*</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ei</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="implementing-the-bayesian-optimization-from-scratch">
<h2>3.	Implementing the Bayesian Optimization from scratch<a class="headerlink" href="#implementing-the-bayesian-optimization-from-scratch" title="Link to this heading">#</a></h2>
<p>Now it is the time to put all components together, here, step by step, the Bayesian optimization is implemented using a simple function.</p>
<p>Here, instead of using our 1D Gaussian Process model, we will use the Gaussian process module provided by Scikit-learn.</p>
<p>The model that is used to be optimized is the Ackley function.</p>
<p>The Ackley function is a commonly used benchmark function in optimization and machine learning. It is a multimodal function that is often used to test the performance of optimization algorithms, including Bayesian optimization. The Ackley function is characterized by a large number of local minima and a relatively flat global minimum. This makes it a challenging optimization problem for many algorithms. The Ackley function is typically defined as a two-dimensional function, but it can be easily reduced to one dimension by fixing one of the input variables.</p>
<div class="math notranslate nohighlight">
\[f(x) = -20 \exp\left(-0.2 \sqrt{0.5 x^2}\right) - \exp\left(0.5 \cos(2\pi x)\right) + 20 + \exp(1)
 \quad\quad (18)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !!! This function will be utilized later to demonstrate  the convergence </span>
<span class="c1"># and evaluation of the Bayesian model in each step</span>
<span class="c1"># define a plot function to observe the function behavior per iterations</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plot_converg_iter</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defining a plot function to observe the function behavior per iteration</span>
<span class="sd">    Inputs:</span>
<span class="sd">        X_sample: calculated x per iterations</span>
<span class="sd">        Y_sample: estimated values</span>
<span class="sd">        n_init: skip the first n_init number of samples</span>
<span class="sd">                (initialization steps in algorithms)</span>
<span class="sd">    Outputs:</span>
<span class="sd">        convergence plots</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Define the main figure size</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="c1"># Remodify samples for plotting (flattening)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n_init</span><span class="p">:]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">Y_sample</span><span class="p">[</span><span class="n">n_init</span><span class="p">:]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="c1"># define the number of iterations without initialization</span>
    <span class="n">r</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># find the distance between points</span>
    <span class="n">x_neighbor_dist</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
    <span class="c1"># create cumulative using the maximization</span>
    <span class="n">y_max_watermark</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># title</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;Distance between </span><span class="se">\n</span><span class="s1">&#39;</span>
               <span class="s1">&#39;consecutive x</span><span class="se">\&#39;</span><span class="s1">s&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1"># plot the distance between points per iteration</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">x_neighbor_dist</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="c1"># adjust labels</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Distance&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1"># adjust tick sizes</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># title</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Value of best </span><span class="se">\n</span><span class="s1">&#39;</span>
              <span class="s1">&#39;selected sample&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1"># plot model evaluations per iterations</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="o">-</span><span class="n">y_max_watermark</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="c1"># adjust label</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Best Y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="c1"># adjust tick sizes</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Step 1: Define the objective function</span>
<span class="c1"># =============================================</span>
<span class="c1"># Define an objective function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">objectivefunction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">opt_type</span><span class="o">=</span><span class="s1">&#39;Minimization&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defining objective function</span>
<span class="sd">    Inputs:</span>
<span class="sd">        x: samples from a specific domain</span>
<span class="sd">        noise: standard deviation integrated in </span>
<span class="sd">               white noise (random Normal)</span>
<span class="sd">        opt_type: define if the algorithm should consider the</span>
<span class="sd">                  problem as a minimization or maximization</span>
<span class="sd">    Outputs:</span>
<span class="sd">        objective function with/out noise (trigonometric)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># constant values</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># constant values</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="c1"># constant values</span>
    <span class="n">term1</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">term2</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">x</span><span class="p">)))</span>
    <span class="c1"># add noise to the model </span>
    <span class="n">whitenoise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">opt_type</span> <span class="o">==</span> <span class="s1">&#39;Minimization&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">term1</span> <span class="o">+</span> <span class="n">term2</span> <span class="o">+</span> <span class="n">a</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">whitenoise</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="p">(</span><span class="n">term1</span> <span class="o">+</span> <span class="n">term2</span> <span class="o">+</span> <span class="n">a</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">whitenoise</span><span class="p">)</span>

<span class="c1"># sample the domain sparsely with noise and without noise</span>
<span class="n">X_observed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_observed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">objectivefunction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
<span class="n">y_observed_without_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">objectivefunction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>

<span class="c1"># Plot the observation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="c1"># title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;The generated data sparsely </span><span class="se">\n</span><span class="s1">&#39;</span>
           <span class="s1">&#39;with noise and without noise&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># plot data without noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_observed</span><span class="p">,</span> <span class="n">y_observed_without_noise</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data Without Noise&#39;</span><span class="p">)</span>
<span class="c1"># plot data with noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_observed</span><span class="p">,</span> <span class="n">y_observed</span><span class="p">,</span> <span class="s1">&#39;b+&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data With Noise&#39;</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y = f(x)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># adjust legends</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ab597c1fc23994f3c8992e7f8a1866abac9f191153e8fc8d5b697b09159050bf.png" src="../../_images/ab597c1fc23994f3c8992e7f8a1866abac9f191153e8fc8d5b697b09159050bf.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Import model-specific libraries (from Scikit-learn)</span>
<span class="c1"># =============================================</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.gaussian_process</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.gaussian_process.kernels</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">Matern</span>
<span class="c1"># =============================================</span>
<span class="c1"># Step 2: Define the Gaussian process model</span>
<span class="c1"># =============================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Models description:</span>
<span class="sd">-----------------------------------</span>
<span class="sd">Src: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html</span>
<span class="sd">GaussianProcessRegressor:</span>
<span class="sd">1- kernel: kernel instance, default=None</span>
<span class="sd">2- alpha: float or ndarray of shape (n_samples,), default=1e-10</span>
<span class="sd">-----------------------------------</span>
<span class="sd">Src: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ConstantKernel.html</span>
<span class="sd">ConstantKernel:</span>
<span class="sd">constant_value: float, default=1.0</span>
<span class="sd">-----------------------------------</span>
<span class="sd">Src: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html</span>
<span class="sd">Matern:</span>
<span class="sd">length_scale: float or ndarray of shape (n_features,), default=1.0</span>
<span class="sd">nu: float, default=1.5</span>
<span class="sd">The parameter nu controls the smoothness of the learned function.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># initialize the model</span>
<span class="c1"># define specific X and y values for custom Bayesian model</span>
<span class="c1">#----------------------------------------------------------------------------</span>
<span class="c1"># !!! for solving the Bayesian model (using custom function) we need to inverse </span>
<span class="c1"># the problem and turn it to maximization to be able to catch the global optimal point</span>
<span class="c1">#----------------------------------------------------------------------------</span>
<span class="n">X_bo_custom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y_bo_custom</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">objectivefunction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_bo_custom</span><span class="p">])</span>
<span class="c1"># reshape into rows and cols</span>
<span class="n">X_bo_custom</span> <span class="o">=</span> <span class="n">X_bo_custom</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_bo_custom</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_bo_custom</span> <span class="o">=</span> <span class="n">y_bo_custom</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_bo_custom</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span>
<span class="n">m52</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">gpr_bo</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># fit the model</span>
<span class="n">gpr_bo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_bo_custom</span><span class="p">,</span> <span class="n">y_bo_custom</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-92 {color: black;background-color: white;}#sk-container-id-92 pre{padding: 0;}#sk-container-id-92 div.sk-toggleable {background-color: white;}#sk-container-id-92 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-92 label.sk-toggleable__label-arrow:before {content: "â¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-92 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-92 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-92 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-92 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-92 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-92 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â¾";}#sk-container-id-92 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-92 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-92 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-92 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-92 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-92 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-92 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-92 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-92 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-92 div.sk-item {position: relative;z-index: 1;}#sk-container-id-92 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-92 div.sk-item::before, #sk-container-id-92 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-92 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-92 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-92 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-92 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-92 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-92 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-92 div.sk-label-container {text-align: center;}#sk-container-id-92 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-92 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-92" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GaussianProcessRegressor(alpha=0.04000000000000001,
                         kernel=1**2 * Matern(length_scale=1, nu=2.5))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-92" type="checkbox" checked><label for="sk-estimator-id-92" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianProcessRegressor</label><div class="sk-toggleable__content"><pre>GaussianProcessRegressor(alpha=0.04000000000000001,
                         kernel=1**2 * Matern(length_scale=1, nu=2.5))</pre></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Step 3: Perform the optimization process</span>
<span class="c1"># =============================================</span>
<span class="c1"># Defining the optimization for acquisition function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">opt_acquisition</span><span class="p">(</span><span class="n">Xsamples</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">surrogate</span><span class="p">,</span> <span class="n">algorithm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defining the optimization for acquisition function</span>
<span class="sd">    Inputs:</span>
<span class="sd">        Xsamples: generated samples per iteration</span>
<span class="sd">        X: observed samples </span>
<span class="sd">        y: observed samples values</span>
<span class="sd">        surrogate: surrogate model (Gaussian process)</span>
<span class="sd">        algorithm: probability improvement or expectation improvement</span>
<span class="sd">    Outputs:</span>
<span class="sd">        the sample corresponds to the best score</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># calculate the acquisition function</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">algorithm</span><span class="p">(</span><span class="n">surrogate</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Xsamples</span><span class="p">)</span>
    <span class="c1"># find the index of best score</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Xsamples</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Add timer to catch the calculation runtime</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="c1"># Define lists to capture the output of the model</span>
<span class="n">f_actual</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># actual values of the model</span>
<span class="n">estimated_values</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># estimated values using Gaussian process</span>
<span class="n">x_per_iteration</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># calculated x per iterations</span>
<span class="c1"># start an iteration</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
    <span class="c1"># generate samples</span>
    <span class="n">X_samples_bo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    <span class="c1"># reshape samples to get the exact dimension</span>
    <span class="n">X_samples_bo</span> <span class="o">=</span> <span class="n">X_samples_bo</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_samples_bo</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># select the next point to sample</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">opt_acquisition</span><span class="p">(</span><span class="n">X_samples_bo</span><span class="p">,</span> <span class="n">X_bo_custom</span><span class="p">,</span> <span class="n">y_bo_custom</span><span class="p">,</span> <span class="n">gpr_bo</span><span class="p">,</span> <span class="n">expected_improvement</span><span class="p">)</span>
    <span class="c1"># sample the point</span>
    <span class="n">actual</span> <span class="o">=</span> <span class="n">objectivefunction</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">opt_type</span><span class="o">=</span><span class="s1">&#39;Maximization&#39;</span><span class="p">)</span>
    <span class="c1"># summarize the finding</span>
    <span class="n">est</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gpr_bo</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">x</span><span class="p">]],</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">f_actual</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
    <span class="n">estimated_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">est</span><span class="p">)</span>
    <span class="n">x_per_iteration</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># add the data to the dataset</span>
    <span class="n">X_bo_custom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X_bo_custom</span><span class="p">,</span> <span class="p">[[</span><span class="n">x</span><span class="p">]]))</span>
    <span class="n">y_bo_custom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">y_bo_custom</span><span class="p">,</span> <span class="p">[[</span><span class="n">actual</span><span class="p">]]))</span>
    <span class="c1"># update the model</span>
    <span class="n">gpr_bo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_bo_custom</span><span class="p">,</span> <span class="n">y_bo_custom</span><span class="p">)</span>
    
<span class="n">t2</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Run time is: &quot;</span><span class="p">,</span> <span class="n">t2</span><span class="o">-</span><span class="n">t1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|ââââââââââ| 100/100 [00:05&lt;00:00, 17.33it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Run time is:  5.780000925064087
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Step 4: Demonstrate the calculated results</span>
<span class="c1"># =============================================</span>
<span class="c1"># Find the index of the best-calculated value</span>
<span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_bo_custom</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal point is: &quot;</span><span class="p">,</span> <span class="n">X_bo_custom</span><span class="p">[</span><span class="n">ix</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># As we inverse the problem (turn minimization to maximization)</span>
<span class="c1"># it is necessary  to multiply the model evaluations by -1 </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal value is: &quot;</span><span class="p">,</span> <span class="o">-</span><span class="n">y_bo_custom</span><span class="p">[</span><span class="n">ix</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Run time is:  </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">t2</span><span class="o">-</span><span class="n">t1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>============================================

The optimal point is:  0.006255250376929866
============================================

The optimal value is:  0.29083370411817233
============================================

Run time is:  5.780000925064087
============================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Step 5: Demonstrate the convergence plots</span>
<span class="c1"># =============================================</span>
<span class="n">plot_converg_iter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_bo_custom</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="n">y_bo_custom</span><span class="p">),</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/523d7b71c0e4969b280bc5469584ac9206ff5e0fcc00fa96f5a67fc740fa2433.png" src="../../_images/523d7b71c0e4969b280bc5469584ac9206ff5e0fcc00fa96f5a67fc740fa2433.png" />
</div>
</div>
<section id="discussion">
<h3>Discussion:<a class="headerlink" href="#discussion" title="Link to this heading">#</a></h3>
<p>In the custom model generated Bayesian optimization, the number of iterations is set to 100, and in each iteration, 100 random samples are generated. The expected improvement is defined as an algorithm for the acquisition function. It is important to note that 5 samples are generated to initialize the algorithm. As this algorithm is developed to solve the maximization problem the objective function for maximization is utilized. The run time for performing the algorithm is approximately between 2.5 to 6s which is extremely good and it makes the algorithm efficient. The global optimum is captured by the algorithm between the points of (0.002, 0.06) and (0.007, 0.4) which is near the actual global optimum. Also, it is worth mentioning that the algorithm converges to the optimal point after approximately 25 iterations.</p>
<p><strong>!!! The algorithm is based on randomly generated samples. Therefore, after running the Notebook, you may get different results.</strong></p>
</section>
</section>
<section id="libraries-to-perform-bayesian-optimization">
<h2>4.	Libraries to Perform Bayesian Optimization<a class="headerlink" href="#libraries-to-perform-bayesian-optimization" title="Link to this heading">#</a></h2>
<section id="gpyopt">
<h3>4.1. GpyOpt<a class="headerlink" href="#gpyopt" title="Link to this heading">#</a></h3>
<p>GPyOpt is a Python open-source library for Bayesian Optimization developed by the Machine Learning group of the University of Sheffield. It is based on GPy, a Python framework for Gaussian process modeling. GpyOpt is able to automatically configure the models and Machine Learning algorithms and design the wet-lab experiments while saving time and money. Among other functionalities, GPyOpt can design experiments in parallel, use cost models and mix different types of variables in system designs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Import model-specific libraries </span>
<span class="c1"># =============================================</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">GPy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">GPyOpt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">GPyOpt.methods</span><span class="w"> </span><span class="kn">import</span> <span class="n">BayesianOptimization</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Models description:</span>
<span class="sd">-----------------------------------</span>
<span class="sd">Src: https://gpyopt.readthedocs.io/en/latest/GPyOpt.methods.html</span>
<span class="sd">BayesianOptimization: main class to initialize a Bayesian Optimization method</span>
<span class="sd">1- f: function to optimize</span>
<span class="sd">2- domain: list of dictionaries containing the description of the inputs variables</span>
<span class="sd">3- Initial_design_numdata: number of initial points that are collected jointly</span>
<span class="sd">                           before start running the optimization</span>
<span class="sd">4- Acquisition_type: type of acquisition function to use. - âEIâ, expected improvement. - </span>
<span class="sd">                    âEI_MCMCâ, integrated expected improvement (requires GP_MCMC model). - </span>
<span class="sd">                    âMPIâ, maximum probability of improvement. - </span>
<span class="sd">                    âMPI_MCMCâ, maximum probability of improvement (requires GP_MCMC model). - </span>
<span class="sd">                    âLCBâ, GP-Lower confidence bound. - </span>
<span class="sd">                    âLCB_MCMCâ, integrated GP-Lower confidence bound (requires GP_MCMC model).</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># start to capture the time (for runtime calculation)</span>
<span class="n">t3</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="c1"># define domain characteristics</span>
<span class="n">bds</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;continuous&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}]</span>
<span class="c1"># start optimization using GpyOpt</span>
<span class="c1"># !!! the acquisition function and initial number of </span>
<span class="c1"># points defined the same as the custom-developed BO model</span>
<span class="c1"># create model</span>
<span class="n">gpy_bo</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">objectivefunction</span><span class="p">,</span> 
                              <span class="n">domain</span><span class="o">=</span><span class="n">bds</span><span class="p">,</span>
                              <span class="n">acquisition_type</span> <span class="o">=</span><span class="s1">&#39;EI&#39;</span><span class="p">,</span>
                              <span class="n">initial_design_numdata</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># run the model</span>
<span class="n">gpy_bo</span><span class="o">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">t4</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="c1"># calculate the runtime</span>
<span class="n">Runtime</span> <span class="o">=</span> <span class="n">t4</span> <span class="o">-</span> <span class="n">t3</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Get the optimal point using x_opt attribute</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal point is: &quot;</span><span class="p">,</span> <span class="n">gpy_bo</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Get the optimal value using fx_opt attribute</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal value is: &quot;</span><span class="p">,</span> <span class="n">gpy_bo</span><span class="o">.</span><span class="n">fx_opt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Report the runtime</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Run time is: &quot;</span><span class="p">,</span> <span class="n">Runtime</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>============================================

The optimal point is:  -0.003514669698889943
============================================

The optimal value is:  0.4876386286568032
============================================

Run time is:  55.83359694480896
============================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrate the convergence plots</span>
<span class="n">plot_converg_iter</span><span class="p">(</span><span class="n">gpy_bo</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">gpy_bo</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fe0d329d7469fab2b906aec644c994edb588f3f9531f18517a1d84e9a1ef55e6.png" src="../../_images/fe0d329d7469fab2b906aec644c994edb588f3f9531f18517a1d84e9a1ef55e6.png" />
</div>
</div>
</section>
<section id="id1">
<h3>Discussion<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>In the GpyOpt, all model characteristics are set to be the same as the BO custom model to make them comparable. As can be seen, the algorithm takes more steps until converging to the optimal point (more than 60 iterations). Also, when the model gets near the optimal point experiences a smaller distance. The interesting point about this model is the large runtime which makes it inefficient for such a simple model.</p>
</section>
<section id="scikit-optimize">
<h3>4.2. Scikit-Optimize<a class="headerlink" href="#scikit-optimize" title="Link to this heading">#</a></h3>
<p>Scikit-optimize is a library for sequential model-based optimization that is based on Scikit-learn. It also supports Bayesian optimization using Gaussian processes. The API is designed around minimization, hence, we have to provide negative objective function values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Import model-specific libraries </span>
<span class="c1"># =============================================</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">skopt</span><span class="w"> </span><span class="kn">import</span> <span class="n">gp_minimize</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Models description:</span>
<span class="sd">-----------------------------------</span>
<span class="sd">Src: https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html#skopt.gp_minimize</span>
<span class="sd">BayesianOptimization: Bayesian optimization using Gaussian Processes.</span>
<span class="sd">1- func: callable, Function to minimize. Should take a single list of parameters and return the objective value.</span>
<span class="sd">2- n_calls: int, default: 100, Number of calls to func.</span>
<span class="sd">3- n_random_starts: int, default: None, Number of evaluations of func with random points before </span>
<span class="sd">                    approximating it with base_estimator.</span>
<span class="sd">4- acq_func: string, default: &quot;gp_hedge&quot;, Function to minimize over the gaussian prior. Can be either:</span>
<span class="sd">                              &quot;LCB&quot; for lower confidence bound.</span>
<span class="sd">                              &quot;EI&quot; for negative expected improvement.</span>
<span class="sd">                              &quot;PI&quot; for negative probability of improvement.</span>
<span class="sd">                              &quot;gp_hedge&quot; Probabilistically choose one of the above three acquisition </span>
<span class="sd">                              functions at every iteration. </span>
<span class="sd">                              &quot;EIps&quot; for negated expected improvement per second to take into account </span>
<span class="sd">                              the function compute time. Then, the objective function is assumed to return two values, </span>
<span class="sd">                              the first being the objective value and the second being the time taken in seconds.</span>
<span class="sd">                              &quot;PIps&quot; for negated probability of improvement per second. The return type of </span>
<span class="sd">                              the objective function is assumed to be similar to that of &quot;EIps&quot;</span>
<span class="sd">5- noise: float, default: âgaussianâ. Use noise=âgaussianâ if the objective returns noisy observations. </span>
<span class="sd">                              The noise of each observation is assumed to be iid with mean zero and a fixed variance.</span>
<span class="sd">                              If the variance is known before-hand, this can be set directly to the variance of the noise.</span>
<span class="sd">                              Set this to a value close to zero (1e-10) if the function is noise-free. </span>
<span class="sd">                              Setting to zero might cause stability issues.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># start to capture the time (for runtime calculation)</span>
<span class="n">t5</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="c1"># start optimization using ScikitOpt</span>
<span class="c1"># !!! the acquisition function and initial number of </span>
<span class="c1"># points defined same as the custom developed BO model</span>
<span class="c1"># create and run the model</span>
<span class="n">skopt_bo</span> <span class="o">=</span> <span class="n">gp_minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">objectivefunction</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># the function to minimize</span>
                  <span class="p">[(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>      <span class="c1"># the bounds on each dimension of x</span>
                  <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span>  <span class="c1"># the acquisition function</span>
                  <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>    <span class="c1"># the number of evaluations of f</span>
                  <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># the number of random initialization points</span>
                  <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="o">**</span><span class="mi">2</span><span class="p">,)</span>      <span class="c1"># the noise level (optional)  </span>


<span class="n">t6</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="c1"># calculate the runtime</span>
<span class="n">Runtime1</span> <span class="o">=</span> <span class="n">t6</span> <span class="o">-</span> <span class="n">t5</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Get the optimal point using x[0] attribute</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal point is: &quot;</span><span class="p">,</span> <span class="n">skopt_bo</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Get the optimal value using fun attribute</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal value is: &quot;</span><span class="p">,</span> <span class="n">skopt_bo</span><span class="o">.</span><span class="n">fun</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Report the runtime</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Run time is: &quot;</span><span class="p">,</span> <span class="n">Runtime1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>============================================

The optimal point is:  0
============================================

The optimal value is:  0.2715006871998069
============================================

Run time is:  32.40369367599487
============================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrate the convergence plots</span>
<span class="n">plot_converg_iter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">skopt_bo</span><span class="o">.</span><span class="n">x_iters</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">skopt_bo</span><span class="o">.</span><span class="n">func_vals</span><span class="p">),</span> <span class="n">n_init</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d539f2d69730434c07e039d7a0b57a1ac5144ec44e87f25031aabf17a22aeb6a.png" src="../../_images/d539f2d69730434c07e039d7a0b57a1ac5144ec44e87f25031aabf17a22aeb6a.png" />
</div>
</div>
</section>
<section id="id2">
<h3>Discussion<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Same as GpyOpt, in the ScikitOpt model all parameters are set to be the same as the custom BO model to make it comparable. In the ScikiOpt, the model converges fast to the optimal point (in less than 25 iterations) and after 75 iterations, there is a minor change in the model results. The algorithm is also available to capture the global optimum with approximately good accuracy. After 25 iterations, oscillation is detected in model distances. The runtime of this model is also high in comparison with the custom-generated model.</p>
</section>
<section id="comparison-between-the-results-of-the-custom-bo-model-gpyopt-and-scikit-opt">
<h3>4.3. Comparison Between the Results of the Custom BO Model, GpyOPT, and Scikit-Opt<a class="headerlink" href="#comparison-between-the-results-of-the-custom-bo-model-gpyopt-and-scikit-opt" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Providing a plot to compare BO custom model</span>
<span class="c1"># with the results of the commercial </span>
<span class="c1"># package models</span>
<span class="c1"># =============================================</span>
<span class="c1"># Plot the observation</span>
<span class="n">fig_compare</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;Data Without Zooming&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># plot data with noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_observed</span><span class="p">,</span> <span class="n">y_observed</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data With Noise&#39;</span><span class="p">)</span>
<span class="c1"># plt.scatter(X_bo_custom, -y_bo_custom_predict, c=&#39;r&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_bo_custom</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="o">-</span><span class="n">y_bo_custom</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;BO Custom Model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gpy_bo</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gpy_bo</span><span class="o">.</span><span class="n">fx_opt</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;GpyOpt Module&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">skopt_bo</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skopt_bo</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ScikitOpt Module&#39;</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y = f(x)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># adjust legends</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;Data With Zooming&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># plot data with noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_observed</span><span class="p">,</span> <span class="n">y_observed</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data With Noise&#39;</span><span class="p">)</span>
<span class="c1"># plt.scatter(X_bo_custom, -y_bo_custom_predict, c=&#39;r&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_bo_custom</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="o">-</span><span class="n">y_bo_custom</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;BO Custom Model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gpy_bo</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gpy_bo</span><span class="o">.</span><span class="n">fx_opt</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;GpyOpt Module&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">skopt_bo</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skopt_bo</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ScikitOpt Module&#39;</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y = f(x)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="c1"># adjust legends</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7a2959b6e191edf127104f6095c622bc401e5c045145c4e5262141fb529de2a9.png" src="../../_images/7a2959b6e191edf127104f6095c622bc401e5c045145c4e5262141fb529de2a9.png" />
</div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Approach</p></th>
<th class="head"><p>Optimal Point</p></th>
<th class="head"><p>Optimal Value</p></th>
<th class="head"><p>Run Time (s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Custom BO Model</p></td>
<td><p>~0.0062</p></td>
<td><p>~0.2908</p></td>
<td><p>~5.7800</p></td>
</tr>
<tr class="row-odd"><td><p>GpyOpt</p></td>
<td><p>~-0.0035</p></td>
<td><p>~0.4876</p></td>
<td><p>~55.8335</p></td>
</tr>
<tr class="row-even"><td><p>Scikit-Opt</p></td>
<td><p>~0.0000</p></td>
<td><p>~0.2710</p></td>
<td><p>~32.4036</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="id3">
<h3>Discussion:<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p><strong>!!! It is important to note that the estimated results can differ by each time running this Jupyter Notebook. Therefore, the sign â~â has been put beside the results. Also, there should not be a major discrepancy between the results each time running the Jupyter Notebook.</strong></p>
<p>Main Observation Optimal Point: (0, 1.1772)</p>
<p>Comparing the results between three different algorithms and the main model all algorithms get close to the optimal point with some discrepancies. The existence of discrepancy considering the noisy model with lots of local optimum points is reasonable. Considering that all algorithms perform somehow well in capturing the optimal point, the parameter that differentiates between these three algorithms could be the model run time. The custom-generated BO model performs well by having a run time of 5.7800s. After that, Scikit-Opt and GpyOpt have second and third places, respectively. The first reason is that the custom-generated model skips lots of error-checking steps provided by other packages. This does not guarantee that the BO model performs well on other problems with more complex space or higher dimensions.</p>
</section>
</section>
<section id="advanced-usage-of-bayesian-optimization">
<h2>5. Advanced  Usage of Bayesian Optimization<a class="headerlink" href="#advanced-usage-of-bayesian-optimization" title="Link to this heading">#</a></h2>
<section id="hyper-parameter-tuning">
<h3>5.1. Hyper-Parameter Tuning<a class="headerlink" href="#hyper-parameter-tuning" title="Link to this heading">#</a></h3>
<p>Hyperparameter-tuning is the process of searching the most accurate hyperparameters for a dataset with a Machine Learning algorithm. To do this, we fit and evaluate the model by changing the hyperparameters one by one repeatedly until we find the best accuracy.</p>
<p><strong>What is the difference between a parameter and a hyperparameter?</strong></p>
<ul class="simple">
<li><p><strong>Model parameters</strong>: These are the parameters that are estimated by the model from the given data. For example the weights of a deep neural network.</p></li>
<li><p><strong>Model hyperparameters</strong>: These are the parameters that cannot be estimated by the model from the given data. These parameters are used to estimate the model parameters. For example, the learning rate in deep neural networks.</p></li>
</ul>
<p>But the main question is that what is the efficient approach for tuning the hyper-parameters?</p>
<p>In general, there are four main approaches including:</p>
<ol class="arabic simple">
<li><p>Manual Search</p></li>
<li><p>Grid Search CV</p></li>
<li><p>Random Search CV</p></li>
<li><p>Bayesian Optimization</p></li>
</ol>
</section>
<section id="manual-search">
<h3>5.1.1 Manual Search<a class="headerlink" href="#manual-search" title="Link to this heading">#</a></h3>
<p>Manual hyperparameter tuning involves experimenting with different sets of hyperparameters manually i.e. each trial with a set of hyperparameters will be performed by you. This technique will require a robust experiment tracker which could track a variety of variables from images, and logs to system metrics.</p>
</section>
<section id="grid-search">
<h3>5.1.2 Grid Search<a class="headerlink" href="#grid-search" title="Link to this heading">#</a></h3>
<p>In the grid search method, we create a grid of possible values for hyperparameters. Each iteration tries a combination of hyperparameters in a specific order. It fits the model on every combination of hyperparameters possible and records the model performance. Finally, it returns the best model with the best hyperparameters.</p>
</section>
<section id="random-search">
<h3>5.1.3 Random Search<a class="headerlink" href="#random-search" title="Link to this heading">#</a></h3>
<p>In the random search method, we create a grid of possible values for hyperparameters. Each iteration tries a random combination of hyperparameters from this grid, records the performance, and lastly returns the combination of hyperparameters that provided the best performance.</p>
<p><img alt="Grid vs Random Search" src="https://raw.githubusercontent.com/ndcbe/optimization/main/media/contrib/Grid_vs_Random.png" /></p>
<p>Image Source: Elyse Lee (2019), An Intro to Hyper-parameter Optimization using Grid Search and Random Search, <a class="reference external" href="http://Medium.com">Medium.com</a>. <a class="reference external" href="https://medium.com/&#64;cjl2fv/an-intro-to-hyper-parameter-optimization-using-grid-search-and-random-search-d73b9834ca0a">url</a></p>
</section>
<section id="bayesian-optimization">
<h3>5.1.4 Bayesian Optimization<a class="headerlink" href="#bayesian-optimization" title="Link to this heading">#</a></h3>
<p>In the case of Hyper-Parameter tuning, there are lots of machine learning algorithms (our black-box problems) such as Descision-Trees, random forests, extreme gradient boosting (XGBoost), and deep neural networks which comprise lots of different types of Hyper-Parameters. Tuning such parameters can extremely impact the performance of the whole model. Some algorithms such as XGBoost (the one that is used in the further problem) and DNNs have many Hyper-Parameters that make the process of optimizing them a challenging task.</p>
<p>The Hyper-Parameter optimization problem can be defined as:</p>
<div class="math notranslate nohighlight">
\[ \gamma^{(*)} = argmin_{\gamma \in \Delta} \Phi (\gamma) =argmin_{\gamma \in {\gamma^{(1)}, ..., \gamma^{(S)}}} \Phi (\gamma) \quad\quad	(19)\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi\)</span> is the Hyper-Parameter response function, <span class="math notranslate nohighlight">\(\gamma\)</span> defines the Hyper-Parameters, <span class="math notranslate nohighlight">\(\Delta\)</span> is a search space, and <span class="math notranslate nohighlight">\({\gamma^{(1)}, ..., \gamma^{(S)}}\)</span> represents the trial points.</p>
<p>Hence, Hyper-Parameter optimization is defined as the minimization of <span class="math notranslate nohighlight">\(\Phi (\gamma)\)</span> over <span class="math notranslate nohighlight">\(\gamma \in \Delta\)</span>.</p>
<p>As can be seen, the problem is the same as the one we had in the <span class="math notranslate nohighlight">\(1D\)</span> BO approach. So, the problem can be solved using the following steps:</p>
<ol class="arabic simple">
<li><p>Define a prior measure such as Gaussian Process (GP)</p></li>
<li><p>Getting the posterior measure given some observation over the objective function by combining the likelihood and prior</p></li>
<li><p>Finding the next step using the loss function</p></li>
</ol>
<p>However, solving such a problem is not an easy task for coding from scratch. For this reason, several developers have developed modules like HyperOpt, GpyOpt, and ScikitOpt to solve these problems.</p>
</section>
<section id="real-world-problem-tuning-hyperparameter-in-ml-based-building-response-estimation-model-under-an-earthquake">
<h3>5.1.5 Real-World Problem: Tuning HyperParameter in ML-Based Building Response Estimation Model Under an Earthquake<a class="headerlink" href="#real-world-problem-tuning-hyperparameter-in-ml-based-building-response-estimation-model-under-an-earthquake" title="Link to this heading">#</a></h3>
<p><strong>Problem Definition</strong>:</p>
<p>A multi-degree of freedom (MDOF) simplified 2D shear-building model combined with 222 ground motions ranging from 0.05g to 1.80 g peak ground accelerations are analyzed using OpenSeesPy. The nonlinearity of models is created using the inter-story hysteretic model proposed in HAZUS. methodology, considering the building type, number of stories, and construction year. Then, response-history analysis is conducted to generate a comprehensive dataset for machine learning algorithms. The proposed algorithm used ground motion as well as structural characteristics such as PGA, Sa (0.2s), Sa(1s), and Sa(T1), story height, story mass, and period as input variables to estimate the responses of different structures.</p>
<p>By performing several analyses, the XGboost Regression model performs better on the mentioned model. However, we still looking for improving the generated model using the hyperparameter tuning approaches.</p>
<p><strong>Goal: We are interested to perform both Random Search and Bayesian Optimization to find optimal Hyperparameters of the model.</strong></p>
<p><img alt="Hyperparameter Tuning Example" src="https://raw.githubusercontent.com/ndcbe/optimization/main/media/contrib/Hyper_Tuning_Model_problem.png" /></p>
</section>
<section id="step-1-import-required-libraries">
<h3><strong>Step 1:</strong> Import required libraries<a class="headerlink" href="#step-1-import-required-libraries" title="Link to this heading">#</a></h3>
<p>The first and foremost step to solving this question is importing the required libraries. For this specific problem, we require a Randomized search and cross-validation (provided by sci-kit learn), and XGBoost regressor library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Importing all required libraries</span>
<span class="c1"># =============================================</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span> <span class="c1"># Dataframe management</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">uniform</span> <span class="c1"># random data generation using Unifrom distribution</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">xgboost</span><span class="w"> </span><span class="kn">import</span> <span class="n">XGBRegressor</span> <span class="c1"># XGboost Resgressor for performing regression analysis and predicting model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span> <span class="c1"># It is used to normalize data based on the minimum and maximum</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomizedSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span> <span class="c1"># Baseline and model selection algorithm</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-prepare-the-dataset">
<h3><strong>Step 2:</strong> Prepare the dataset<a class="headerlink" href="#step-2-prepare-the-dataset" title="Link to this heading">#</a></h3>
<p>The dataset is provided on the GitHub repository and it can be imported using the following links. The data also can get extracted using the Pandas module.</p>
<p>It is important to note that there are several inputs that do not have any effect on the prediction model, therefore they are removed from the dataset (Data Cleaning).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Download the dataset</span>
<span class="c1"># =============================================</span>
<span class="c1"># Get the raw data from Github and extract its data using Pandas dataframe (read_csv) method</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/ParisaToofani/OptimizationJourney/main/OptimizationinDL/c2.csv&quot;</span><span class="p">)</span>
<span class="c1"># The mentioned columns are dropped from dataset for two reasons:</span>
<span class="c1"># 1. They do not have critical effect on the model and data</span>
<span class="c1"># 2. Reduce the dimensionality  of the model</span>
<span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;#&#39;</span><span class="p">,</span> <span class="s1">&#39;seismicity&#39;</span><span class="p">,</span> <span class="s1">&#39;story_num&#39;</span><span class="p">,</span> <span class="s1">&#39;disp_dir1&#39;</span><span class="p">,</span> <span class="s1">&#39;react_dir1&#39;</span><span class="p">,</span> <span class="s1">&#39;Mass&#39;</span><span class="p">,</span> <span class="s1">&#39;Period&#39;</span><span class="p">,</span> <span class="s1">&#39;accel_dir1&#39;</span><span class="p">,</span> <span class="s1">&#39;disp_dir1&#39;</span><span class="p">,</span> <span class="s1">&#39;vel_dir1&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># In order to show the dataset we only need to call it down</span>
<span class="n">dataset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sa02</th>
      <th>sa1</th>
      <th>sat</th>
      <th>pga</th>
      <th>zi/h</th>
      <th>Total height</th>
      <th>drift_dir1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.9294</td>
      <td>0.7536</td>
      <td>2.1144</td>
      <td>0.5831</td>
      <td>0.0</td>
      <td>3.2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2842</td>
      <td>0.2023</td>
      <td>0.2172</td>
      <td>0.1395</td>
      <td>0.0</td>
      <td>3.2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3191</td>
      <td>0.3029</td>
      <td>0.3062</td>
      <td>0.1591</td>
      <td>0.0</td>
      <td>3.2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.8270</td>
      <td>0.3352</td>
      <td>1.0861</td>
      <td>0.3378</td>
      <td>0.0</td>
      <td>3.2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.8847</td>
      <td>0.3805</td>
      <td>1.2538</td>
      <td>0.5465</td>
      <td>0.0</td>
      <td>3.2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>19975</th>
      <td>0.7142</td>
      <td>0.5385</td>
      <td>0.5041</td>
      <td>0.3854</td>
      <td>1.0</td>
      <td>38.4</td>
      <td>0.005181</td>
    </tr>
    <tr>
      <th>19976</th>
      <td>0.6358</td>
      <td>0.4857</td>
      <td>0.3770</td>
      <td>0.3461</td>
      <td>1.0</td>
      <td>38.4</td>
      <td>0.001399</td>
    </tr>
    <tr>
      <th>19977</th>
      <td>0.9575</td>
      <td>0.2976</td>
      <td>0.2387</td>
      <td>0.4431</td>
      <td>1.0</td>
      <td>38.4</td>
      <td>0.007332</td>
    </tr>
    <tr>
      <th>19978</th>
      <td>0.4173</td>
      <td>0.1540</td>
      <td>0.1221</td>
      <td>0.1727</td>
      <td>1.0</td>
      <td>38.4</td>
      <td>0.001016</td>
    </tr>
    <tr>
      <th>19979</th>
      <td>0.5971</td>
      <td>0.2945</td>
      <td>0.2631</td>
      <td>0.3138</td>
      <td>1.0</td>
      <td>38.4</td>
      <td>0.008683</td>
    </tr>
  </tbody>
</table>
<p>19980 rows Ã 7 columns</p>
</div></div></div>
</div>
</section>
<section id="step-3-define-a-model">
<h3><strong>Step 3:</strong> Define a model<a class="headerlink" href="#step-3-define-a-model" title="Link to this heading">#</a></h3>
<p>Here, the XGBoost model is utilized for performing the machine learning algorithm. The reason for utilizing such an algorithm is that XGBoost usually performs well on large-scale machine-learning problems on structured datasets. The second reason is that this algorithm can provide good accuracy compared to neural network approaches.</p>
<p>The main Hyper-Parameters of the XGBoost model for the regression task are:</p>
<p><span class="math notranslate nohighlight">\(\eta\)</span> (Learning Rate): Step size shrinkage used in the update to prevent overfitting. After each boosting step, we can directly get the weights of new features, and <span class="math notranslate nohighlight">\(\eta\)</span> shrinks the feature weights to make the boosting process more conservative. The default value is 0.3 and accepts values between <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\gamma\)</span>: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger <span class="math notranslate nohighlight">\(\gamma\)</span> is, the more conservative the algorithm will be. The default value is 0.0 and accepts values between <span class="math notranslate nohighlight">\((0, \infty]\)</span>.</p>
<p>Maximum depth: It defines the maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. The default value is 6.0 and accepts values between <span class="math notranslate nohighlight">\([0, \infty]\)</span>.</p>
<p>Min-chile-weight: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In a linear regression task, this simply corresponds to a minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. The default value is 6.0 and accepts values between <span class="math notranslate nohighlight">\([0, \infty]\)</span>.</p>
<p>n_estimators: The number of trees (or rounds) in an XGBoost model is specified to the XGBClassifier or XGBRegressor class in the n_estimators argument. The default in the XGBoost library is 100.</p>
</section>
<section id="step-4-data-visualization">
<h3><strong>Step 4:</strong> Data visualization<a class="headerlink" href="#step-4-data-visualization" title="Link to this heading">#</a></h3>
<p>Before performing any machine learning task, it is important to have some visualization regarding the data. This can help to decide on various things like normalization, data extraction, and the best algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Visualizing model output regarding each model</span>
<span class="c1"># feature</span>
<span class="c1"># =============================================</span>
<span class="c1"># Plot the observation</span>
<span class="c1"># The standard plot size get modified to capture better visualization</span>
<span class="n">fig_compare</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># plot data with respect to the Sa02</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;drift_dir1&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;sa02&#39;</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Drift&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sa(T=0.2s) (g)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#------------------------------------------------------------------------------------</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># plot data with respect to the Sa01</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;drift_dir1&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;sa1&#39;</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Drift&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sa(T=1s) (g)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#------------------------------------------------------------------------------------</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># plot data with respect to the Sa0t</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;drift_dir1&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;sat&#39;</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Drift&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sa(T) (g)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#------------------------------------------------------------------------------------</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1"># plot data with respect to the pga</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;drift_dir1&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;pga&#39;</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Drift&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;PGA (g)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#------------------------------------------------------------------------------------</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="c1"># plot data with respect to the zi/h</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;drift_dir1&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;zi/h&#39;</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Drift&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Z/H&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#------------------------------------------------------------------------------------</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="c1"># plot data with respect to the Total height</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;drift_dir1&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;Total height&#39;</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Drift&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Total Height (m)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/df1db95e7f02cadc54611ae4ef1e6979ec05f9366dc977ec7b68948ccf77594d.png" src="../../_images/df1db95e7f02cadc54611ae4ef1e6979ec05f9366dc977ec7b68948ccf77594d.png" />
</div>
</div>
</section>
<section id="step-5-preparing-data">
<h3><strong>Step 5:</strong> Preparing Data<a class="headerlink" href="#step-5-preparing-data" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Preparing data</span>
<span class="c1"># =============================================</span>
<span class="c1"># Create an array of data for better data extraction</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span>
<span class="c1"># Get the model input (the first 6 columns)</span>
<span class="n">Xdata</span><span class="o">=</span><span class="n">data</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">]</span>
<span class="c1"># Get the model output (the last column)</span>
<span class="n">ydata</span><span class="o">=</span><span class="n">data</span><span class="p">[:,</span><span class="mi">6</span><span class="p">:]</span>
<span class="c1"># Normalizing data based on the minimum and maximum values</span>
<span class="n">scaler_x</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="c1"># Fitting data to scalar (inputs)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scaler_x</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xdata</span><span class="p">))</span>
<span class="c1"># Transform data the get the scaled data</span>
<span class="n">xscale</span><span class="o">=</span><span class="n">scaler_x</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Xdata</span><span class="p">)</span>
<span class="c1"># Fitting data to scalar (output)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scaler_y</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ydata</span><span class="p">))</span>
<span class="c1"># Transform data the get the scaled data</span>
<span class="n">yscale</span><span class="o">=</span><span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ydata</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MinMaxScaler()
MinMaxScaler()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Splitting the data</span>
<span class="c1"># =============================================</span>
<span class="c1"># !!! This is a necessary  step to help us with prediction model evaluation</span>
<span class="c1"># 85% of data are used for model training and 15% of </span>
<span class="c1"># data are used for further steps (model evaluation)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">xscale</span><span class="p">,</span> <span class="n">yscale</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-6-defining-the-baseline-model">
<h3><strong>Step 6:</strong> Defining the baseline model<a class="headerlink" href="#step-6-defining-the-baseline-model" title="Link to this heading">#</a></h3>
<p>In this step, the XGBRegressor is utilized to create a model. It is important to note that the model is developed based on the default parameters. Also, the evaluation model here (the baseline) is defined based on the mean of the cross-validation score. The scoring parameter is also chosen as a negative mean squared error.</p>
<p>Here, a brief discussion is provided about the cross-validation score based on an article by Stephen Allwright.</p>
<p>Cross_val_score is a function in the Scikit-Learn package which trains and tests a model over multiple folds of your dataset. This cross-validation method gives you a better understanding of model performance over the whole dataset instead of just a single train/test split. Cross_val_score is used as a simple cross-validation technique to prevent over-fitting and promote model generalization. The process that cross_val_score uses is typical for cross-validation and follows these steps:</p>
<ul class="simple">
<li><p>The number of folds is defined, by default, this is 5</p></li>
<li><p>The dataset is split up according to these folds, where each fold has a unique set of testing data</p></li>
<li><p>A model is trained and tested for each fold</p></li>
<li><p>Each fold returns a metric for its test data</p></li>
<li><p>The mean and standard deviation of these metrics can then be calculated to provide a single metric for the process</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Instantiate an XGBRegressor with default</span>
<span class="c1"># hyperparameter settings</span>
<span class="c1"># =============================================</span>
<span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">()</span>
<span class="c1"># compute a baseline </span>
<span class="n">baseline</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-7-tuning-hyper-parameters">
<h3><strong>Step 7:</strong> Tuning Hyper-Parameters<a class="headerlink" href="#step-7-tuning-hyper-parameters" title="Link to this heading">#</a></h3>
<p>In this step, we define a dictionary of Hyper-Parameters and their correspondent range (param-dist), or the list of dictionaries (bds) to utilize them in both random search and the âBayesianOptimizationâ model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Hyper-parameter tuning using randomized search</span>
<span class="c1"># model</span>
<span class="c1"># =============================================</span>
<span class="c1"># Define the model hyperparameters and the range</span>
<span class="c1"># to be tuned</span>
<span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
              <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
              <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span>
              <span class="s2">&quot;n_estimators&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">300</span><span class="p">),</span>
              <span class="s2">&quot;min_child_weight&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)}</span>

<span class="c1"># Perform random search task for 25 iterations</span>
<span class="c1"># Model data:</span>
<span class="c1"># Src: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html</span>
<span class="c1"># xgb: XGBoost regression model used for training data</span>
<span class="c1"># param_distributions: model hyper-parameters</span>
<span class="c1"># scoring: model evaluation metric</span>
<span class="c1"># n_iter: number of iterations</span>
<span class="n">rs</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_dist</span><span class="p">,</span> 
                        <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="c1"># Fitting the model structure to perform- </span>
<span class="c1"># hyper-parameter tuning</span>
<span class="n">rs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Model structure and parameters value after </span>
<span class="c1"># performing Hyper-Parameter tuning</span>
<span class="c1"># =============================================</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal learning rate is: &quot;</span><span class="p">,</span> <span class="mf">0.3195</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal gamma is: &quot;</span><span class="p">,</span> <span class="mf">0.4097</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal max_depth is: &quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal n_estimators is: &quot;</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal min_child_weight is: &quot;</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">rs</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>============================================

The optimal learning rate is:  0.3195
============================================

The optimal gamma is:  0.4097
============================================

The optimal max_depth is:  20
============================================

The optimal n_estimators is:  101
============================================

The optimal min_child_weight is:  7
============================================
</pre></div>
</div>
<div class="output text_html"><style>#sk-container-id-95 {color: black;background-color: white;}#sk-container-id-95 pre{padding: 0;}#sk-container-id-95 div.sk-toggleable {background-color: white;}#sk-container-id-95 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-95 label.sk-toggleable__label-arrow:before {content: "â¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-95 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-95 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-95 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-95 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-95 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-95 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â¾";}#sk-container-id-95 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-95 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-95 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-95 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-95 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-95 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-95 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-95 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-95 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-95 div.sk-item {position: relative;z-index: 1;}#sk-container-id-95 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-95 div.sk-item::before, #sk-container-id-95 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-95 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-95 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-95 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-95 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-95 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-95 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-95 div.sk-label-container {text-align: center;}#sk-container-id-95 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-95 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-95" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=0.40973497757318345, gpu_id=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.31952389742711895, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=20, max_leaves=None,
             min_child_weight=7, missing=nan, monotone_constraints=None,
             n_estimators=101, n_jobs=None, num_parallel_tree=None,
             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-95" type="checkbox" checked><label for="sk-estimator-id-95" class="sk-toggleable__label sk-toggleable__label-arrow">XGBRegressor</label><div class="sk-toggleable__content"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=0.40973497757318345, gpu_id=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.31952389742711895, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=20, max_leaves=None,
             min_child_weight=7, missing=nan, monotone_constraints=None,
             n_estimators=101, n_jobs=None, num_parallel_tree=None,
             predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Hyper-parameter tuning using Bayesian Optimization</span>
<span class="c1"># =============================================</span>
<span class="c1"># Optimization objective </span>
<span class="k">def</span><span class="w"> </span><span class="nf">cv_score</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define the objective function based on the cross-validation score</span>
<span class="sd">    Inputs:</span>
<span class="sd">        parameters: model hyper-parameters to be tuned </span>
<span class="sd">    Outputs:</span>
<span class="sd">        cross-validation score   </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># get the model parameters </span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># define a cross-validation score mode and calculate it</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span>
                <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                              <span class="n">gamma</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                              <span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                              <span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>
                              <span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span> 
                              <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># remodify a score</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="c1"># Define the model hyperparameters and the range</span>
<span class="c1"># to be tuned</span>
<span class="n">bds</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;continuous&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;continuous&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;max_depth&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;discrete&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;discrete&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;min_child_weight&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;discrete&#39;</span><span class="p">,</span> <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}]</span>


<span class="c1"># Model data:</span>
<span class="c1"># Python package: GpyOpt</span>
<span class="c1"># f: objective function</span>
<span class="c1"># domain: model hyper-parameters boundaries</span>
<span class="c1"># model_type: surrogate model -&gt; GP</span>
<span class="c1"># acquisition_type: expectation improvement</span>
<span class="c1"># acquisition_jitter: a small number added to avoid model instability -&gt;0.05</span>
<span class="c1"># exact_feval: if True: it is assumed that cross-validation in Hyper-Parameter-</span>
<span class="c1"># space is deterministic</span>
<span class="c1"># maximize: optimization type</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">cv_score</span><span class="p">,</span> 
                                 <span class="n">domain</span><span class="o">=</span><span class="n">bds</span><span class="p">,</span>
                                 <span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;GP&#39;</span><span class="p">,</span>
                                 <span class="n">acquisition_type</span> <span class="o">=</span><span class="s1">&#39;EI&#39;</span><span class="p">,</span>
                                 <span class="n">acquisition_jitter</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
                                 <span class="n">exact_feval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                 <span class="n">maximize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Start running the model</span>
<span class="c1"># Maximum number of iterations is set to be 20,</span>
<span class="c1"># considering the number of initial points (default = 5)</span>
<span class="c1"># the maximum number of iterations is 25</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># Hyper-parameters value after tuning  them using</span>
<span class="c1"># Bayesian optimization</span>
<span class="c1"># =============================================</span>
<span class="n">hp_tunes_BO</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">x_opt</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal learning rate is: &quot;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">hp_tunes_BO</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">4</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal gamma is: &quot;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">hp_tunes_BO</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">4</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal max_depth is: &quot;</span><span class="p">,</span> <span class="n">hp_tunes_BO</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal n_estimators is: &quot;</span><span class="p">,</span> <span class="n">hp_tunes_BO</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The optimal min_child_weight is: &quot;</span><span class="p">,</span> <span class="n">hp_tunes_BO</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>============================================

The optimal learning rate is:  0.165
============================================

The optimal gamma is:  0.0948
============================================

The optimal max_depth is:  50.0
============================================

The optimal n_estimators is:  300.0
============================================

The optimal min_child_weight is:  1.0
============================================
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-8-results-and-discussion">
<h3><strong>Step 8:</strong> Results and discussion<a class="headerlink" href="#step-8-results-and-discussion" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># The plot of negative mean squared variation </span>
<span class="c1"># per iteration considering both algorithms</span>
<span class="c1"># =============================================</span>
<span class="n">y_rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">rs</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">])</span>
<span class="n">y_bo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="o">-</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Y</span><span class="p">[:])</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="c1"># start plotting</span>
<span class="n">fig_compare_tuning</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="c1"># title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;Value of the best </span><span class="se">\n</span><span class="s1">&#39;</span>
           <span class="s1">&#39;sampled CV score&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">);</span>
<span class="c1"># plot random search results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_rs</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Random search&#39;</span><span class="p">)</span>
<span class="c1"># plot Bayesian optimization results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_bo</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bayesian optimization&#39;</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39; Negative MSE&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Baseline  MSE = </span><span class="si">{</span><span class="n">baseline</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Random search  MSE = </span><span class="si">{</span><span class="n">y_rs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bayesian optimization  MSE = </span><span class="si">{</span><span class="n">y_bo</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e4ba2ff05357a1a0d57839b162faed24ebcfc2eb13f06a091cb5bd4d6b04cc31.png" src="../../_images/e4ba2ff05357a1a0d57839b162faed24ebcfc2eb13f06a091cb5bd4d6b04cc31.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Baseline  MSE = -0.00061
Random search  MSE = -0.00125
Bayesian optimization  MSE = -0.00071
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================</span>
<span class="c1"># the models&#39; prediction</span>
<span class="c1"># =============================================</span>
<span class="c1"># find the predicted values using random search</span>
<span class="n">y_predict_rs</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># find the predicted values using Bayesian optimization</span>
<span class="c1"># In the Bayesian optimization approach we need to reconstruct the model</span>
<span class="n">model_bo</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">optimizer</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                              <span class="n">gamma</span><span class="o">=</span><span class="n">optimizer</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                              <span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                              <span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>
                              <span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">)</span>
<span class="n">model_bo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predict_bo</span> <span class="o">=</span> <span class="n">model_bo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># -------------------------------------------------------------------------</span>
<span class="c1"># Plot the observation</span>
<span class="n">fig_compare</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;Randomized Search&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># plot data predicted values versus actual data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_predict_rs</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Y Predicted&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y Real&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">((</span><span class="s1">&#39;Bayesian Optimization&#39;</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># plot data predicted values versus actual data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_predict_bo</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># define labels attributes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Y Predicted&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y Real&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># adjust tick sizes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span><span class="n">top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/b63679ce7ec21ebe3722855c934dc6c7dc17b00fec175dcfeb21f02791f17e82.png" src="../../_images/b63679ce7ec21ebe3722855c934dc6c7dc17b00fec175dcfeb21f02791f17e82.png" />
</div>
</div>
</section>
<section id="id4">
<h3>Discussion<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>Before starting the discussion here is some information regarding the evaluation metrics.</p>
<p>Mean Squared Error (MSE) is a commonly used metric for evaluating the performance of regression models. It measures the average of the squared differences between the predicted and actual values of the target variable.</p>
<p>Negative Mean Squared Error (NMSE) is simply the negation of the MSE. In other words, itâs the negative value of the average of the squared differences between the predicted and actual values of the target variable.</p>
<p>The reason for using the negative value of MSE is mainly for mathematical conveniences, such as when minimizing an objective function. By negating the MSE, we can treat the optimization problem as a maximization problem, which can sometimes be easier to solve or interpret.</p>
<p>First, the NMSE values in tuned models are near zero. This means the model is performing well while can make a risk of overfitting.</p>
<p>Second, the NMSE values of the model are lower than that of the baseline model, this indicates that the models are performing better.</p>
</section>
</section>
<section id="further-studies-bayesian-multi-objective-optimization">
<h2>6. Further Studies (Bayesian Multi-Objective Optimization)<a class="headerlink" href="#further-studies-bayesian-multi-objective-optimization" title="Link to this heading">#</a></h2>
<p>Bayesian multi-objective optimization is a framework for optimizing multiple objective functions simultaneously while accounting for uncertainty. The objective is to identify a set of solutions that approximate the Pareto front, which represents the set of non-dominated solutions that achieve the best trade-offs between the different objectives.</p>
<p>The Bayesian approach involves constructing a probabilistic model of the objective functions and constraints, which can be represented as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) \sim GP(m(x), k(x,x')) \quad \quad (20)\]</div>
<div class="math notranslate nohighlight">
\[g(x) \sim Bernoulli(p(x)) \quad \quad (21)\]</div>
<p>where GP denotes a Gaussian process, <span class="math notranslate nohighlight">\(m(x)\)</span> is the mean function, <span class="math notranslate nohighlight">\(k(x,x')\)</span> is the covariance function, and p(x) is the probability of the constraints being satisfied.</p>
<p>Given this probabilistic model, Bayesian multi-objective optimization involves iteratively selecting the next input x to evaluate based on an acquisition function that balances exploration and exploitation and updating the model with the new observation. The acquisition function can be defined based on different criteria, such as expected improvement, probability of improvement, or hypervolume.</p>
<p>The iterative process continues until a stopping criterion is met, such as a maximum number of evaluations or convergence to a satisfactory Pareto front approximation. The final set of solutions X* represents a set of non-dominated solutions that achieve the best trade-offs between the different objectives.</p>
<p>There are several variations of Bayesian multi-objective optimization, such as constrained multi-objective optimization, where constraints are explicitly considered in the optimization process, and multi-objective Bayesian optimization with preferences, where the userâs preferences are incorporated into the optimization process.</p>
<p>Bayesian multi-objective optimization has numerous applications in various fields, including:</p>
<ul class="simple">
<li><p>Engineering design: optimization of complex systems with multiple objectives, such as minimizing weight, cost, and energy consumption while maximizing performance.</p></li>
<li><p>Finance: portfolio optimization to maximize returns while minimizing risk.</p></li>
<li><p>Machine learning: hyperparameter tunings of machine learning models, such as selecting the optimal regularization parameter, learning rate, or architecture.</p></li>
<li><p>Environmental management: optimization of water resources management, energy planning, and land use management.</p></li>
<li><p>Healthcare: optimization of treatment plans for patients with multiple health conditions, such as cancer patients with multiple tumors.</p></li>
<li><p>Robotics: optimization of robot control policies to achieve multiple objectives, such as maximizing task performance and minimizing energy consumption.</p></li>
</ul>
<p>Here are some key papers and resources on Bayesian multi-objective optimization:</p>
<ul class="simple">
<li><p>âBayesian Multi-Objective Optimizationâ by HernÃ¡ndez-Lobato et al. (2016) presents a comprehensive overview of Bayesian multi-objective optimization, including the formulation of the problem, the different approaches and algorithms that have been proposed, and their applications in different fields.</p></li>
<li><p>âBayesian multi-objective optimization with preference articulationâ by Hernandez-Lobato et al. (2017) introduces a new approach that allows the incorporation of preferences into the optimization process, allowing the user to guide the search towards solutions that better meet their preferences.</p></li>
<li><p>âBOCS: Bayesian optimization for constrained multi-objective optimization problemsâ by Li et al. (2018) proposes an algorithm that can handle constrained multi-objective optimization problems by using a constrained acquisition function that takes into account both the feasibility and the quality of the solutions.</p></li>
<li><p>âBayesian multi-objective optimization using Gaussian processes, preference modeling, and parallelizationâ by Cremers et al. (2021) presents an approach that combines preference modeling, parallelization, and Bayesian optimization with Gaussian processes to efficiently solve multi-objective optimization problems with complex and high-dimensional search spaces.</p></li>
<li><p>The MOBOpt library (<a class="github reference external" href="https://github.com/numbbo/coco/blob/master/code-experiments/bbob/python/mocobbo.py">numbbo/coco</a>) provides a collection of multi-objective optimization problems and algorithms, including Bayesian optimization approaches.</p></li>
</ul>
<p>Overall, Bayesian multi-objective optimization is a powerful and flexible framework for optimizing multiple objectives simultaneously while accounting for uncertainty. It has been widely used in many fields, including engineering design, finance, and machine learning.</p>
</section>
<section id="conclusion">
<h2>7. Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Bayesian optimization is one of the powerful approaches to optimize the objective function that usually takes a long time (minutes or hours) to evaluate.</p></li>
<li><p>Bayesian optimization plays a key role in optimizing nonparametric models like machine learning algorithms.</p></li>
<li><p>Currently, there are various packages to perform Bayesian optimization like GpyOpt, ScikitOpt, and Hyperopt. But when it comes to models that are not complex, these packages are not necessarily efficient. In this case, sometimes custom-developed functions perform better.</p></li>
<li><p>There are various applications for using Bayesian optimization. Among all of them, hyper-parameter tuning is of importance. Bayesian optimization helps to find the best model parameters to develop a better prediction model.</p></li>
<li><p>Finally, Bayesian optimization approaches are still evolving. For instance, Bayesian multi-objective optimization has been recently used as a promising approach for solving multi-objective optimization problems efficiently. This field is still evolving, and new approaches and algorithms are being proposed to improve its effectiveness and applicability.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p><strong>Publications:</strong></p>
<ol class="arabic simple">
<li><p>Frazier, P. I. (2018). A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811.</p></li>
<li><p>Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., &amp; De Freitas, N. (2015). Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1), 148-175.</p></li>
<li><p>Jones, D. R., Schonlau, M., &amp; Welch, W. J. (1998). Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4), 455.</p></li>
<li><p>Brochu, E., Cora, V. M., &amp; De Freitas, N. (2010). A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599.</p></li>
<li><p>Putatunda, S., &amp; Rama, K. (2019, December). A modified bayesian optimization based hyper-parameter tuning approach for extreme gradient boosting. In 2019 Fifteenth International Conference on Information Processing (ICINPRO) (pp. 1-6). IEEE.</p></li>
</ol>
<p><strong>Websites:</strong></p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7">https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7</a></p></li>
<li><p><a class="reference external" href="https://machinelearningmastery.com/what-is-bayesian-optimization/">https://machinelearningmastery.com/what-is-bayesian-optimization/</a></p></li>
<li><p><a class="reference external" href="https://distill.pub/2020/bayesian-optimization/">https://distill.pub/2020/bayesian-optimization/</a></p></li>
<li><p><a class="reference external" href="http://krasserm.github.io/2018/03/21/bayesian-optimization/">http://krasserm.github.io/2018/03/21/bayesian-optimization/</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/hyperopt-hyperparameter-tuning-based-on-bayesian-optimization-7fa32dffaf29">https://towardsdatascience.com/hyperopt-hyperparameter-tuning-based-on-bayesian-optimization-7fa32dffaf29</a></p></li>
<li><p><a class="reference external" href="https://stephenallwright.com/cross_val_score-sklearn/">https://stephenallwright.com/cross_val_score-sklearn/</a></p></li>
</ol>
</section>
<section id="appendix-a-required-packages-and-installation-tips">
<h2>Appendix A. Required Packages and Installation Tips<a class="headerlink" href="#appendix-a-required-packages-and-installation-tips" title="Link to this heading">#</a></h2>
<p><strong>The project is implemented in the Anaconda environment and Jupyter notebook.</strong></p>
<p>For installing Anaconda following link is provided:</p>
<p><a class="reference external" href="https://www.anaconda.com/download/">https://www.anaconda.com/download/</a></p>
<p><strong>!!! Important point: the following notebook can be implemented in the Colab, however there are some bugs in the module named âScikit-Optimizeâ.
If any error faced, probably a developement version of this package can solve the problem.</strong></p>
<p>Also, following packages are necessary to install:</p>
<p>GPy==1.10.0</p>
<p>GPyOpt==1.2.6</p>
<p>ipykernel</p>
<p>ipython</p>
<p>ipython-genutils</p>
<p>matplotlib==3.6.3</p>
<p>matplotlib-inline</p>
<p>scikit-image</p>
<p>scikit-learn</p>
<p>scikit-learn-intelex</p>
<p>scikit-optimize==0.9.0</p>
<p>scipy==1.10.0</p>
<p>hyperopt==0.2.7</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/contrib"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Deterministic_Global_Optimization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Deterministic Global Optimization</p>
      </div>
    </a>
    <a class="right-next"
       href="Bayesian_Optimization2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian Optimization Tutorial 2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1.	Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-we-need-bayesian-optimization">1.1.    When We Need Bayesian Optimization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-vs-nonparametric-in-bayesian-optimization">1.2.  Parametric vs. Nonparametric in Bayesian Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-bayesian-optimization-pbo">1.2.1.	Parametric Bayesian Optimization (PBO)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonparametric-bayesian-optimization-nbo">1.2.2.	Nonparametric Bayesian Optimization (NBO)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#method">2.	Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonparametric-bayesian-optimization">2.1. Nonparametric Bayesian Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-process">2.2. Gaussian Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-mean-function">2.2.1.	The prior mean function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-choice-of-kernel-in-gaussian-process">2.2.2.	The Choice of Kernel in Gaussian Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acquisition-function">2.3. Acquisition Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-improvement-pi">2.3.1.	Probability of Improvement (PI)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-improvement-ei">2.3.2.	Expected Improvement (EI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-the-bayesian-optimization-from-scratch">3.	Implementing the Bayesian Optimization from scratch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#libraries-to-perform-bayesian-optimization">4.	Libraries to Perform Bayesian Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpyopt">4.1. GpyOpt</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-optimize">4.2. Scikit-Optimize</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-between-the-results-of-the-custom-bo-model-gpyopt-and-scikit-opt">4.3. Comparison Between the Results of the Custom BO Model, GpyOPT, and Scikit-Opt</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Discussion:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-usage-of-bayesian-optimization">5. Advanced  Usage of Bayesian Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyper-parameter-tuning">5.1. Hyper-Parameter Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-search">5.1.1 Manual Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">5.1.2 Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-search">5.1.3 Random Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-optimization">5.1.4 Bayesian Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-problem-tuning-hyperparameter-in-ml-based-building-response-estimation-model-under-an-earthquake">5.1.5 Real-World Problem: Tuning HyperParameter in ML-Based Building Response Estimation Model Under an Earthquake</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-required-libraries"><strong>Step 1:</strong> Import required libraries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-prepare-the-dataset"><strong>Step 2:</strong> Prepare the dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-define-a-model"><strong>Step 3:</strong> Define a model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-data-visualization"><strong>Step 4:</strong> Data visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-preparing-data"><strong>Step 5:</strong> Preparing Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-defining-the-baseline-model"><strong>Step 6:</strong> Defining the baseline model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-tuning-hyper-parameters"><strong>Step 7:</strong> Tuning Hyper-Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-8-results-and-discussion"><strong>Step 8:</strong> Results and discussion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Discussion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-studies-bayesian-multi-objective-optimization">6. Further Studies (Bayesian Multi-Objective Optimization)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">7. Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-a-required-packages-and-installation-tips">Appendix A. Required Packages and Installation Tips</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander Dowling, et al.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>