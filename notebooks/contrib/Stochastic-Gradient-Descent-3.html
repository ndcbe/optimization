
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Stochastic Gradient Descent Tutorial 3 &#8212; Optimization for Decision Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/contrib/Stochastic-Gradient-Descent-3';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Machine Learning and Applied Statistics" href="data.html" />
    <link rel="prev" title="Stochastic Gradient Descent Tutorial 2" href="Stochastic-Gradient-Descent-2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/cbe_logo.jpg" class="logo__image only-light" alt="Optimization for Decision Science - Home"/>
    <script>document.write(`<img src="../../_static/cbe_logo.jpg" class="logo__image only-dark" alt="Optimization for Decision Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Optimization for Decision Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Organization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/intro.html">Welcome</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../org/syllabus.html">Syllabus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/calendar.html">Fall 2024 Calendar</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/contribute.html">Contribution Instructions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/workshop.html">Computational Optimization in Python (SÃ£o Paulo, Brazil)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/assignments.html">Assignments</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo1.html">Pyomo Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo2.html">Pyomo Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo3.html">Pyomo Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/project1.html">Project 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms1.html">Algorithms Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms2.html">Algorithms Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/project2.html">Project 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms3.html">Algorithms Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Algorithms4.html">Algorithms Homework 4</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../org/archive.html">Archive</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../assignments/Pyomo-Mini-Project.html">Pyomo Mini-Project: Receding Horizon Stochastic Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../org/semester_project.html">Semester Project (Spring 2023)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Optimization Modeling in Pyomo</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../1/getting-started.html">1. Getting Started with Pyomo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../1/Local-Install.html">1.1. Local Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Optimization-Modeling.html">1.2. Optimization Modeling with Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Pyomo-Introduction.html">1.3. Your First Optimization Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/LP.html">1.4. Continuous Optimization: Linear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/NLP.html">1.5. Continuous Optimization: Nonlinear Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/IP.html">1.6. Integer Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1/Pyomo-Nuts-and-Bolts.html">1.7. 60 Minutes to Pyomo: An Energy Storage Model Predictive Control Example</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../2/logic.html">2. Logical Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../2/Logical_Modeling_GDP.html">2.1. Logical Modeling and Generalized Disjunctive Programs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2/Modeling_Disjunctions_Strip_Packing.html">2.2. Modeling Disjunctions through the Strip Packing Problem</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../3/dynamics.html">3. Dynamic Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_car.html">3.1. Pyomo.DAE Example: Race Car</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_TCLab.html">3.2. Pyomo.DAE Example: Temperature Control Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/DAE_background.html">3.3. Differential Algebraic Equations (DAEs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/DAE_numeric_integration.html">3.4. Numeric Integration for DAEs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_theory.html">3.5. Dynamic Optimization with Collocation and Pyomo.DAE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3/PyomoDAE_example.html">3.6. Pyomo.DAE: Racing Example Revisited</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../4/uncertainty.html">4. Optimization Under Uncertainty</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../4/SP.html">4.1. Stochastic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/blocks.html">4.2. Blocks and Other Pyomo Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/AdvancedTopics.html">4.3. Advanced Topics in Stochastic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4/RiskMeasures.html">4.4. Risk Measures and Portfolio Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5/data.html">5. Data Science and Applied Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5/Parmest-tutorial.html">5.1. Parameter estimation with <code class="docutils literal notranslate"><span class="pre">parmest</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../5/Parmest-generate-data.html">5.2. Supplementary material: data for parmest tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5/Pyomo_DoE_Tutorial.html">5.3. Optimizing Experiments with <code class="docutils literal notranslate"><span class="pre">Pyomo.DoE</span></code></a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Algorithms and Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../6/unconstrained.html">6. Unconstrained Nonlinear Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../6/Math-Primer-1.html">6.1. Linear Algebra Review and SciPy Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Math-Primer-2.html">6.2. Mathematics Primer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Optimality.html">6.3. Unconstrained Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Newton-Methods.html">6.4. Newton-type Methods for Unconstrained Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Quasi-Newton-Methods.html">6.5. Quasi-Newton Methods for Unconstrained Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6/Globalization.html">6.6. Descent and Globalization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../7/constrained.html">7. Constrained Nonlinear Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../7/Convexity.html">7.1. Convexity Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Local-Optimality.html">7.2. Local Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/KKT-Multipliers.html">7.3. Analysis of KKT Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Constraint-Qualifications.html">7.4. Constraint Qualifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Second-Order.html">7.5. Second Order Optimality Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/degeneracy_hunter.html">7.6. NLP Diagnostics with Degeneracy Hunter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Interior-Point1.html">7.7. Simple Netwon Method for Equality Constrained NLPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7/Interior-Point2.html">7.8. Inertia-Corrected Netwon Method for Equality Constrained NLPs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../8/special-topics.html">8. Special Topics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../8/MILP.html">8.1. Integer Programming with Simple Branch and Bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../8/MINLP-Algorithms.html">8.2. MINLP Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../8/Global-Opt.html">8.3. Deterministic Global Optimization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Contributions</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="pyomo.html">More Pyomo Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="semiconductor_manufacturing.html">Semiconductor Production Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="student_diet.html">Optimization of Daily Diet Using Pyomo</a></li>
<li class="toctree-l2"><a class="reference internal" href="blending.html">Blending Under Uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="vehicle_routing.html">Vehicle Routing</a></li>

<li class="toctree-l2"><a class="reference internal" href="portfolio_optimization_extended.html">Risk Measures and Portfolio Optimization: Expanded</a></li>
<li class="toctree-l2"><a class="reference internal" href="race_car_extended.html">Extended Race Car Optimization Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="hot_air_balloon.html">Hot Air Balloon Dynamic Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="reactor_design.html">Chemical Reactor Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="Disaster_Response_Plan.html">Disaster Response Plan Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="Sudoku_Solver.html">Sudoku Solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="more_circle_packing.html">Circle Packing Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="modeling.html">Modeling Paradigms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="multi_objective.html">Multi-Objective Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced_stochastic_programming.html">Advanced Topics in Stochastic Programming</a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="algorithms.html">Global Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Deterministic_Global_Optimization.html">Deterministic Global Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Optimization1.html">Bayesian Optimization Tutorial 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Optimization2.html">Bayesian Optimization Tutorial 2</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="sgd.html">Stochastic Gradient Descent</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Stochastic-Gradient-Descent-1.html">Stochastic Gradient Descent Tutorial 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="Stochastic-Gradient-Descent-2.html">Stochastic Gradient Descent Tutorial 2</a></li>






<li class="toctree-l2 current active"><a class="current reference internal" href="#">Stochastic Gradient Descent Tutorial 3</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="data.html">Machine Learning and Applied Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="EM-MAP.html">Expectation Maximization Algorithm and MAP Estimation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ndcbe/optimization/blob/master/notebooks/contrib/Stochastic-Gradient-Descent-3.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ndcbe/optimization" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ndcbe/optimization/issues/new?title=Issue%20on%20page%20%2Fnotebooks/contrib/Stochastic-Gradient-Descent-3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/contrib/Stochastic-Gradient-Descent-3.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Stochastic Gradient Descent Tutorial 3</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">2. Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-algorithm">2.1. Gradient Descent Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-visualization">2.2. Learning rate and visualization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">2.2.1. Visualization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#change-learning-rate">2.2.2. Change learning rate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nonconvex-function">2.2.3. Nonconvex function</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-applications">2.3. More applications</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate">2.3.1. Univariate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bivariate">2.3.2. Bivariate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares">2.3.3. Ordinary Least Squares</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-algorithms">3. Stochastic Gradient Descent Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formulation">3.1. Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-study">3.2. Parametric study</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">3.2.1. Cost function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#change-values-of-alpha">3.2.2. Change Values of <span class="math notranslate nohighlight">\(\alpha\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#change-values-of-batch-size">3.2.3. Change Values of Batch Size</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-cost-analysis">3.3. Computational cost analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#takeaway-conclusion">3.4. Takeaway conclusion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variants-of-sgd">3.5. Variants of SGD</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-between-gd-and-newton-based-method">4. Comparison between GD and Newton-based method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-based-method">4.1. Newton-based method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rosenbrock-function">4.2. Rosenbrock function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-saddle-point">4.3. Single saddle point</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-saddle-points">4.4. Multiple saddle points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">4.5. Takeaway conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-sgd-on-glm-with-real-dataset">5. Implement SGD on GLM with real dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glm-structure">5.1. GLM Structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-wrangling">5.2. Data Wrangling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-the-model">5.3. Developing the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-other-package">5.4. Using other package</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">6. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="stochastic-gradient-descent-tutorial-3">
<h1>Stochastic Gradient Descent Tutorial 3<a class="headerlink" href="#stochastic-gradient-descent-tutorial-3" title="Link to this heading">#</a></h1>
<p><strong>Prepared by:</strong> <a class="reference external" href="https://github.com/hanfengzhang">Hanfeng Zhang</a> (<a class="reference external" href="mailto:hzhang25&#37;&#52;&#48;nd&#46;edu">hzhang25<span>&#64;</span>nd<span>&#46;</span>edu</a>, 2023)</p>
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Nowadays, people are more and more getting used to enjoying the achievements of deep learning. However, for such large-scale machine learning models, we need to deal with optimization problems with millions of training samples. The classical optimization techniques such as interior point methods or gradient descent are not efficient because they have to go through all data points then just evaluate the objective once. Stochastic Gradient Descent (SGD) calculates the gradient using just a random small part of the observations instead of all of them. Although SGD usually takes a higher number of iterations to reach the minima due to the randomness in descents, it is still computationally efficient than the typical gradient descent method. Especially when combined with the backpropagation algorithm, SGD is dominant in neural network training applications.</p>
<p>In this notebook, we want to demonstrate the following topics for readers:</p>
<ul class="simple">
<li><p>what gradient descent algorithm is and how it works</p></li>
<li><p>the difference between gradient descent and stochastic gradient descent (or the essential concepts of minibatch)</p></li>
<li><p>the effects of parameters for stochastic gradient descent</p></li>
<li><p>the comparison between gradient descent and Newton-based methods</p></li>
</ul>
<p>For each example, visualization of the trajectories taken by the optimization algorithm are tried to show.</p>
</section>
<section id="gradient-descent">
<h2>2. Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<p>In this section, we are going to explore and comprehend the fundamental details of gradient descent before talking with âstochasticityâ.</p>
<section id="gradient-descent-algorithm">
<h3>2.1. Gradient Descent Algorithm<a class="headerlink" href="#gradient-descent-algorithm" title="Link to this heading">#</a></h3>
<p>To grasp the concept of the gradient descent algorithm, consider a drop of water trickling down the side of a bowl or a ball rolling down a hill. These objects move towards the direction of the steepest descent until they reach the lowest point, gaining momentum and accelerating over time.</p>
<p>Similarly, the idea behind gradient descent involves starting with an arbitrarily chosen position of the point or vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = \left(\theta_1, \cdots, \theta_{n} \right) \)</span> and iteratively moving it towards the direction of the fastest decrease in the cost function. The negative gradient vector, <span class="math notranslate nohighlight">\(-\nabla \theta\)</span> or <span class="math notranslate nohighlight">\(-G\)</span>, points in this direction.</p>
<p>After selecting a random starting point <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = \left(\theta_1, \cdots, \theta_{n} \right) \)</span>, you update or move it to a new position by subtracting the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> (a small positive value <span class="math notranslate nohighlight">\(0 &lt; \alpha \leq 1\)</span>) multiplied by the negative gradient: <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \rightarrow \boldsymbol{\theta} - \alpha G\)</span>.</p>
<p>Itâs essential to choose an appropriate learning rate as it determines the size of the update or moving step. If <span class="math notranslate nohighlight">\(\alpha\)</span> is too small, the algorithm may converge slowly, while large <span class="math notranslate nohighlight">\(\alpha\)</span> values can cause issues with convergence or make the algorithm diverge.</p>
<!-- [Reference](https://realpython.com/gradient-descent-algorithm-python/) --><p><strong>Pseudocode:</strong></p>
<ul class="simple">
<li><p>Initialize the starting point <span class="math notranslate nohighlight">\(\theta^0\)</span> and the model hyperparameters (e.g., learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> where <span class="math notranslate nohighlight">\(0 &lt; \alpha \leq 1\)</span>)</p></li>
<li><p>In the <span class="math notranslate nohighlight">\(k\)</span>âs iteration:</p>
<ol class="arabic simple">
<li><p>compute the gradient <span class="math notranslate nohighlight">\(\nabla \theta^k\)</span></p></li>
<li><p>update to get <span class="math notranslate nohighlight">\(\theta^{k+1} = \theta^k - \alpha \nabla \theta^k\)</span></p></li>
<li><p>Check convergence: if <span class="math notranslate nohighlight">\(\lvert \theta^{k+1} - \theta^k \rvert &lt; \epsilon\)</span>, then STOP.</p></li>
</ol>
</li>
</ul>
<p><em>Reminder: The pseudocode for SGD is in the next section.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gradient_descent</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function for gradient descent algorithm</span>

<span class="sd">    Arguments:</span>
<span class="sd">        grad = the function or any Python callable object that </span>
<span class="sd">               takes a vector and returns the gradient of the function to minimize</span>
<span class="sd">        start = the points where the algorithm starts its search</span>
<span class="sd">        lr = the learning rate that controls the magnitude of the vector update</span>
<span class="sd">        n_iter = the number of iterations</span>
<span class="sd">        tol = tolerance to stop the algorithm</span>
<span class="sd">        trajectory = flag to return only optimal result or trajectories</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        vector = optimal solution</span>
<span class="sd">        traj = optimization trajectories</span>
<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="c1"># Initializing the values of the variables</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)</span>
    <span class="n">traj</span> <span class="o">=</span> <span class="n">vector</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># Performing the gradient descent loop</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="c1"># Recalculating the difference</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>

        <span class="c1"># Checking if the absolute difference is small enough</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tol</span><span class="p">):</span>
            <span class="k">break</span>
        
        <span class="c1"># Updating the values of the variables</span>
        <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
        <span class="n">traj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">traj</span><span class="p">,</span> <span class="n">vector</span><span class="p">))</span>

    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trajectory</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">vector</span> <span class="k">if</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">vector</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">traj</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="learning-rate-and-visualization">
<h3>2.2. Learning rate and visualization<a class="headerlink" href="#learning-rate-and-visualization" title="Link to this heading">#</a></h3>
<p>Consider a simple convex problem as a ball rolling down a hill,</p>
<div class="math notranslate nohighlight">
\[
\min f(x) = x^2
\]</div>
<p>It is trivial to get its first derivative as:
$<span class="math notranslate nohighlight">\(
\frac{\partial f}{\partial x} = 2x
\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># objective function</span>
<span class="n">function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="c1"># gradient function</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="c1"># implement GD starting from 10 and learning rate = 0.8</span>
<span class="n">gradient_descent</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-4.77519666596786e-07
</pre></div>
</div>
</div>
</div>
<p>We can find the minimizer of the given objective function <span class="math notranslate nohighlight">\(x^* \approx 0\)</span></p>
<section id="visualization">
<h4>2.2.1. Visualization<a class="headerlink" href="#visualization" title="Link to this heading">#</a></h4>
<p>Instead of only looking at the minimizer, we want to know how the gradient descent works, so the visualization is needed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># gradient descent visualization</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gd_visual</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">plot_range</span><span class="o">=</span><span class="mi">11</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function to visualize the process of gradient descent optimization</span>
<span class="sd">    this function is only used for this section for convenience</span>

<span class="sd">    Arguments:</span>
<span class="sd">        func = the function or objective to minimize</span>
<span class="sd">        grad = the gradient of given function or objective</span>
<span class="sd">        start = the starting points for input</span>
<span class="sd">        lr = learning rate</span>
<span class="sd">        n_iter = the number of iterations for visualization</span>
<span class="sd">        tol = tolerance to stop the algorithm </span>
<span class="sd">        plot_range = the range for plotting</span>
<span class="sd">    </span>
<span class="sd">    Returns: the process of optimization</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Define arrays for inputs and outputs</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">start</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iter</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># Initializing the values of the variables</span>
    <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">start</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iter</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># Initializing the values of the variables</span>
    <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">output</span>

    <span class="c1"># Performing the gradient descent loop</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Recalculating the difference</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
        <span class="c1"># Computing the outputs of current step</span>
        <span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
        
        <span class="c1"># Updating the values of the variables</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tol</span><span class="p">):</span>
            <span class="k">break</span>
        
        <span class="c1"># Updating the vector and store to (i+1)&#39;th</span>
        <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">diff</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;current local minimizer = &#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>
    
    <span class="c1"># plot the function and trajectory</span>
    <span class="n">xlist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">plot_range</span><span class="p">,</span> <span class="n">plot_range</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xlist</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">xlist</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Function&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">outputs</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Trajectory&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;f(x)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now choose starting point <span class="math notranslate nohighlight">\(x = 10\)</span> and learning rate <span class="math notranslate nohighlight">\(\alpha = 0.8\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd_visual</span><span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">plot_range</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current local minimizer =  [2.2107392e-06]
</pre></div>
</div>
<img alt="../../_images/bc114f1419bdcd7630c6915dc13f8cc41073283688c562e1ef0398c9a646b9ab.png" src="../../_images/bc114f1419bdcd7630c6915dc13f8cc41073283688c562e1ef0398c9a646b9ab.png" />
</div>
</div>
<p>It starts from the rightmost red dot (<span class="math notranslate nohighlight">\(x=10\)</span>) and move toward the minimum (<span class="math notranslate nohighlight">\(x=0\)</span>). The updates are larger at first because the value of the gradient (and slope) is higher. As approaching the minimum, updates become lower.</p>
</section>
<section id="change-learning-rate">
<h4>2.2.2. Change learning rate<a class="headerlink" href="#change-learning-rate" title="Link to this heading">#</a></h4>
<p>The learning rate is a very important parameter of the algorithm. Different learning rate values can significantly affect the behavior of gradient descent. Consider the previous example, but with a learning rate of 0.8 instead of 0.2:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd_visual</span><span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">plot_range</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current local minimizer =  [-4.77519667e-07]
</pre></div>
</div>
<img alt="../../_images/f3ba821775393769de6bef0d9f38b4429e25dabda2ac98b53b5624b78c870278.png" src="../../_images/f3ba821775393769de6bef0d9f38b4429e25dabda2ac98b53b5624b78c870278.png" />
</div>
</div>
<p>In this case, it again starts with <span class="math notranslate nohighlight">\(x=10\)</span>, but because of the high learning rate, it gets a large change in <span class="math notranslate nohighlight">\(x\)</span> that passes to the other side of the optimum and becomes â6. It crosses zero a few more times before convergence.</p>
<p>So what happens if we change learning rate to a very small value (e.g., <span class="math notranslate nohighlight">\(\alpha = 0.005\)</span>)?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd_visual</span><span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">plot_range</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current local minimizer =  [6.17290141]
</pre></div>
</div>
<img alt="../../_images/b9627b19673bea3a74f3ddac7e414342979e878fe3ddbf8102caafe8fffb9cba.png" src="../../_images/b9627b19673bea3a74f3ddac7e414342979e878fe3ddbf8102caafe8fffb9cba.png" />
</div>
</div>
<p><strong>Question:</strong> To find the local minimizer, what do we need to change except changing learning rate?</p>
<p><em>Hint: look at the arguments of the codes.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd_visual</span><span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">plot_range</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current local minimizer =  [9.95251885e-05]
</pre></div>
</div>
<img alt="../../_images/c299862ae4424d626537b222af20b06b01143064f9dc02c729672dc5a332afab.png" src="../../_images/c299862ae4424d626537b222af20b06b01143064f9dc02c729672dc5a332afab.png" />
</div>
</div>
</section>
<section id="nonconvex-function">
<h4>2.2.3. Nonconvex function<a class="headerlink" href="#nonconvex-function" title="Link to this heading">#</a></h4>
<p>Nonconvex functions might have local minima or saddle points where the algorithm can get trapped. In such situations, our choice of learning rate or starting point can make the difference between finding a local minimum and finding the global minimum.</p>
<p>Consider a new optimization problem:</p>
<div class="math notranslate nohighlight">
\[
\min f(x) = x^4 - 5x^2 -3x
\]</div>
<p>It has a global minimizer at <span class="math notranslate nohighlight">\(x \approx 1.6\)</span> and a local minimizer at <span class="math notranslate nohighlight">\(x \approx -1.42\)</span>. The gradient of this function is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x} = 4x^3 - 10x -3
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define new function and gradient</span>
<span class="n">function2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span>
<span class="n">gradient2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">10</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span><span class="mi">3</span>

<span class="n">gd_visual</span><span class="p">(</span><span class="n">function2</span><span class="p">,</span> <span class="n">gradient2</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">plot_range</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current local minimizer =  [-1.63120206]
</pre></div>
</div>
<img alt="../../_images/fb204ee84216ed6c511b7a0b2b6e3a1f98a724f7dc128138aefd9090d689ca55.png" src="../../_images/fb204ee84216ed6c511b7a0b2b6e3a1f98a724f7dc128138aefd9090d689ca55.png" />
</div>
</div>
<p>We started at zero this time, and the algorithm ended near the local minimum. During the first two iterations, the vector was moving toward the global minimum, but then it crossed to the opposite side and stayed trapped in the local minimum.</p>
<p>However, we can prevent this with a smaller learning rate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd_visual</span><span class="p">(</span><span class="n">function2</span><span class="p">,</span> <span class="n">gradient2</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">plot_range</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>current local minimizer =  [1.03933248]
</pre></div>
</div>
<img alt="../../_images/a109bacb2dcc6b4378faa1232fabdd0d5a175b0e7fb7b64cfffe6de3bfe5d4cd.png" src="../../_images/a109bacb2dcc6b4378faa1232fabdd0d5a175b0e7fb7b64cfffe6de3bfe5d4cd.png" />
</div>
</div>
<p>When decreasing the learning rate from 0.2 to 0.1, we get a solution very close to the global minimum. Remember that gradient descent is an approximate method. This time, we avoid the jump to the other side. A lower learning rate prevents the vector from making large jumps, and in this case, the vector remains closer to the global optimum.</p>
<p>Adjusting the learning rate is tricky. We usually canât know the best value in advance. There are many techniques and heuristics that try to help with this issue. In addition, machine learning practitioners often tune the learning rate during model selection and evaluation. Moreover, besides the learning rate, the starting point can affect the solution significantly, especially with nonconvex functions.</p>
</section>
</section>
<section id="more-applications">
<h3>2.3. More applications<a class="headerlink" href="#more-applications" title="Link to this heading">#</a></h3>
<section id="univariate">
<h4>2.3.1. Univariate<a class="headerlink" href="#univariate" title="Link to this heading">#</a></h4>
<p>Consider the problem:</p>
<div class="math notranslate nohighlight">
\[
\min f(x) = x - \log{x}
\]</div>
<p>The gradient of this function is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x} = 1 - \frac{1}{x}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0000011077232125
</pre></div>
</div>
</div>
</div>
<p>With the provided set of arguments, <code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> correctly calculates that this function has the minimum at <span class="math notranslate nohighlight">\(x=1\)</span>. We can try it with other values for the learning rate and starting point.</p>
</section>
<section id="bivariate">
<h4>2.3.2. Bivariate<a class="headerlink" href="#bivariate" title="Link to this heading">#</a></h4>
<p>We can also use <code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> with functions of more than one variable. The application is the same, but we need to provide the gradient and starting points as vectors or arrays.</p>
<p>For example, we can find the minimum of the problem:</p>
<div class="math notranslate nohighlight">
\[
\min f(x_1, x_2) = x_1^2 + x_2^4
\]</div>
<p>With the gradient vector <span class="math notranslate nohighlight">\(\left(2x_1, 4x_2^3 \right)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span><span class="p">]),</span> 
                 <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([8.08281277e-12, 9.75207120e-02])
</pre></div>
</div>
</div>
</div>
<p>In this case, our gradient function returns an array, and the start value also is an array, so we would get an array as the result. The resulting values are almost equal to zero, so we can say that <code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> correctly found that the minimum of this problem is at <span class="math notranslate nohighlight">\(x_1 = x_2 = 0\)</span>.</p>
</section>
<section id="ordinary-least-squares">
<h4>2.3.3. Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Link to this heading">#</a></h4>
<p>Linear regression and the ordinary least squares (OLS) method start with the observed values of the inputs <span class="math notranslate nohighlight">\(\boldsymbol{x} = \left(x_1, \cdots, x_n\right)\)</span> and outputs <span class="math notranslate nohighlight">\(y\)</span>. They define a linear function <span class="math notranslate nohighlight">\(f(\boldsymbol{x}) = b_0 + b_1 x_1 + \cdots + b_n x_n\)</span>, which is as close as possible to observation <span class="math notranslate nohighlight">\(y\)</span>. This is an optimization problem. It finds the values of weights <span class="math notranslate nohighlight">\(b_0, b_1, \cdots, b_n\)</span> that minimize the sum of squared residuals <span class="math notranslate nohighlight">\(\text{SSR} = \sum_{i} \left(y_i - f(\boldsymbol{x_i})\right)^2\)</span>or the mean squared error <span class="math notranslate nohighlight">\(\text{MSE} = \text{SSR} / {n}\)</span>. Here, <span class="math notranslate nohighlight">\(n\)</span> is the total number of observations and <span class="math notranslate nohighlight">\(i = 1, \cdots, n\)</span>. We can also use the cost function <span class="math notranslate nohighlight">\(\mathcal{L} = \text{SSR} / (2n)\)</span>, which is mathematically more convenient than SSR or MSE.</p>
<p>The most basic form of linear regression is simple linear regression. It has only one set of inputs <span class="math notranslate nohighlight">\(x\)</span> and two weights: <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>. The equation of the regression line is <span class="math notranslate nohighlight">\(f(x) = b_0 + b_1 x\)</span>. Although the optimal values of <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> can be calculated analytically, we could implement gradient descent to determine them.</p>
<p>First, use calculus to find the gradient of the cost function <span class="math notranslate nohighlight">\(\mathcal{L} = \sum_{i} \left(y_i - b_0 - b_1 x_i\right)^2 / (2n)\)</span>. Since we have two decision variables, <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>, the gradient <span class="math notranslate nohighlight">\(\nabla C\)</span> is a vector with two components:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial b_0} = \frac{1}{n} \sum_{i} \left(b_0 + b_1 x_i - y_i\right) = \text{mean} \left(b_0 + b_1 x_i - y_i\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial b_1} = \frac{1}{n} \sum_{i} \left(b_0 + b_1 x_i - y_i\right)x_i = \text{mean} \left(\left(b_0 + b_1 x_i - y_i\right) x_i \right)\)</span></p></li>
</ol>
<p>We need the values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> to calculate the gradient of this cost function. Our gradient function will have as inputs not only initial <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> but also <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> as data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the SSR of linear regression model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">ssr_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function to calculate the gradient of SSR</span>

<span class="sd">    Arguments:</span>
<span class="sd">        x = the given value for features</span>
<span class="sd">        y = the observations</span>
<span class="sd">        theta = parameter vector for linear model</span>
<span class="sd">    </span>
<span class="sd">    Returns: the gradient of SSR</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Number of data points</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Insert unit column to X feature matrix</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

    <span class="c1"># Compute residual between prediction and observation</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">x_new</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ssr_gradient</span></code> takes the arrays of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, which contain the inputs and outputs, and the array <span class="math notranslate nohighlight">\(b\)</span> that holds the current values of the decision variables <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>. This function first calculates the array of the residuals for each observation and then returns the pair of values of <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial b_0\)</span> and <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial b_1\)</span>.</p>
<p>Instead of the only objective function, we need to deal with feature <span class="math notranslate nohighlight">\(x\)</span> and observation <span class="math notranslate nohighlight">\(y\)</span>, so there are some small changes from <code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> to the following <code class="docutils literal notranslate"><span class="pre">gd_OLS</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># For the OLS example, we need to fix our gradient_descent</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gd_OLS</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tor</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function of gradient descennt method for OLS</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        grad = the gradient of given function or objective</span>
<span class="sd">        x = arrays of inputs/features</span>
<span class="sd">        y = arrays of outputs/observations</span>
<span class="sd">        start = the starting points for input</span>
<span class="sd">        lr = learning rate</span>
<span class="sd">        n_iter = the number of iterations for visualization</span>
<span class="sd">        tol = tolerance to stop the algorithm</span>

<span class="sd">    Returns:</span>
<span class="sd">        optimal solution for OLS</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Initializing the values of the variables</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">start</span>

    <span class="c1"># Performing the gradient descent loop</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="c1"># Recalculating the difference</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">vector</span><span class="p">))</span> <span class="c1">### changed line</span>
        
        <span class="c1"># Checking if the absolute difference is small enough</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tor</span><span class="p">):</span>
            <span class="k">break</span>
        
        <span class="c1"># Updating the values of the variables</span>
        <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>
    
    <span class="k">return</span> <span class="n">vector</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> now accepts the inputs <span class="math notranslate nohighlight">\(x\)</span> and outputs <span class="math notranslate nohighlight">\(y\)</span> and can use them to calculate the gradient. Converting the result of <code class="docutils literal notranslate"><span class="pre">grad(x,</span> <span class="pre">y,</span> <span class="pre">vector)</span></code> to a <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> array enables elementwise multiplication of the gradient elements by the learning rate, which is not necessary in the case of a single-variable function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># use gd_OLS to find the regression line for some arbitrary values of x and y</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">55</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">38</span><span class="p">])</span>

<span class="n">gd_OLS</span><span class="p">(</span><span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100_000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5.62822349, 0.54012867])
</pre></div>
</div>
</div>
</div>
<p>The result is an array with two values that correspond to the decision variables: <span class="math notranslate nohighlight">\(b_0 = 5.63\)</span> and <span class="math notranslate nohighlight">\(b_1 = 0.54\)</span>. The best regression line is <span class="math notranslate nohighlight">\(f(x) = 5.63 + 0.54x\)</span>. As in the previous examples, this result heavily depends on the learning rate. You might not get such a good result with too low or too high of a learning rate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization for the given OLS example</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gd_OLS_visual</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Visualization function just for OLS </span>

<span class="sd">    Arguements:</span>
<span class="sd">        grad = the gradient of given function or objective</span>
<span class="sd">        x = arrays of inputs/features</span>
<span class="sd">        y = arrays of outputs/observations</span>
<span class="sd">        start = the starting points for input</span>
<span class="sd">        lr = learning rate</span>
<span class="sd">        n_iter = the number of iterations for visualization</span>
<span class="sd">        tol = tolerance to stop the algorithm</span>

<span class="sd">    Returns: plots between GD regression results and observations</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Initializing the values of the variables</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">start</span>

    <span class="c1"># Performing the gradient descent loop</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="c1"># Recalculating the difference</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">vector</span><span class="p">))</span> <span class="c1">### changed line</span>

        <span class="c1"># Checking if the absolute difference is small enough</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">tol</span><span class="p">):</span>
            <span class="k">break</span>
        
        <span class="c1"># Updating the values of the variables</span>
        <span class="n">vector</span> <span class="o">+=</span> <span class="n">diff</span>

    <span class="c1"># plot the given x and y</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observation&#39;</span><span class="p">)</span>

    <span class="c1"># plot the regression line</span>
    <span class="n">xlist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">vector</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">vector</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xlist</span><span class="p">,</span> <span class="n">reg</span><span class="p">(</span><span class="n">xlist</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Regression&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd_OLS_visual</span><span class="p">(</span><span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100_000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/10a1946a9b56efafffd6e90f3fb64c8c026d07c84ea99c8989ecb5ec4f72d5df.png" src="../../_images/10a1946a9b56efafffd6e90f3fb64c8c026d07c84ea99c8989ecb5ec4f72d5df.png" />
</div>
</div>
</section>
</section>
</section>
<section id="stochastic-gradient-descent-algorithms">
<h2>3. Stochastic Gradient Descent Algorithms<a class="headerlink" href="#stochastic-gradient-descent-algorithms" title="Link to this heading">#</a></h2>
<p>A recurring problem in machine learning is that large training sets are necessary for good generalization, but large training sets are also more computationally expensive. On the contrary, the stochastic gradient descent samples a <strong>minibatch</strong> of examples drawn uniformly from the training set on each updating step, which is the reason named as â<strong>stochastic</strong>â.</p>
<p>To be mentioned, there are several different definitions and classifications for SGD, here we follows the definition in <strong>Section 5, <a class="reference external" href="https://www.deeplearningbook.org/">Deep Learning</a> by Goodfellow, et al</strong>, which call gradient descent on minibatches as stochastic gradient descent. Stochastic approaches is that not only is the gradient over few examples cheaper to compute, not getting the exact direction (using only a small number of examples), but actually leads to better solutions. This is particularly true for large datasets with redundant information wherein the examples might not be too different from each other. Another reason for stochastic approaches performing better is the presence of multiple local minima with different depths. In a sense, the noise in the direction allows for jumping across the trenches while a fullbatch approach will converge in the trench it started with.</p>
<p><img alt="minibatch.png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABCYAAAJKCAYAAADwTUQkAAABfGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGAqSSwoyGFhYGDIzSspCnJ3UoiIjFJgv8PAzcDDIMRgxSCemFxc4BgQ4MOAE3y7xsAIoi/rgsxK8/x506a1fP4WNq+ZclYlOrj1gQF3SmpxMgMDIweQnZxSnJwLZOcA2TrJBUUlQPYMIFu3vKQAxD4BZIsUAR0IZN8BsdMh7A8gdhKYzcQCVhMS5AxkSwDZAkkQtgaInQ5hW4DYyRmJKUC2B8guiBvAgNPDRcHcwFLXkYC7SQa5OaUwO0ChxZOaFxoMcgcQyzB4MLgwKDCYMxgwWDLoMjiWpFaUgBQ65xdUFmWmZ5QoOAJDNlXBOT+3oLQktUhHwTMvWU9HwcjA0ACkDhRnEKM/B4FNZxQ7jxDLX8jAYKnMwMDcgxBLmsbAsH0PA4PEKYSYyjwGBn5rBoZt5woSixLhDmf8xkKIX5xmbARh8zgxMLDe+///sxoDA/skBoa/E////73o//+/i4H2A+PsQA4AJHdp4IxrEg8AAAGeaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjEwNjI8L2V4aWY6UGl4ZWxYRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpQaXhlbFlEaW1lbnNpb24+NTg2PC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+ClwXhG0AAEAASURBVHgB7N0NvFxlfSD+X94vSUhCkiaEJEAEokCB1OILBcRUsWXxhaqUylZE2iIrrdqFYlf+XUUXV8FspSsUsYuKrfzRdksFPrQqiy8oCiyCKSgRCJAESCQhCUm8ed/zzL1zMzP35s7cOzN3ztz5ns8nmTlvz/Oc7++cO3N+c85zxuzNhjAQIECAAAECBAgQIECAAAECBFogMLYFdaqSAAECBAgQIECAAAECBAgQIFAQkJiwIxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQkJiwDxAgQIAAAQIECBAgQIAAAQItE5CYaBm9igkQIECAAAECBAgQIECAAAGJCfsAAQIECBAgQIAAAQIECBAg0DIBiYmW0auYAAECBAgQIECAAAECBAgQGI+AQDsLfO1rX4unn346fu3Xfi3OP//8dt6U3LSdaW5CoSEECBAgQIAAAQIEOkJgzN5s6IgttZGjTmDPnj0xbty4vu1auXJlHH744X3j3gxdgOnQzaxBgAABAgQIECBAgEB9Aq6YGKJfOnFL/9IwduzYwr8hFjHkxXft2tW3zvjx1UN22223xapVq2L27Nnxzne+c0Ta2NfAAd40qz1jxowZoDaT6hFgWo+edQkQIECAAAECBAgQGI6APiaGoPbAAw8UfqGfMGFCpH/p1/q3vOUtQyhhaIum+tKJYrG+9Pqxj31s0EKuu+66eOtb3xoXX3xxnHPOOfHxj3980OWbPTNv7Wn29iqfAAECBAgQIECAAAECBIYmIDExBK+7776739K333571WRBv5VqmJCuynjVq17Vb8kvf/nL/aYNNmHOnDmDzR7xeXlrz4gDqJAAAQIECBAgQIAAAQIEygQkJso4Bh+ZMmXKgAtcccUVfbd3DLjAMCZef/31w1grYuvWrWXrrVu3rmx8pEfy1p6R3v5q9aXbdN73vvfFueeeG5deemm1xc0nQIAAAQIECBAgQIDAqBOQmBhGSGfMmNFvrTvuuKPftHompFsx0jBQXYOVm55MUVwnvabxVg55a08rLQaqe/Xq1XHDDTfEzTffHMuWLWt4gmugOk0jQIAAAQIECBAgQIBAngSq96SYp9bmpC0bN24snPyn1+Jw3nnnxYsvvlgcrev1nnvu6Vs/1fG6170uvve97/VNG+xNemxmo9oxWD21zstbe2pt90gtl/oNMRAgQIAAAQIECBAgQKCTBVwxMczon3LKKXHttdf2rZ0SCE899VTfeD1v3v3ud/et/q53vavQkWXfBG8IECBAgAABAgQIECBAgMAoEpCYGGYw0+M4/+iP/qhs7T/7sz8rGx/OyJo1a8oSHNdcc01s27ZtOEVZhwABAgQIECBAgAABAgQI5F5AYmKYIdq0aVNMmjSpcJtFsYj0hI7t27cXR4f1etFFF/Wtl/qISLdC5CExsXfv3r52teObZrW/3nInTpxYxjl2bGMPyXrbV9a4ipFmlp2qSuU3o45mlFlBY5QAAQIECBAgQIAAgSEINPYsaAgVj5ZFv/CFL5Rtyv/6X/+rbHwoIykBkZIbxeGmm24qvh3Sa3rCw2mnnRZvectbIj31YbChuGx6LR1SWz72sY/FQQcdFGPGjIl0wpxeFy1aFNddd13VckvLKtZRS3tK16vnfUoQXX311YX2lrY/bU96+sUvf/nLIRefTG677baCayqztNz0Ppmn+elRr/sb0klxqj/9S76nn3562aLpCR3F+aWvX/va18qWG2gklf3tb397wPaluCWP4Wx3sa5G7hPFMktf09VCaZtTW4u2xf1uyZIlkQyq7c+l5RXfP/DAA/s1STHIU58sxTZ7JUCAAAECBAgQINBRAtnJjKFGgaxPiXTZQOHf4Ycf3rdWcVrxtW/GEN989KMf7Ss/lZWd4BZK2F+9AxW/e/fusjIee+yxgRYrTKtcNo2nobIdxe2qfP3+979fWH6w/yrrGKw9g5Uz0LzkU9qm7AkXhcVKvUrnV75/85vfPFCx/aalck844YSyuirLqhz/6U9/2q+cNGHlypVDKqe03OL+MFDBKRalyw72PutMdW/2GNeBiinsc6XrFk0buU9UVrx58+a96XgqrXew91/84hcrixhwPJWbXXVUU7m17gsDVmQiAQIECBAgQIAAAQJ1CbhiIjsDqnfITpTKili+fHnZeC0jWRTjiiuu6Fs0OxEs/GrcN6HGN+mX5tKh8laB0nmVy+7cubPwa3VpO0qXr3x/6qmnRvo1erChso7B2jNYObXM27BhQ+GqheKjVqutk65OSb/ODzakJ6QsWLAgHn744cEW6zfv+OOPH9CmnqdwVFoWK01XQqRY1DqkJ7xMmTKlpisFkmm6WqGR+0RpO1OHsdOmTSvrV6V0/kDv3/ve90a6smSwIV0xk8otfXJOcfni43SL4+k17QuunCgV8Z4AAQIECBAgQIDAyAlITDTA+g//8A/LSil9qkbZjEFGSm/hSItddtllgyzdnFldXV1lJ4gXXnhhZL86993rn05S06NLS4dXvepVg966ULpss9+nZEDpY1VTW7Nf/AvtS4mf7u7uuOqqq8qakU6MBzvJ/fSnP122fCrz/vvvj5TESWWmf9lVIZFdCRLZr/5lyyabymH+/PmFNmVXThReS5/skpZNZad5lf9SHAYa0u0NlftKilt2RURf+9J2f+tb3+q3eupYtdqQTEuTMo3cJ9JtJZWJoezKlIJlMi36pm1P9ZYON9xwQ+GWotJppe/f9KY3lY7GJZdc0meSEhDpdpsUs9L9eTi3iZRVYoQAAQIECBAgQIAAgeEJZF/+DTUKlN4ikC49Lx2yx3qWXTKeLiMfylB6yXnlZeWD1VtZR+XtDenWgf0Nlctme1DfNgx2y0Xltt5yyy37q6LfrQGDtWe/hexnxmDtH+w2k9SG0m1N77Oky4C1ZCfEhWWzE9j9LlO6YlqutOzB2pHWW7duXdnyaZtqHbLkSNm6qd5q9aXbIIrty5I0/aoazLRR+0Sx0tJ9PrUp7eeDDan+YtuLr8mgcki3qRTnp9eBtrN0nVRudoVS6STvCRAgQIAAAQIECBAYQQFXTGRnLo0Y0uX0pUOtl76ndVasWFF2yfn1119fWtSIv09XRixevHi/9f7DP/xD2bzBrjgoW3CERrKT/TjllFP2W1u6sqHy9psrr7xywOX/9m//NpLHd7/73UJHoAMuVDLxzjvvLBmLuPfee8vGK0d27NhRNik79svGBxv5b//tv5XNTldFDLbdaeHzzz+/cKVGulLgpJNOKlt/sJFG7xPpFqDS2yzSlSzvf//7B2tCYZ/Mkghly6QrJyqHFP/SIV0tMdiQ9vXUCaaBAAECBAgQIECAAIHWCEhMNMg9XaJfeu/6smXLCpei11L8n/zJn/QtlspIZbVqSCeg6ckVgw2pr4PsF+a+RdIJZvZLe994K9+kWzfSI1arDekEvXRI8RpoSE+FqOZRut7kyZMjJT6KQ+mtJcVpjXotTX6lOt/4xjfWVHTav1KipVoSo1hYM/aJs88+u1h84fUv/uIvysb3N5KSCKW+A/UnUtmPx/765thfHaYTIECAAAECBAgQIDCyAhITDfSufLznXXfdVbX0l156qaxfhPS4yVYN2W0ANZ+ElyZTWtXeynrTFQNDSepU9jeRHlfZiOE973lPXzH//u//3ve+kW8q21r52NpG1dWMfSI9djT17VEcsluBim9rev3GN75RtlxlUiz1/1E6bNmypXTUewIECBAgQIAAAQIEciYgMdHAgJx55pllpVX+Klw2s3ek9FfvNKnWX7EHKqveaenpE7UOlSd/qYPFVg9HH330kJpQGZ/KbRpSYSULz5kzp2SsOW8rO7McylM5htKiZuwTTzzxRFkT3va2t5WNVxuZOXNm2SLr168vG583b17ZeHo6h44ty0iMECBAgAABAgQIEMiVgMREA8ORLvsvvZ893eJQ+ct2aXXpl97SWwgqn9BQumze3leefFfe19+K9g41sXDooYeWNfPrX/962XieR0qvxEi3NkyaNKnlza11n/jXf/3XsraWPoEj9bFR7V9lnNMTSEqHZJF1IFs6KdLtHd/+9rfLphkhQIAAAQIECBAgQCAfAuPz0YzR04rLL7+8LNmQ7p3/6le/OuAG/uM//mPZ9D/6oz8qG8/zyAEHHJDn5tXUtuH0PZB+eX/ooYfi7rvvLtyCs2rVqti0aVNZfaW3KZTNaODIP/3TP/WVNn369L73rXxT6z4xZcqUsmZWjpfNHObIv/zLv8S4cePK1j799NML4+kWntQ3ReoPxECAAAECBAgQIECAQOsFXDHR4BikjhJPOOGEvlJvvvnm/V5Gfs455/Qtlz2CMxe/evc1qAPfDNZp5osvvhhvectbCr+8v+pVr4rLLrssbr/99nj44YcjJSJK/40EXWkyojIxMhL111PHk08+Wc/qNa2brl5KVySlp49UDil2KRly2mmnDXpFU+V6xgkQIECAAAECBAgQaI6AKyaa4Pp3f/d3kU5ei8Pf//3fFx7TWBxPr8uXLy8djWuuuaZs3EjzBSofzZk6ZRxouO666wq/sA80z7ShC1TeUlGayBtqacccc0wcdthhA66WrohJTx9Jt1NddNFFhURS6YLpiSmpD42UvEhXwKRkhoEAAQIECBAgQIAAgZEXkJhogvmJJ55YVup73/vefomJd7/73X3LpD4CBvu1vm9BbxoqUHkrR2VfBamy9JSUykdSphPpz33uc4UrY9Iv76XlpPcf+9jHorJT04Y2PCus3a6SKN3+Cy+8sMz0Jz/5SZlh6bKNeJ+e1JLimBJPqR+XdMVE6ZASFOm2j9TXheREqYz3BAgQIECAAAECBEZGwE+ETXKufBRl6RUS6baAdAtAcWinTheLbR6Nr5W/vKf+JN761reWber9999f6GMiPT3lwAMPLJzIpmRE8V9auLITyLICGjTyjne8o6+k0ts6+ia20ZvKK1ea1fTUp0Tq8yXVV/nI0VTn0qVLm1W1cgkQIECAAAECBAgQGERAYmIQnHpmVf7KXnqFROW8yiss6qnXurULvPDCC2ULn3zyyWXjd955Z9l4OpnNS6xe9rKX9bUtJblG6uS+r9I63syePbts7dQXxEgPqb+QykfcpisnWtGWkd529REgQIAAAQIECBDIm4DERJMikn6dLe14L508pkvJ04lP6hCzOHzxi18svvVap0B6JORQhsorVSofQ5meuFE6VD6CsnReq98//fTTrW5CzfVXSwDVXFCdC6bHij722GNlpTzzzDNl40YIECBAgAABAgQIEGi+gMREE42/8IUvlJX+pS99KSofEfqHf/iHZcsYGb7AmWeeOaSV06NdS4fU10fpcMstt/SNpnmlfUn0zRjgzUB9VQywWGFSZTKk1nXPPvvssiI/8pGPlI3neeSQQw4pa94HPvCBsvGRHDnyyCPLqmv322LKNsYIAQIECBAgQIAAgTYRkJhoYqAWL15cVnq6haP0EaGpE8Dx4/U/WoZUx0i6KiU9ArKW4Z577omNGzf2LVrZJ0iaUXqFRHocaC23S6S+RCo7V+yrZIA3lf1RPPfccwMs1X9S6ix1xowZfTPSVTgvvfRS3/hgb9J2XH311fHAAw8MtljT5qUET9r3i0OybVVbKjsRTf2KGAgQIECAAAECBAgQGFkBiYkmew92q8anPvWpJtfeecWnfgKWLFkyaBIhncCfeuqpZTiV/X6kmemJG6VD5a0fpfPS+/QYzOOPP75y8qDjBxxwQNn8G264oWx8sJGbbrqpbPa0adOq9pGQOl5NT55IyZNLLrmkbP2RHKnc99PjdVOCotYhJRBSMmOgPiFS/C+99NIB51WWXxn3WbNmVS5inAABAgQIECBAgACBJgtITDQZeH+3aqRHTh500EFNrr0zi09XTqST79InoRQlVqxYEekEvnRIJ+ipT5DK4V3velfZpHS1y9e+9rWyaWlk+/btce6558bpp5/eb161CenqgdIrH5YtW9av3WvWrBnwpD114Fg5pMdeDrTdqX+TdLI+c+bMylVaMp72/dKrJlIjFi1aVLiSI3kONKQrPVLyIlmn/kRSMuP6668vWzQlKlL8k2OyuO666wrxKVsoG0mJjVROaX8v6QoZjwutlDJOgAABAgQIECBAoPkC7iNosnG6VSOd4JaeAKUqv/KVrzS55s4rvtK5ePVCMcGQnrJRevtGEkoJos985jMDYqWT58oyU3Ii/Usn1enKi0cffbTs0a8DFlRl4l//9V/He9/73r6lUrvTSXJ6HGnpfjPQrSTr1q3r93jS4nanzldTEqbytpViRX/1V39VfNuS189//vPx85//PNJVLsUhXcmR/qVkzRlnnBGpP4pnn3027r333gGTM8X19vearohI/1IfIW9605sKi33zm98csKx//ud/3l8xphMgQIAAAQIECBAg0ESBMdnJzt4mlj+qik6/vhYv/U4nOitXrqxp+9Iv3gsWLChbdijsQ6k3lVv6q29qY2rrQMNQlq1cv9Z1a12usvxaxivLTuPpdopar1xIJ+7f/e53q1aVbg1Iv8LXMqRbd1InnMW+I2rdT1ISpDJpUllf2r6Bhl/+8pd99Q00f6Bp999//4CPPq00HWz/qSx3uOumKznSFQ7DGdIjP9PTNUqH4ZS3YcMGVzCVInpPgAABAgQIECBAYAQF3MoxBOzSy/2H0nv//Pnzy5ID3/jGN4ZQa5TdZjCUelMlQ1l+KMtWbkBlfwyV84vj9dRRLGN/r+nE+I1vfGOkJ1uUdlxZuXy6SiKdmNeSlEjrPvTQQ5FiVnrLRWWZ6cqKzZs3x/nnn1/Woemhhx5aueiA46nvh8pbG0oXHKyvktQRZrqF4dprry1dpd/7tN3pSSNp2RNPPLHf/IEm1BOvWveJdMVKSgwMtv3FtqUYpFtvUsIkxbsyKZGWS+WtXr26cLVLcb2BXlNZyTWV47aqgYRMI0CAAAECBAgQIDAyAq6YGKJzsbO90qsSai1ipNZNJ1rpX+q/oNojLoeybOV21rpurctVll/LeLHsgeKR+hFYu3ZtoZj0WM7UL8HcuXPLEge11FG6TOr/oPTJGenke/bs2f2cB2tXaXmV79N66cqJ1CdEGtKtGFOnTu1XfuV6peNp3XSLR3EY6nYX217L/lOso/haz7rFMort37FjR6ED0pRoSsmXlCQZKM7F9fb3mm65Wb9+fSH+qaxizAZKauyvDNMJECBAgAABAgQIEGiegMRE82yVTIAAAQIECBAgQIAAAQIECFQRcCtHFSCzCRAgQIAAAQIECBAgQIAAgeYJSEw0z1bJBAgQIECAAAECBAgQIECAQBUBiYkqQGYTIECAAAECBAgQIECAAAECzROQmGierZIJECBAgAABAgQIECBAgACBKgISE1WAzCZAgAABAgQIECBAgAABAgSaJyAx0TxbJRMgQIAAAQIECBAgQIAAAQJVBCQmqgCZTYAAAQIECBAgQIAAAQIECDRPQGKiebZKJkCAAAECBAgQIECAAAECBKoISExUATKbAAECBAgQIECAAAECBAgQaJ6AxETzbJVMgAABAgQIECBAgAABAgQIVBGQmKgCZDYBAgQIECBAgAABAgQIECDQPAGJiebZKpkAAQIECBAgQIAAAQIECBCoIiAxUQXIbAIECBAgQIAAAQIECBAgQKB5AhITzbNVMgECBAgQIECAAAECBAgQIFBFQGKiCpDZBAgQIECAAAECBAgQIECAQPMEJCaaZ6tkAgQIECBAgAABAgQIECBAoIqAxEQVILMJECBAgAABAgQIECBAgACB5glITDTPVskECBAgQIAAAQIECBAgQIBAFQGJiSpAZhMgQIAAAQIECBAgQIAAAQLNE5CYaJ6tkgkQIECAAAECBAgQIECAAIEqAhITVYDMJkCAAAECBAgQIECAAAECBJonIDHRPFslEyBAgAABAgQIECBAgAABAlUEJCaqAJlNgAABAgQIECBAgAABAgQINE9AYqJ5tkomQIAAAQIECBAgQIAAAQIEqghITFQBMpsAAQIECBAgQIAAAQIECBBonoDERPNslUyAAAECBAgQIECAAAECBAhUEZCYqAJkNgECBAgQIECAAAECBAgQINA8AYmJ5tkqmQABAgQIECBAgAABAgQIEKgiIDFRBchsAgQIECBAgAABAgQIECBAoHkCEhPNs1UyAQIECBAgQIAAAQIECBAgUEVAYqIKkNkECBAgQIAAAQIECBAgQIBA8wQkJppnq2QCBAgQIECAAAECBAgQIECgioDERBUgswkQIECAAAECBAgQIECAAIHmCUhMNM9WyQQIECBAgAABAgQIECBAgEAVAYmJKkBmEyBAgAABAgQIECBAgAABAs0TkJhonq2SCRAgQIAAAQIECBAgQIAAgSoCEhNVgMwmQIAAAQIECBAgQIAAAQIEmicgMdE8WyUTIECAAAECBAgQIECAAAECVQQkJqoAmU2AAAECBAgQIECAAAECBAg0T0Bionm2SiZAgAABAgQIECBAgAABAgSqCEhMVAEymwABAgQIECBAgAABAgQIEGiegMRE82yVTIAAAQIECBAgQIAAAQIECFQRkJioAmQ2AQIECBAgQIAAAQIECBAg0DwBiYnm2SqZAAECBAgQIECAAAECBAgQqCIgMVEFyGwCBAgQIECAAAECBAgQIECgeQISE82zVTIBAgQIECBAgAABAgQIECBQRUBiogqQ2QQIECBAgAABAgQIECBAgEDzBCQmmmerZAIECBAgQIAAAQIECBAgQKCKgMREFSCzCRAgQIAAAQIECBAgQIAAgeYJSEw0z1bJBAgQIECAAAECBAgQIECAQBUBiYkqQGYTIECAAAECBAgQIECAAAECzROQmGierZIJECBAgAABAgQIECBAgACBKgISE1WAzCZAgAABAgQIECBAgAABAgSaJyAx0TxbJRMgQIAAAQIECBAgQIAAAQJVBCQmqgCZTYAAAQIECBAgQIAAAQIECDRPQGKiebZKJkCAAAECBAgQIECAAAECBKoISExUATKbAAECBAgQIECAAAECBAgQaJ6AxETzbJVMgAABAgQIECBAgAABAgQIVBGQmKgCZDYBAgQIECBAgAABAgQIECDQPAGJiebZKpkAAQIECBAgQIAAAQIECBCoIiAxUQXIbAIECBAgQIAAAQIECBAgQKB5AhITzbNVMgECBAgQIECAAAECBAgQIFBFQGKiCpDZBAgQIECAAAECBAgQIECAQPMEJCaaZ6tkAgQIECBAgAABAgQIECBAoIqAxEQVILMJECBAgAABAgQIECBAgACB5glITDTPVskECBAgQIAAAQIECBAgQIBAFQGJiSpAZhMgQIAAAQIECBAgQIAAAQLNE5CYaJ6tkgkQIECAAAECBAgQIECAAIEqAhITVYDMJkCAAAECBAgQIECAAAECBJonIDHRPFslEyBAgAABAgQIECBAgAABAlUEJCaqAJlNgAABAgQIECBAgAABAgQINE9AYqJ5tkomQIAAAQIECBAgQIAAAQIEqghITFQBMpsAAQIECBAgQIAAAQIECBBonoDERPNslUyAAAECBAgQIECAAAECBAhUEZCYqAJkNgECBAgQIECAAAECBAgQINA8AYmJ5tkqmQABAgQIECBAgAABAgQIEKgiIDFRBchsAgQIECBAgAABAgQIECBAoHkCEhPNs1UyAQIECBAgQIAAAQIECBAgUEVAYqIKkNkECBAgQIAAAQIECBAgQIBA8wQkJppnq2QCBAgQIECAAAECBAgQIECgioDERBUgswkQIECAAAECBAgQIECAAIHmCUhMNM9WyQQIECBAgAABAgQIECBAgEAVAYmJKkBmEyBAgAABAgQIECBAgAABAs0TkJhonq2SCRAgQIAAAQIECBAgQIAAgSoCEhNVgMwmQIAAAQIECBAgQIAAAQIEmicgMdE8WyUTIECAAAECBAgQIECAAAECVQQkJqoAmU2AAAECBAgQIECAAAECBAg0T2B884pWcqnAU2s2xLd+8PP4/gMr48lVL8RLW7tj585dpYt4T4AAAQIECBAgQIAAAQJtKDBhwvg4cEpXvGzh7Dj1xEVx+smviMPnz2zDLWlNk8fszYbWVN0Ztf744adi2Y3fiSdWrY9xXQdFjD8wxk2YHGPHTYgxY8d1BoKtJECAAAECBAgQIECAwCgW2Ltnd+zZvTN279wWseul2N39YhyxcFZccsHr4zUnHD6Kt7wxmyYx0RjHfqVs3tId/2XZ7fHjn66KMQccEhOnzIoxY8b0W84EAgQIECBAgAABAgQIEBhdAun3/x1b18feXz0brzl+Yfz3S94c06Z2ja6NbODWSEw0ELNY1MrV6+M9H/5qdO+ZHBOmHZolJHTlUbTxSoAAAQIECBAgQIAAgU4R2Lt3T+zc/Ex0jd0WX/70ubFowaxO2fQhbafExJC4qi+ckhLv+vObYvekuTFp6tzqK1iCAAECBAgQIECAAAECBEa1wPYta2Pc9rVx81+fJzkxQKQlJgZAGe6kdPvGm9/3hfjVmFmSEsNFtB4BAgQIECBAgAABAgRGoUBKThywd33c/vk/cVtHRXzdY1ABUs9o6lMi3b7hSol6FK1LgAABAgQIECBAgACB0SeQzhPT+WI6bzSUC0hMlHsMeyw9fSN1dJn6lDAQIECAAAECBAgQIECAAIFKgXS+mM4b0/mjYZ+AxMQ+i7repUeCpqdv6OiyLkYrEyBAgAABAgQIECBAYNQKpPPFdN6Yzh8N+wQkJvZZDPvdU1mHl0+sWl94JOiwC7EiAQIECBAgQIAAAQIECIx6gYlTZhXOH59as2HUb2utGygxUavUIMt964ePxbiug7KrJcYMspRZBAgQIECAAAECBAgQINDpAum8MZ0/fusHP+90ir7tl5jooxj+m+/dvzJi/IHDL8CaBAgQIECAAAECBAgQINA5Atn54/cfyM4jDQUBiYkG7AgrV78Q4yZMbkBJiiBAgAABAgQIECBAgACB0S6Qzh+fXPXCaN/MmrdPYqJmqv0v+NLW7hg7bsL+FzCHAAECBAgQIECAAAECBAj0CqTzx3QeaegRkJhowJ6wc+euGDN2XANKUgQBAgQIECBAgAABAgQIjHaBdP6YziMNPQISE/YEAgQIECBAgAABAgQIECBAoGUCEhMto1cxAQIECBAgQIAAAQIECBAgIDFhHyBAgAABAgQIECBAgAABAgRaJiAx0TJ6FRMgQIAAAQIECBAgQIAAAQISE/YBAgQIECBAgAABAgQIECBAoGUCEhMto1cxAQIECBAgQIAAAQIECBAgIDFhHyBAgAABAgQIECBAgAABAgRaJiAx0TJ6FRMgQIAAAQIECBAgQIAAAQISE/YBAgQIECBAgAABAgQIECBAoGUCEhMto1cxAQIECBAgQIAAAQIECBAgIDFhHyBAgAABAgQIECBAgAABAgRaJiAx0TJ6FRMgQIAAAQIECBAgQIAAAQISE/YBAgQIECBAgAABAgQIECBAoGUCEhMto1cxAQIECBAgQIAAAQIECBAgIDFhHyBAgAABAgQIECBAgAABAgRaJiAx0TJ6FRMgQIAAAQIECBAgQIAAAQISE/YBAgQIECBAgAABAgQIECBAoGUCEhMto1cxAQIECBAgQIAAAQIECBAgIDFhHyBAgAABAgQIECBAgAABAgRaJiAx0TJ6FRMgQIAAAQIECBAgQIAAAQISE/YBAgQIECBAgAABAgQIECBAoGUC41tWs4oJECBAgACBERGYf/Kxccu7D66oa09s2bAlHrzv6fj0retiU8XcWkenz5gQsXHnsNevtZ7qy42L+TMi1mzcXX1RSxAgQIAAAQK5EnDFRK7CoTEECBAgQKDxAlMmFj/ud8UL67bEcxu2RffusTF15rR43e8eF3d88tg4aRjVLl56bNzxqdfFLZccMoy1G7nKpPjsstfHLZ/6rfjjuY0sV1kECBAgQIDASAi4YmIklNVBgAABAgRyILDlZ4/FWdc839eSpUtfHh8+Z0GWoDg4PvHhTfHGT6/um1fLm7U7epfataeWxZu4zK6+sncM99KPvhK8IUCAAAECBEZaQGJipMXVR4AAAQIEWiUwvnjlRE8D7r77sVixdWzccsEh0bXoiLjsyOfiqsd7boVY8uoFcf6pc+PwuZOja1xE99Zt8fA9j8dHv5nO/MfF+y/89Vh65MxCQVNftihu/PC8OGD8rrjl2uVx68aIwdffB7Dk5EXxgd85OA6ZPjFi+454+uerYtmNq2PFvkXijDOPinNPnhPzpoyPXWmZx5+NZTc8XVhm+jEL4qrfmx8vn5JWGB9vv+Q34vXZ66+efDIuvnl9SSneEiBAgAABAnkVkJjIa2S0iwABAgQIjIDAmvsej++ddXC8bub4OOJl2deCLDGR+qT4XLFPit27ojs70Z89ZWK84e0nxuy4Ny7+5u449tjZMW9SbwMnTY7FiyZnI7vikGxa9fW3FVacvuTlWT0LCu+7t2aXX0yZHMe++uXxH+7IEhNre8r+4w+eHOcf3VUY2bI5a8m0bJlXHhk3XjU5LrjsZxFzD4pjF07tWTjGxuyFM7M2ZomU7mLjemd5IUCAAAECBHIrIDGR29BoGAECBAgQGAmBnfHzNTuyxERXzFyQTvC3R2ztjlWrno9bb3k8bnk8G8+G8y58TVz4yqnx8uOyHia/+Wxc/MG74qQzT4ir3zI7tvzisfjdZatjenZpxabuLDExr9r6PYmJt502p1D2itt+HBfcsSV7Py7OetPcWNt7O8b0JUf1JiW2xVc/c19cl67m6Joen7/yxDh22iFxyZlPx/vuWB6n3D01Pn/Na+LYSTviq5/8YVy3LmJ6t04wC7j+I0CAAAECbSBQfk1nGzRYEwkQIECAAIHGCsycnd1GkQ0HHJA9YSMb1jz0RLzrykf6khJp2k3ZrRPZHRrRdcisWJwmZMP6bcW+JXpeN/UmA2pdf2LvzyNzXjEr5hdK3B23ZkmPe7PkRhredlrPk0RW3rW8JymRJnZvio//2wvpXRzxiixJUhh2xq96u5nYsT1LSGTt0NVEL40XAgQIECDQBgKumGiDIGkiAQIECBBopsBzL2S3UczrihfX/WpfNXOnx4d+Z0H85pHT4qCu9DvG+CikAWr95lDD+l+/89n4g6MOjxlHHZn1TXForHj4mfjCTU/3JSaKiYtFb3hNfPuUPTE+1Z11tDl+Um8jKvrM2Nd47wgQIECAAIF2Eqj160U7bZO2EiBAgAABAjULTIgT5vdcMbFt087CWvOXHBFfvujw6OnZYVds3NAdv8r6bygkJvY9AGO/NdS6/qZHn4h3fGZrfPI/HhEnZImRxVnfEVe/8pDsdox747pnImYWEiJZNdu3xTPP7ogJpd9asqTEqp+kazgMBAgQIECAQLsLlH7Et/u2aD8BAgQIECAwRIHpr35Z1r9EuiKiOx5+uKfvhzPesKCQlHhh+c/iPdc+23tbxJy4/frjepITlXVUJCuGsv6mx5+Pi6/IHmGadWL52YuPjxPnTI63n31IXLfs2Xh+S88tIg/f+kBcfHdP0qSy6uJ4z00oxTGvBAgQIECAQDsJ6GOinaKlrQQIECBAoB6B7DaI0uGkpUdljwrteSrGlp89EdcVnoQxIRZO7/l6sOahtX19NSw9a16/Wzm27ugpr2vGASXF1r7+SUsO6uuvIta+GFf/eHOhnGKe45frejqbOOGMxbGkpIb0dvrcSTG9ZNq2Ql+X42PhrJKJ3hIgQIAAAQJtIeCKibYIk0YSIECAAIH6BaYefXTc/slFsSu7DWLqtK7eWzWyayWeWx0fuCa7aqEw7IyfrO6ON2RXLpzw+yfG1UdsiAnzZseJhceBZgtkWYP0YNA0rHlkfWyJg2PqvMPj1g9Pi52zxse/Lbu/xvUnx59c9MpYvLs7Vjy6IZ7fOTGOP2FmodwXn+3p6+LOmx+Pc097ZSyadnB87pqp8cDDG2Nb1tHEwgUzY9GcibHif98bF3wzXeWxPZZnTxY5aVr2SNP3vSYWrhsbB25cGWdfW9ymQrH+I0CAAAECBHIq4IqJnAZGswgQIECAQKMEilc2ROonYubkmJ0lJcbv3hUvPLch7vzfD8Ybr3gsVpRUdusNy+PedVkGYtLUOOm3Di0kJVYuXx0rC08O3RM9N3xkK2x8Pr76YHrMZ8TsRTNjXpYYSE8brW39bfFvP9wQu8ZlfUscd0i87pWzY8a4iOeWPxYX3fxiocyIF+PdH10ej2zoacuJr16QLXdwISmxZd26+EbvrSdp4Zv++Yl4Ll01kbV58cLJcaCfXnoNvRAgQIAAgfwLjNmbDflvZr5beMwZV8bMw16b70ZqHQECBAgQGKLA9LmT45gDx8b61VtiRbqromtcTB/gUZzTZ0yK7M6K2Lppe6zpufuiUFNt64+LxYdOinQHxjPrtpWtX9rcVMei2VlPEi/tjF9W1LNvuaysuSkjsSdWrB28T4p963hHgAABAgRaI7Dh6R/Fo3de3prKc1ar3xNyFhDNIUCAAAECeRHYtHZb3Fvod6K3RQMkJdKcTRu39/VFUdr22tbfHSue6bsGo3T1svepjoeyf4MPWVlrC51NDL6YuQQIECBAgECuBNzKkatwaAwBAgQIECBAgAABAgQIEOgsAYmJzoq3rSVAgAABAgQIECBAgAABArkSkJjIVTg0hgABAgQIECBAgAABAgQIdJaAxERnxdvWEiBAgAABAgQIECBAgACBXAlITOQqHBpDgAABAgQIECBAgAABAgQ6S0BiorPibWsJECBAgAABAgQIECBAgECuBCQmchUOjSFAgAABAgQIECBAgAABAp0lIDHRWfG2tQQIECBAgAABAgQIECBAIFcCEhO5CofGECBAgAABAgQIECBAgACBzhKQmOiseNtaAgQIECBAgAABAgQIECCQKwGJiVyFQ2MIECBAgAABAgQIECBAgEBnCUhMdFa8bS0BAgQIECBAgAABAgQIEMiVgMRErsKhMQQIECBAgAABAgQIECBAoLMEJCY6K962lgABAgQIECBAgAABAgQI5EpAYiJX4dAYAgQIECBAgAABAgQIECDQWQISE50Vb1tLgAABAgQIECBAgAABAgRyJSAxkatwaAwBAgQIECBAgAABAgQIEOgsAYmJzoq3rSVAgAABAgQIECBAgAABArkSkJjIVTg0hgABAgQIECBAgAABAgQIdJaAxEQD4j1hwvjYu2d3A0pSBAECBAgQIECAAAECBAiMdoF0/pjOIw09AhITDdgTDpzSFXt272xASYogQIAAAQIECBAgQIAAgdEukM4f03mkoUdAYqIBe8KiBbNj985tDShJEQQIECBAgAABAgQIECAw2gXS+ePLFs4e7ZtZ8/ZJTNRMtf8FX/eqRRG7Xtr/AuYQIECAAAECBAgQIECAAIGiQHb+eOqJ2XmkoSAgMdGAHeH033p57O5+Mfbu3duA0hRBgAABAgQIECBAgAABAqNVIJ03pvPH009+xWjdxCFvl8TEkMn6r3D4gllxxMJZsWPr+v4zTSFAgAABAgQIECBAgAABAr0C6bwxnT8ePn8mk14BiYkG7QqXXPD62PurZ7OrJvY0qETFECBAgAABAgQIECBAgMBoEkjni+m8MZ0/GvYJSEzss6jr3WtOODxec/zC2Ln5mbrKsTIBAgQIECBAgAABAgQIjE6BdL6YzhvT+aNhn4DExD6Lut/990veHF1jt8X2LWvrLksBBAgQIECAAAECBAgQIDB6BNJ5YjpfTOeNhnIBiYlyj7rGpk3tii9/+twYt32t5ERdklYmQIAAAQIECBAgQIDA6BFISYl0npjOF9N5o6FcYEzWI6hHSZSb1D22cvX6eM+HvxrdeybHhGmHxpgx8j91oyqAAAECBAgQIECAAAECbSaQ+pRIt2+kKyVSUmJR9uAEQ38BiYn+Jg2ZsnlLd/yXZbfHj3+6KsYccEhMnDIrS1CMaUjZCiFAgAABAgQIECBAgACB/Aqk3//T0zdSR5epT4l0+4YrJfYfL4mJ/ds0ZM6PH34qlt34nXhi1foY13VQxPgDY9yEyTF23IQYM3ZcQ+pQCAECBAgQIECAAAECBAi0TmDvnt2xZ/fO2L1zW8Sul2J394uFR4Kmp2/o6LJ6XCQmqhs1ZImnsts7vvXDx+L7D6yMJ1e9EC9t7Y6dO3c1pGyFECBAgAABAgQIECBAgEDrBCZMGB8HTumKly2cHaeeuChOP/kVcfj8ma1rUJvVLDHRZgHTXAIECBAgQIAAAQIECBAgMJoE9Mo4mqJpWwgQIECAAAECBAgQIECAQJsJSEy0WcA0lwABAgQIECBAgAABAgQIjCYBiYnRFE3bQoAAAQIECBAgQIAAAQIE2kxAYqLNAqa5BAgQIECAAAECBAgQIEBgNAlITIymaNoWAgQIECBAgAABAgQIECDQZgISE20WMM0lQIAAAQIECBAgQIAAAQKjSUBiYjRF07YQIECAAAECBAgQIECAAIE2E5CYaLOAaS4BAgQIECBAgAABAgQIEBhNAhIToymatoUAAQIECBAgQIAAAQIECLSZgMREmwVMcwkQIECAAAECBAgQIECAwGgSkJgYTdG0LQQIECBAgAABAgQIECBAoM0EJCbaLGCaS4AAAQIECBAgQIAAAQIERpOAxMRoiqZtIUCAAAECBAgQIECAAAECbSYgMdFmAdNcAgQIECBAgAABAgQIECAwmgQkJkZTNG0LAQIECBAgQIAAAQIECBBoMwGJiTYLmOYSIECAAAECBAgQIECAAIHRJCAxMZqiaVsIECBAgAABAgQIECBAgECbCYxvs/ZqLgECBAgQINBCgWPOuLLptT965+VNr0MFBAgQIECAQH4EJCbyEwstIUCAAAECbSEw87DXNq2dG57+UdPKVjABAgQIECCQTwG3cuQzLlpFgAABAgQIECBAgAABAgQ6QkBioiPCbCMJECBAgAABAgQIECBAgEA+BSQm8hkXrSJAgAABAgQIECBAgAABAh0hIDHREWG2kQQIECBAgAABAgQIECBAIJ8CEhP5jItWESBAgAABAgQIECBAgACBjhCQmOiIMNtIAgQIECBAgAABAgQIECCQTwGJiXzGRasIECBAgAABAgQIECBAgEBHCEhMdESYbSQBAgQIECBAgAABAgQIEMingMREPuOiVQQIECBAgAABAgQIECBAoCMEJCY6Isw2kgABAgQIECBAgAABAgQI5FNAYiKfcdEqAgQIECBAgAABAgQIECDQEQISEx0RZhtJgAABAgQIECBAgAABAgTyKSAxkc+4aBUBAgQIECBAgAABAgQIEOgIAYmJjgizjSRAgAABAgQIECBAgAABAvkUkJjIZ1y0igABAgQIECBAgAABAgQIdISAxERHhNlGEiBAgAABAgQIECBAgACBfApITOQzLlpFgAABAgQIECBAgAABAgQ6QkBioiPCbCMJECBAgAABAgQIECBAgEA+BSQm8hkXrSJAgAABAgQIECBAgAABAh0hIDHREWG2kQQIECBAgAABAgQIECBAIJ8CEhP5jItWESBAgAABAgQIECBAgACBjhCQmOiIMNtIAgQIECBAgAABAgQIECCQTwGJiXzGRasIECBAgAABAgQIECBAgEBHCEhMdESYbSQBAgQIECBAgAABAgQIEMingMREPuOiVQQIECBAgAABAgQIECBAoCMEJCY6Isw2kgABAgQIECBAgAABAgQI5FNAYiKfcdEqAgQIECBAgAABAgQIECDQEQISEx0RZhtJgAABAgQIECBAgAABAgTyKSAxkc+4aBUBAgQIECBAgAABAgQIEOgIAYmJjgizjSRAgAABAgQIECBAgAABAvkUkJjIZ1y0igABAgQIECBAgAABAgQIdISAxERHhNlGEiBAgAABAgQIECBAgACBfApITOQzLlpFgAABAgQIECBAgAABAgQ6QkBioiPCbCMJECBAgAABAgQIECBAgEA+BSQm8hkXrSJAgAABAgQIECBAgAABAh0hIDHREWG2kQQIECBAgAABAgQIECBAIJ8CEhP5jItWESBAgAABAgQIECBAgACBjhCQmOiIMNtIAgQIECBAgAABAgQIECCQTwGJiXzGRasIECBAgAABAgQIECBAgEBHCEhMdESYbSQBAgQIECBAgAABAgQIEMingMREPuOiVQQIECBAgAABAgQIECBAoCMEJCY6Isw2kgABAgQIECBAgAABAgQI5FNAYiKfcdEqAgQIECBAgAABAgQIECDQEQISEx0RZhtJgAABAgQIECBAgAABAgTyKSAxkc+4aBUBAgQIECBAgAABAgQIEOgIAYmJjgizjSRAgAABAgQIECBAgAABAvkUkJjIZ1y0igABAgQIECBAgAABAgQIdISAxERHhNlGEiBAgAABAgQIECBAgACBfApITOQzLlpFgAABAgQIECBAgAABAgQ6QmDM3mzoiC21kQQIECBAgECZwH0/fTre91f/f2zfsatserWRmYe9ttoiw56/4ekfDWndSRPHxw2f+IN41fGHDWk9CxMgQIAAAQL5EZCYyE8stIQAAQIECIy4wP1ZcuI/fezrMW7aETGha9qI1z/cCnd2b47dm5+Iv/3Y2ZISw0W0HgECBAgQyImAWzlyEgjNIECAAAECrRBIVxqkk/t0kp9O9tthkJRohyhpIwECBAgQqF1AYqJ2K0sSIECAAIFRKdBOyQlJiVG5C9ooAgQIEOhwAYmJDt8BbD4BAgQIEEgC7ZCckJSwrxIgQIAAgdEpIDExOuNqqwgQIECAwJAF8pyckJQYcjitQIAAAQIE2kZAYqJtQqWhBAgQIECg+QJ5TE5ISjQ/7mogQIAAAQKtFJCYaKW+ugkQIECAQA4F8pSckJTI4Q6iSQQIECBAoMECEhMNBlUcAQIECBAYDQJ5SE5ISoyGPck2ECBAgACB6gISE9WNLEGAAAECBDpSoJXJCUmJjtzlbDQBAgQIdKiAxESHBt5mEyBAgACBWgRakZyQlKglMpYhQIAAAQKjR0BiYvTE0pYQIECAAIGmCIxkckJSoikhVCgBAgQIEMi1gMRErsOjcQQIECBAIB8CI5GckJTIR6y1ggABAgQIjLSAxMRIi6uPAAECBAi0qUAzkxOSEm26U2g2AQIECBBogIDERAMQFUGAAAECBDpFoBnJCUmJTtl7bCcBAgQIEBhYQGJiYBdTCRAgQIAAgf0INDI5ISmxH2STCRAgQIBABwlITHRQsG0qAQIECBBolEAjkhOSEo2KhnIIECBAgEB7C0hMtHf8tJ4AAQIECLRMoJ7khKREy8KmYgIECBAgkDsBiYnchUSDCBAgQIBA+wgMJzkhKdE+8dVSAgQIECAwEgISEyOhrA4CBAgQIDCKBYaSnJCUGMU7gk0jQIAAAQLDFJCYGCac1QgQIECAAIF9ArUkJyQl9nl5R4AAAQIECOwTkJjYZ+EdAQIECBAgUIfAYMkJSYk6YK1KgAABAgRGuYDExCgPsM0jQIAAAQIjKTBQckJSYiQjoC4CBAgQINB+AmP2ZkP7NVuLCRAgQIAAgTwL3PfTp+P9H/t67J00N8ZsXxt/+7GzIyUtDAQIECBAgACBSgGJiUoR4wQIECBAgEBDBFJy4s8+8U/xub96h6REQ0QVQoAAAQIERqeAxMTojKutIkCAAAECuRD4VffOOKBrQi7aohEECBAgQIBAPgUkJvIZF60iQIAAAQIECBAgQIAAAQIdIaDzy44Is40kQIAAAQIECBAgQIAAAQL5FJCYyGdctIoAAQIECBAgQIAAAQIECHSEgMRER4TZRhIgQIAAAQIECBAgQIAAgXwKSEzkMy5aRYAAAQIECBAgQIAAAQIEOkJAYqIjwmwjCRAgQIAAAQIECBAgQIBAPgUkJvIZF60iQIAAAQIECBAgQIAAAQIdISAx0RFhtpEECBAgQIAAAQIECBAgQCCfAhIT+YyLVhEgQIAAAQIECBAgQIAAgY4QkJjoiDDbSAIECBAgQIAAAQIECBAgkE8BiYl8xkWrCBAgQIAAAQIECBAgQIBARwhITHREmG0kAQIECBAgQIAAAQIECBDIp4DERD7jolUECBAgQIAAAQIECBAgQKAjBCQmOiLMNpIAAQIECBAgQIAAAQIECORTQGIin3HRKgIECBAgQIAAAQIECBAg0BECEhMdEWYbSYAAAQIECBAgQIAAAQIE8ikgMZHPuGgVAQIECBAgQIAAAQIECBDoCIHxHbGVOdjIp9ZsiG/94Ofx/QdWxpNP3wA4AAA5EklEQVSrXoiXtnbHzp27ctCy9mvChAnj48ApXfGyhbPj1BMXxeknvyIOnz9zxDZELBtLLZ6N9WxlaWLZSv3G1y2ejTdtVYli2Sr55tQrns1xbUWpYtkK9ebV2ep4Nm/LRqbkMXuzYWSq6sxafvzwU7Hsxu/EE6vWx7iugyLGHxjjJkyOseMmxJix4zoTpc6t3rtnd+zZvTN279wWseul2N39YhyxcFZccsHr4zUnHF5n6ftfXSz3b1PPHPGsRy9f64plvuJRb2vEs17B/KwvlvmJRSNaIp6NUMxHGWKZjzg0qhWtimej2t/qciQmmhSBzVu6478suz1+/NNVMeaAQ2LilFkxZsyYJtXW2cWm3NqOretj76+ejdccvzD++yVvjmlTuxqGIpYNo6ypIPGsiaktFhLLtghTzY0Uz5qpcr+gWOY+RENqoHgOiSvXC4tlrsMz5MY1O55DblDOV5CYaEKAVq5eH+/58Feje8/kmDDt0CwhoSuPJjD3K3Lv3j2xc/Mz0TV2W3z50+fGogWz+i0z1AliOVSxxi0vno2zbHVJYtnqCDS2fvFsrGcrSxPLVuo3vm7xbLxpq0oUy1bJN6feZsSzOS1tbakSEw32Tyey7/rzm2L3pLkxaercBpeuuFoEtm9ZG+O2r42b//q8upITYlmLdvOXEc/mG49UDWI5UtIjU494jozzSNQiliOhPHJ1iOfIWTe7JrFstvDIlt+oeI5sq0euNomJBlqnS/7f/L4vxK/GzJKUaKDrcIpKB/4Be9fH7Z//k2Hd1iGWw1Fv3jri2TzbkS5ZLEdavLn1iWdzfUeydLEcSe3m1yWezTceqRrEcqSkR6aeeuM5Mq1sTS3uMWige+pTIt2+4UqJBqIOs6gUgxSLFJPhDGI5HLXmrSOezbMd6ZLFcqTFm1ufeDbXdyRLF8uR1G5+XeLZfOORqkEsR0p6ZOqpN54j08rW1CIx0SD39MSG1NFl6lPCkA+BFIsUkxSboQxiORStkVtWPEfOutk1iWWzhUe2fPEcWe9m1iaWzdQd+bLFc+TNm1WjWDZLtjXlDjeerWntyNUqMdEg6/RI0PT0DR1dNgi0AcWkWKSYpNgMZRDLoWiN3LLiOXLWza5JLJstPLLli+fIejezNrFspu7Ily2eI2/erBrFslmyrSl3uPFsTWtHrlaJiQZYP5V1ePnEqvWFR4I2oDhFNFAgPaY1xeapNRtqKlUsa2Jq2ULi2TL6hlcslg0nbWmB4tlS/oZWLpYN5Wx5YeLZ8hA0rAFi2TDKXBQ01HjmotFNboTERAOAv/XDx2Jc10HZ1RJjGlCaIhopkGKSYvOtH/y8pmLFsiamli0kni2jb3jFYtlw0pYWKJ4t5W9o5WLZUM6WFyaeLQ9Bwxoglg2jzEVBQ41nLhrd5EZITDQA+Hv3r4wYf2ADSlJEUwSy2Hz/gSxGNQxiWQNSqxcRz1ZHoHH1i2XjLPNQknjmIQqNaYNYNsYxL6WIZ14iUX87xLJ+wzyVMIR45qnZzWqLxEQDZFeufiHGTZjcgJIU0QyBFJsnV71QU9FiWRNTSxcSz5byN7RysWwoZ8sLE8+Wh6BhDRDLhlHmoiDxzEUYGtIIsWwIY24KGUo8c9PoJjZEYqIBuC9t7Y6x4yY0oCRFNEMgxSbFqJZBLGtRau0y4tla/0bWLpaN1Gx9WeLZ+hg0qgVi2SjJfJQjnvmIQyNaIZaNUMxPGUOJZ35a3byWSEw0wHbnzl0xZuy4BpSkiGYIpNikGNUyiGUtSq1dRjxb69/I2sWykZqtL0s8Wx+DRrVALBslmY9yxDMfcWhEK8SyEYr5KWMo8cxPq5vXEomJ5tkqmQABAgQIECBAgAABAgQIEKgiIDFRBchsAgQIECBAgAABAgQIECBAoHkCEhPNs1UyAQIECBAgQIAAAQIECBAgUEVAYqIKkNkECBAgQIAAAQIECBAgQIBA8wQkJppnq2QCBAgQIECAAAECBAgQIECgioDERBUgswkQIECAAAECBAgQIECAAIHmCUhMNM9WyQQIECBAgAABAgQIECBAgEAVAYmJKkBmEyBAgAABAgQIECBAgAABAs0TkJhonm3bljz/yFlx3tJZsaRrOJswIc5aenCcc8zk4axsnYYL1BeP+vaFhm9MxxdYXzzq2xc6Hr/hAPXFo759oeEb0/EF1heP+vaFjsdvOEB98ahvX2j4xnR8gfXFo759oePxGw5QXzzq2xcavjEKHEBg/ADTTGpjgfknHxu3vPvgwhZsWfVknHPlyti0n+05412/GZefNqMw97n7Hoqzb1yfvZ8U//XPl8Sx4yJWTL43Lrhj237WHnjy4jcdE5e+fXbE7hfisYsfjocGXszUmgQmxdVXnRInTUsL74g7/+aHceWjuwdes+ug+MqnXxmLJqXZW+J/fujHcUt3RH3xqG9fGLihnTvVsTl6Yl9/LB2b+dkb6v8763MzP9F0bOYnFvW3xLFZv2F+SnBs5icWeW6JKybyHJ1htG3KxH0hnbrw8Hjfkfsr5KA4rzcpkZY48KDCGW32bk88v25X4XX9xv2cBKcV9jNs3bgtsvPhiE07Ymgpjf0U2OGTJ2cJop5hYpx+9qHFkX6vS3/vyN6kRJo1MWZN71mkvnjUty/0a2SHT3Bsjp4doP5YRjg287M/1Pt31udmfmLp2MxPLBrREsdmIxTzUYZjMx9xyHsrXDGR9wgNs327tu+J8ZPGxm+feXBcdc3z/UpZcuahsbDf1DRhZ3z0iu/GRwecV33imvt+EW/M/hkaI7CzUMye6N49NrrmHRJ/PGNl/N3GyrInxx+8tnBZRd+MHb3v6otHfftCX2O8KRNwbJZxtPXI8GMZ4djMT+jr/TvrczM/sSy2xLFZlGjvV8dme8dvoNY7NgdSMa0osO/n9eIUr6NC4JEfrY4Xsi2ZevTCOKPfFk2I/3hadrtFdnvAvT9MS5UPZ5x9XNz6yd+MDx3T+3N916z4/MdPimvPmh6LlxwWN3785PjXa06N2z/+m3HZq4tXWvSWkS17bbbsVy5eEL0/2pcXbmzoAptfiLtXpOtPuuJ33npQv/Xnn3xYHJuFYcvKZ+PhDRWzB4jHGWefELd+/NhYOmNSfOjC34jbl50a/7rs5Cxmh8XiitX77QsV840OXcCxOXSzvK5RTyzDsZmvsNbzdzbbkn5/K31utjS+js2W8je2csdmYz1bXJpjs8UByHn1EhM5D9Bwm7fzmefiO8/tyVafFmefWdER5TGHxqvSD+wbno///UTxt/V9Nc0/dEbMnjkjju/psCBi+qQ4bM7kOOF3T4wbLzoyFs/piqmTJsaMOTPirRe8Ni4rvV0kW/aIbNlFi2fFon1FelePwLhd8X/uWBfpBpt5v3loLCkra1yc/8aePkUe/LdnY1vlNVADxGP+IdNi9pyD4xOfOiXe+cqZMWPKxJg6pSsWHXdk3HD5YWWl99sXyuYaGY6AY3M4avlcp55Ypr+rlX8rHZstjHM9f2ezZvf7W+lzs4XBzK5hqeM7kGOzpaHrX7ljs79JG09xbLZx8Eag6RITI4DciiomTNweX/z2ukLVi087LOaXNOL9ZxwS6fz1ke88FY9G/11gx66U0Mg+2HuuoYvYHoWT4jRtV5bMuPIv74pTLro37t2QlhtfuF0kzSsMxWWzMvQxUUSp83X8+Hjm8Wfioa1ZOZNmx388ecK+Ag+dF6fOy2K4/YX40kPbYkJfnxS9iwwQjx1ZPyI9w5544LYHs1jeFX96y7OFGI9fuCC7XWRf8f32hX2zvBumgGNzmHA5XK2eWPb9XS35W+nYbGGQ6/k7mzW739/K4t/ebJ7PzZGPq2Nz5M2bVqNjs2m0rSjYsdkK9faps/9Zafu0XUsHERg/YWxs+sHqWJn6r5x2cJxfvKqha0686aiJ2cTNces3d8bUks4yBymuZ9bujXHFRx6JOwt9HGyLz2a/4heG8XajqnZ1LjAl6/vjXx7YXCjlVW9c0FfaOb+7MKZmY8/936diRfZakrLoW2Z/b1be9WB86I4XC7MfuvvxuL9Q/NiYWHF3zv7WN314Ao7N4bnlca2mxDLbUMdma6LdjL+z4XOzJcF0bLaEvWmVOjabRjviBTs2R5y8rSp0RtlW4RpKY9NP55vi9kfTMzLGxqlnHlJYeenvHRapd4mNy1fFndnrlMLUGv/r3hFrShZd88j67MGU6ZoJw0gI3H3nqkg5ofGpE8y5qcaD4qxXptt0uuO2b6SHwo7N0he1D5s3FZ6f0rvCzli+pue2nv4399RepiVrEXBs1qLUHss0IZbZhjs2Wxf9Rv+dDZ+bLQqmY7NF8E2r1rHZNNoRLtixOcLgbVWdxERbhWvojb0l63cgnX5OPXp+LI1J8c7C0xt2xbdv6/+kjqGXbvcZulkda2x8Pr69KvU0kXWC+dtTY/7SnierdK9aHTcVrmIZWtnjI304GFol4NhslXzj6210LB2bjY9RzSU2+O9s/3p9bvY3ad4Ux2bzbEe8ZMfmiJM3s0LHZjN127dsn5DtG7vaWv746riv8KSGafH+y5cUnt4QG56NLz5T2+qWypfA17/Vc/vMvBOPjc+cMbPQuPvuWJ2vRmpNbQKOzdqc2mEpsWyHKNXcRn9na6bK/4KOzfzHaAgtdGwOASvvizo28x6hlrRPYqIl7CNZ6c742g96niE5b+HUwm0XD3/nmewmD0M7Cqy5b1U8knWqFlOmxsJp2eG7dV3W6WXqSMTQfgKOzfaL2f5aLJb7k2nH6f7OtmPU9tdmx+b+ZNpxumOzHaO2vzY7Nvcn08nTJSY6IPoP3fVMPFfczqwjrn/8ZjqzHcaQdSaRejSoHNLNBf2G/SzbbzkThiiwJW59OPXs0TOs/NEzhU4vi+PF19S9adkgHmUceRlxbOYlEvW3Y9ixdGzWj9/wEob5d3agduwnvj43B8JqzjTHZnNcW1OqY7M17s2p1bHZHNd2LlViop2jN2jbS35F714f//azno4O09Mb7h5ovbJvST27xa927isj+26VPZJy4EeAFuaVlDnYsiWLeVuDwEBP2bjzjp5OMCPrevSfbhv42pfSDiz7x6N/fPfflKEsu/9SzCkV2HdchWOzFKYN39cTy96Og8v+rg7leBvKsm1IO4JNbsTf2dT5cBp8bo5g4AatyrE5KE+bzHRstkmghtRMx+aQuDps4TF7s6HDtrnhm3vMGVfGzMNe2/ByFdg4gQ1P/ygevfPyqgWKZVWiXCwgnrkIQ0MaIZYNYcxNIeKZm1DU3RCxrJswVwWIZ67CUVdjxLIuvtytXGs8c9fwJjSoJ8XfhIIVSYAAAQIECBAgQIAAAQIECBCoJiAxUU3IfAIECBAgQIAAAQIECBAgQKBpAhITTaNVMAECBAgQIECAAAECBAgQIFBNQGKimpD5BAgQIECAAAECBAgQIECAQNMEJCaaRqtgAgQIECBAgAABAgQIECBAoJqAxEQ1IfMJECBAgAABAgQIECBAgACBpglITDSNVsEECBAgQIAAAQIECBAgQIBANQGJiWpC5hMgQIAAAQIECBAgQIAAAQJNE5CYaBqtggkQIECAAAECBAgQIECAAIFqAhIT1YRaPP+8i0+Ke65/Q1x98qQWtySrfsbBcXvWlns+eXTMr7c1XRNi8Yxx9ZbSXus30q/eLW9UW7I4Ljlyapx05OSYXm+b2mz9PB2bjWnLuFh86OQsllNjyVzHZst2x0Ydm30bMC7mz5jQUcdnY46HPsC63uSpLXVtSKtWbvjxUMeGNLQt6e9t+uyc2lHfhfJ0POSpLXXsla1btaHHQ52b0bC2FL/TZt+DOu0cpc4QNGr18Y0qSDlNEhjfkzuaNqVxJwrTsy+psXFnbBpyk8dGYYeZMj5+LVt3zZDXTyuMi/MuOD4uePXMrKwd8aWPfj/+bu2wCmrDlRrhV7nZ6aQji8XG3ZUzqozX15bpRx4SV513RBw7Z2JJPTvigdt+Gh+6Y+h7Vkkh7fM2T8dmXW2ZEH98wa/HH2THZFeJfve652PZJx+JO7tLJo7at/UdDwOztObYLGtL15z4+mePi3lp4tZ1ccEly2NF2QKjdKSu42Fgk2F/btbTlq5ZcfOyJbFwoI//rc9n8XykA+I52o7Ncdnf2+P7/7197pl4zxW/GOb3qoH32VxOred42M8GteLYPO+DJ8eFR5d+YlY0bvOzccFlPxvlx+doOjYnxPsvXhK/f9y0nvOc3nB2b1gXn7tqedy6sSK+Rpsm4IqJptHms+DFS4+NOz71urjlkkOG1cBdaa3sv23DWHv+MQviK8teHxcWkhLDKGAUrFKPX//NnxSfzTxv+dRvxR/P7T+32pTht2VSfPLPjy4kJXZt7Y5Vz22JLYW8yMQ48S2vjMuPGehbdLXWmF/vsTlcwcVLj4nzC0mJXbFq5QvxyKothaK65hwcl3/4iI75pX34x8NA8q06Nsvb8v6LX96TlCifbGyIAq06NmP6pDio989p9+bu2Fj8t3VHbNzQHVuHuB3tuvjoOTbHxYc+fMq+v7e/eD6+d9/zsWLdjuia1hVT2jVALWx3q47NzS/tiO7tO2JLdiz2/cuOz8K+mjzGdcbvvqPl2DznwhPj3EJSYkc88uAzcdeD6yL93tc1c05c+p+P6pjvQS08lPuq7owjp29z2/nNUH8RH3hb1+7onb5rz8AL1DJ1WHvNpPivH3h5LMrKX/Hg6phw7IJYlIO7U2rZ3IYvMyy/gVrR9xEYO4Z7kcKw2rIrfrFiQ2z62ZPxkW/2Vtz3y97YOPX0LEvy6LMDNXiUTsvRsRlDb8vatevjgQc3xnU3PN33686SM4+Lz71lTsTMqYVj9qFRGrl+mzWs46FfKdmEVh2bJW055qj4/aNKr2gqmdcxb4d+PAxE05DPzWEcm7G9d0/KrnZ5z2XLR/+v6QPhF6eNgmNz+pIj452L0oZsiRs++uO4qWOuFi0GsfS1vY/NW2+8P24t3Zze9x/68GmFGD/38LN9n6cDLDa6JrX9sTk5Xnfs5Cwme+LOv/l+XPlob3gOPSy+/ZEjo2vOzPjd7OKYWzri6tHW75oN251avymjuwW7YnKcd94r4u2/Pi26xu2JF59dF39z7S/i3pIDZcmrF8T5p86Nw+dOzpaJ6N66LR6+5/H4aOHkcVy8/8Jfj6VHzixATX3Zorjxw/PigPG74pZri5cpjYtzzl4cZ/3GzDho0tjYla3/0x9nJ593vFiOu31PHLFkQfzl2w+LQ6Zny23aFt+7/d/jqvuyb1H7HfbEzx58Nu7/Pyvi7x4fm/3S38GJieR3zCFxydmHxWHTxseu7d3xf7/zsyxOPb9UFwi7Jsf7f29RnHT0tPi17NaZ2L0rfrnmhbjxy7+Iu7NLyqZnV59c9Xvz4+WFn1jGx9sv+Y14fXYB2q+efDIuvnl9oYjpRx4cV7xjYRw5pyvG794Tv1z7Qnz17x+PO9eWfCEYVix3x2ev+Ul5pLvXx62PdsefHZf99e6wvyp5OjZ3TZiaXSZ8TJx17NQsDFnMn1wdn7h2X8KhPGg9Y5seXR0fKn4Q9y7w0F2/jI1ZYmJGVkZHDW1/bBajNSmuPv/QbB/YFXfe8kQsOaczr5xo92OzGM302vG/po+CY/O9v3NwIaSP3La8w5MSKeGWn++0w/ncLD02+95nJ7JvLiSetsVt3+j5HtY3bzS/GQXHZk949sTWkq/hsW57pFOsruz797qSc63RHMo8bFuHnULkgXx4bTjh7a+ME0pWnXrUoXH1lV3xp9k9w+nXzPknHxufe3fPh146ie3OvpLOnjIx3vD2E2N23BsXf3N3HHvs7JhXvEph0uRYvChlCHfFIYVp4+Kyy0+Jty7s2SV2ZX9oxs+ZEa/LLs3//LZ74n13lyQdpmWXeF/UW1dqU9bPwFsveG3Ehu/GVY+nCQMNO+OzN/ysd0aH73bJ7wMlfoU4vSYOnnBvvO+OdJPMhCy2J8VJvd9Eu7fviq4pk2PqtEPjE3/VE/Ntcw+KYxdO7fUcG7MXzszinCWjunsCPP2YI+KfPnB4b58Be7Ioj41F0xbE5R+ZGE98sOQ+82HFsrfaipfDZvfEtfulfb8WVywyKkfzdGye8JYl5X8njjsybrg84vVXPj0k+z9+36IsKZEN2WWqvxzSmm2+8Cg5Nk8665g4aVr292DlL+LKu/fE7ee0eVyG2fxRc2x2jY/5Wc53bfrFIbu+eLgXyA2TMR+rtf2xOTmOOiR9RnbH/7kr+5xPHYBnP+xE9l1rRdbnV6cNo+bYLAnceb+3oPCdq/sXq+Km7Aekjhna/tjcFo88uytOyJJK7/zgb8RzV/0kblk7IS6/5KjC96DuJ9fG3R0TzNZvqD4mWh+DGluwJx7+1wfjlIvuijM/81i8kNaaMif+05tSciEb0r3+q56P//mZe+KUi78bb7z4rrjhwZ7U38uPS6cY2+PiD94Vf3FbYc3Y8ovHesr60D1xXXY54eKTX9GblNgW//g334vXf/DuOPOTD8XDm3dkJ7X9h10bno8r//KurIx7494N6VfV8fHbZ5acbPdfxZQSgV1ZhzpXfjT5fSe+tDwlIyKOfdMRsaR3mV8+uzkeuGt5nJPF+40f/G6c8tFH4rk0b8qM+K3sTokVdy/P1v1xPFLIF+2Ir37yO3HKh74T71iWbqGYFP/f+T1JiY0/ezwr4+54fVbPZ+56IbZkX4Iqh4bEcu6C+O15PYmJh+/roF8KCph5Ojb3ZB2Q9vyd+NNbni0cu+MXLog/LmQZKiO/b3z6oXPiigtfHp+84Oj4yidPi/OPzv6u7N4SX7r+sY67fLztj83stqr//LvpyrhtcdPns78HMzr5Y77Nj83sysXCX9VxM+MTn31D1j/U6+OO65fG7R89Ns7KPgc6bWjrY3PGjDis8LtBV7zrIyfHPZ99Xdx4xSlxY9bn1z3XnBSXv7rnR4XOiWmbH5uVgco6Gn57oTPMPXH3v6yunDvqx9v62Myic901P41HNmffj6fMjD+7Ymn86zWvizMWTowtq1bHpcs6L56t3GE7+RtLK92HXPeq7z4YF9/ac0vFpsdXx+d/2JN0OKyQdMieyvDQE/GuKx+JWx7fd2XDTdk94ylp23XIrFjcW+P6bcUT057XTd09l/W/9sSeWzxWfffR+OyjPdn7Tc+sj4sv+35cXHq1RCpn98a44iNZb/2p8OzL72fvWJfeZLkJu1MPRJX/C37Ls1sq0nK74++uXdGTdJjUk3SI2BlXLbs/PvT1dftOCtc+H/+4vDtbfmIce0LxC8zO+FVv1mjH9iyOWSwLv6TNmBVZHz7ZsDmWXfN0bxm749avPxy/e1lFL+4NieW4uOLioyJdv7Fr3VPxPx4quVUkNWOUD3k6Nlfe9WD2VJSevxMP3f143L854Y+NicVdZj+xmPuK+fGGVy6I1736kFg0syfBFLt2xY6JHdaR6Sg4Ns973ysKHV6mv+WFX+1KYt+bxt7PXjD6Jrf9sbl2Yyx/bls899zGWLFyQ6xct61w9duMeQfHpVe8Js4a5KEAoy6ao+DYLMZkdnZ75QvPbYgHlq+L57Zm38WyK1jPuODEOK9KArm4/mh4bftjsyIIZ/z+YYWrVmPDs/G5/V45XLHSaBkdDcdm94vxvi890xuRsTG193Pz2SdfKFyVPlpC1Q7b0fsNtB2a2tlt3LA+nZTuG55YlX5lL17K3zt97vT40O8siN88cloc1JWSBON7LseuIcozp/YkFVY/0/Pr/b6aBnjXvWPfCXM2e80j67OunA7u+WVngMVNqhCo8EsdYa3Julefl926UeybNK2xdOlh8bZXz47Dp2d9RGQx7JrS8y10fPbI1UGH2ZN6YrF5S/XOlyraMpxYnpN9oXrDnLT/bIm//R9PdNxlxnk6NjdvKv07sTOWr9lRuKS/dL8aaN9Z8c1/jz99Mvskzhact3BGnHVm9ijYmTPiwkt/K6Z+8vtxXfHzeqCVR9O0iuOh3Y7N6ccsigvSr3bZo+ouvbn3gv+1xWvesj5HRlOsatiW9j82t8RfXHFv+ZbOmBM3X3lc9gjRqXH+ubPi1hs75Aq1Nj82+4KYncR95vL/W/L4wclx7VUnxQnZUzlOfvXkuOmbNXwH6yusfd+0/7FZaj89zn1N4degeOBbT3Xcd6AYBcfmSVmH31enDr+zfrVW3Pdk/Hz87HjrK2fE4tOWxLcPfTLe8emVnRfX0l18BN/XcMo6gq1R1X4FKk9GJ08uD938JUfEly/quXw/9RuRHiX2q+yX0kICvvi9dL+lT4qjCr+SZuu9WLyiYr8LDzCjJ6kxwAyTahIYF9l3kpJhQlz24dfGWxf19Kjfnd2ms2Vr1m9Idt6YbjGuNizO+p5IxaVHeZb241NtvZ75Q4vl0rN/I/7s1YVrJeLO6x+IWwpX0dRW02hZKk/HZmVbajfeGQ893nOl1EPPbIk7f/DL+OxVp8SJ0ybGf8g+rK+7tveqqNoLHCVLttex+bbTF/Re+j8t/mvWIe6E9DGRdaRb+BzompmdEP1GrH7oyfiLOzqjl4LK46GVn5uVbRn2AbJxXdzy8La49JVZJ9cH9v6sN+zC2nnF9jo2szsse47NXTviqbLPyez+9vXZ/e1ZR9gTJg/t87edo1d5PLTzsbnkTYfHovTdbPsL8aXKK4zbOUjDbnubHZtzD4nLC0mJXXHXjT+Mj96Xvgs9HZ9fsihuuehlMXXRy+KKk7NOwn/QeX3BDHsXqGPF8rPbOgqyanMFdu0svzz+gN4PsGIAz3hDT6c7Lyz/Wbzn2md7M3tz4vbrj+v5UlrZvLJkxfb4yTM74oSjJ8YR6Rmej45cxr7n1LuycaN/vPz58xOy7i57hoLH3LnxpkJSYlt89TP3xXWP98T+rItPjkvTUy9KhuJ6JZNixaPpCpY5MXXmtKY+6vGkM0+IT7wh3QK0J+79yo/iyg67haNoPjqPze1x75ruLDGR7W8ddovWaDg2Y8rUOPao4h7a+zpuYizMOsmdtyu7bqJDEhOj89jMrvyf0HsCW/wCUBHq0Tra1sdmdlvO09mdtsdOmlnoJ+qhkkeFFq9o27lzOD8MtWe0R8+xOSnOf2Pqejxi1Y+e6tjL/tv52Fx8wq8VzpN2PfdMb1Ki55ja9NDK+MaqQ+Pc7KEAB0xJ37YlJnpkmvt/56Rnm+vY9NJPeHP2CMDCz16pquwSzlN6Rp57Jv05mBALU+/O2bAm+7Qr/ha29Kx5PUmJki8vW3f0fPB1zTigsHzxvzUv9nw0Lj5tUV9/FJHdMnDeecdlmcJG/yqzLyuypdjYYkNG+WshFFmnpZ9416y+LT3prN5se9bZ4M+yLyvTD5vS8zSNrZvjH3qTEtnUOOOw8qREKmBbIWcxPhbuKy5i0/Z4Kc2cNDvOX1oSu0MPjmsvOaIkvmmh4Q1Llh6bXfaWPox3xfe+ck/8xQ/29W0yvBLbd612PzaXZrdtXLa08rawQ+JdhY68Il58eujX3bRjNEfDsXnT538Y5/zl90r+fSfrLPnxwiPP0q95f/Wh78TbrlndjuEZVpvb/dhcvGRWnHFoxWVycw+Od/YmqJ9Y3hm3cYyGYzN1QP7EhvTdZ3y86Z0lHYXPOLj3EZMR69d0zolPux+bxT9I0199eJbAT2Nb4tZ/7rAvtIW9Ofuv3b/T9iYEx2e3r56UQtk3TIgFU3rOrSYM9Ctg33LeNFKg8Pe+kQUqq0kCWU+xl37q1HjrzzbGgVkP+j2P/dwcX/16zx/Cn6zuzu7znxwn/P6JcfURG2LCvNlxYuFxoFl7ss/CYqdnxT4Eps47PP5fe/cbWtd53wH8sWQlsuN/sxUncSPPSpm7pE0dtqxZvHUMtlAyBtubEcibQRl0sI69CKyMQRkb3QhLXmywFwslY3kRKBtraAtpN8JGofOyrWna0Jo49ZQ4TvwH/0viOEpkyXuOrmTJruzcK51z7k/3+VwIke8993l+z+d7j3T107nnPPOFLWl6x/r0rXyixS9/7bX0uf0fT2P5sj9PPr4p/fDIVNq+O19eNJ/3YPL04SsXlV81C+MtfWCx3bD03oWv88cTHrkn7d+a0nszQ+nmPG51IseHv3hfevDiUHrvx4fSZ58a/DdZC0bV59a+eeeZdPjdG/Ilijq/FB57/tXOJYl+cDZfdeX2fLnXW9OXH5lNPzo7lPb9/K1p7Kr3p9Ubnc45BPJlYT93Xxo/OZQ2n5tMv/N3+USZL1xIf5gP9b33oV9O//SpU+lkbnV05jlzZXYryTKf9f/PHlp4Y7U+3fWZe9M3fmuxx7l+eCr9w198t5yPdUTaNxd2tx7+/9FP7M4fG9qTfuO385V98tVg3r44mv/avqVz2HH+PPTTz5TRmBiIfTOfAPeN+RMaL74EpjvXYr84O/dYUW+d1/i++euf+WR6eGIo/dHJc+nw6Q/Shm0b0x23bersm/k8In/zr2U0hAdi38wnuv77Z4/nS6vnn+13fzxfWeWWdPBUSnffPdY5W9jJI+nRF8tpTFRXPwjznnbxm2WPXw2nzz+4a+455w++lr6y9BRPPY60VjcfhH3z0L8fTa8/tDON56OZ/jpfIef7PzyV3pxanz6x79Y0PteYOJ++9Z32jiRfq6+Fuupe/G2irhGNU7NAFdEH6bnnjqZTMzekvXfmpkT1S/37uSnx2Avp2fnZnnnipXTgZP4WceOmdP/+3XNNicmXjqbJufcts/naGfO3c8fT0/OXER2byIf15s+Q599v87XRj6c/+Nsfp2PV19VhwHd2mhLn8qVynlyyQ851svIlJy+PtzBu/v/1u1xDadeuLWls55Y0nt9YLfztf9P2TWk8N1R23bzkL/tLxhy0Lyuj85NH0nOvXEibdm6/3JR4/aWX0+891bmaQpo6mR79lzfnfpm47Wd2pV/7VG5KTJ1L335h7hILaekhkE999XA6Vh01kXPfO74xbZ4P4StP/G/654OdXyhvmxjrzDMzlQ58/f8uH2q48iyHrzigbSznty0f8r/w36Z8ks4yPvocad/sfCt/76qPfHWz/zz79VfSoTMfpPU3jqaJiZ1p33xT4vyx4+mxP//u5e8x3Yy1lrcZjH1zuQTmDw+f/96w3BaDd99g7Jvf+96b6Vz+ebtp57b8Ucudae9cU2I2HTv4avr8Hx/88JMbD0iwg7JvvvXfL6cvffPk3M/2bfmPR/fPNyVOvZLz/OIrl492HZDYrrGMwdg35xa37eZ0/22d9Xztq8evsd7Bvnsw9s2z6ff/Ml+xrjqiKV8hZ9/P7U4P7t8115SYOnMmPfFYmedP69crd92lfOvX5IMy710Pfilt/+lfbGg5w2nraL4M5Fwndjjt3b0h/8X7/fTikeU761tv2Zju2jyUTh/NV2SonpPPlrh14TKSSyrcuu3GdEvuBbybD/vPHyW/4vaRPMbu/NiRfGmyqx+7YsM19I8zr/1X+tGzf/qhFTebZf5ARnX2yvk8qgzuyodBnD56oZPVT1Q3nO7JeW/IbYADRzp/Gauev3CJ18XN8+vilurHQz6b8ImrXhejI+me23OY7+TXzNWPLQ6w5r6KkeeA7Zv5tbJ350iqPhXU5r4fI0v7Zl3fBGLkOVj7ZvWzYmJL5zPOk/lnQVtHvcTIchD3zc57uR03zKYj+ed/W++zYuQ5WPtmXd83ex0nRpaDt2/Ofa8dG0kb8sfeT+ffgeZ+j+o1nBVs322eKxh6zT2lqL+hrLl05gpeaEpU/5hJh/IZ8693e+vEhXRgyUmVFn4Jvvo5b5279pubN/IYb1z9BP+uRWBpU6HK4MAVZ+e+eoqZ3IC6Mu+lz1/cOr8uTnROkLl43/xXU4tXW/iJx9yxSoEB2zfza+XQNRqeq4RaE09fum/ZN9dEZNcpcrD2zer1+GL+r9Tb4O2bH/5ebnCzHqx9c3Bz6m5lg7Zvlv69trvUm92qc/xvs3MYnQABAgQIECBAgAABAgQIECCwrIDGxLIs7iRAgAABAgQIECBAgAABAgTaENCYaEPZHAQIECBAgAABAgQIECBAgMCyAhoTy7K4kwABAgQIECBAgAABAgQIEGhDQGOiDWVzECBAgAABAgQIECBAgAABAssKaEwsy+JOAgQIECBAgAABAgQIECBAoA0BjYk2lM1BgAABAgQIECBAgAABAgQILCugMbEsS293joysT5dmZ3p7kq1bE6iyqTLq5ibLbpT6u408++tf5+yyrFOz/2PJs/8Z1FWBLOuSjDGOPGPkUEcVsqxDMc4YveQZp+rmKtGYqMF2802jaXZmuoaRDNGEQJVNlVE3N1l2o9TfbeTZX/86Z5dlnZr9H0ue/c+grgpkWZdkjHHkGSOHOqqQZR2KccboJc84VTdXicZEDbYTt4+lmekLNYxkiCYEqmzuGB/ramhZdsXU143k2Vf+WieXZa2cfR9Mnn2PoLYCZFkbZYiB5BkihlqKkGUtjGEG6SXPMEU3WIjGRA24v/ILEyldfKeGkQzRiEDO5tP35oy6uMmyC6R+byLPfidQ3/yyrM8ywkjyjJBCPTXIsh7HKKPIM0oSq69Dlqs3jDRCD3lGKrupWjQmapB9YP/H0szU2XTp0qUaRjNEnQJVJlU2D/zSz3Y1rCy7YurbRj3nmXO3b/YtrutOLMvr8qy5B+W55iK7ZsE9Z+k90DUtIzwgzwgp1FODLOtxjDJKr3lGqbvJOjQmatDdc/uO9NHxHemDd0/XMJoh6hSoMqmy2fOR7V0NK8uumPq2Uc955tztm32L67oTy/K6PGvuQXmuuciuWXDPWXoPdE3LCA/IM0IK9dQgy3oco4zSa55R6m6yDo2JmnQf+eyvpkvvvZmPmpitaUTDrFagyqLKpMqml5sse9Fqb1t5tmfd9EyybFq43fHl2a53k7PJsknd9seWZ/vmTc0oy6Zk+zPuSvPsT7XtzaoxUZP1ffv2pPs+OZ6m3z5S04iGWa1AlUWVSZVNLzdZ9qLV3rbybM+66Zlk2bRwu+PLs13vJmeTZZO67Y8tz/bNm5pRlk3J9mfclebZn2rbm1Vjokbrv3rkN9Po0IX0/vkTNY5qqJUIVBlUWVSZrOQmy5WoNfcceTZn2/bIsmxbvNn55Nmsb5ujy7JN7ebnkmfzxm3NIMu2pNuZZ7V5tlNlf2bRmKjRfcum0fSPjz6cht8/oTlRo2uvQ1U7fJVBlUWVyUpuslyJWjPPkWczrv0YVZb9UG9uTnk2Z9v2yLJsW7zZ+eTZrG+bo8uyTe3m56ojz+ar7N8M6/IZQV1Komb/yaOn0+9+4ek0NbsxjWzZndat0/+pmXjZ4arPa1WHRlVHSlRNiYl8Qq7V3mS5WsGVP1+eK7eL9kxZRktkdfXIc3V+kZ4ty0hprL4Wea7eMMoIsoySRD11NJFnPZXFGkVjoqE83j4/lf7k8W+k53/welq3YVe64aYduUGxrqHZyh626q1VZ7atTnRZnVOi+hjGSo+UWE5SlsupNHefPJuzbXtkWbYt3ux88mzWt83RZdmmdvNzybN547ZmkGVb0u3M03Se7ayivVk0Jhq2fv77r6bHn/yPdPj102l49KdSWr85DY9sTEPDI2nd0HDDsw/m8JdmZ9LszHSamb6Q0sV30szU2blLQlZX0+j1RJe9CMmyF63ut5Vn91bRt5Rl9IR6q0+evXlF3lqWkdPpvTZ59m4W9RmyjJrMyurqV54rqzbeszQmWsrk1fzxjn/7z5fTt/9nMk0ePZXeeXcqTU9fbGn2wZpmZGR92nzTaLpjfCx9+t6J9MD+j6U9NXxso1slWXYr1d128uzOaS1sJcu1kFL3Ncqze6voW8oyekK91SfP3rwiby3LyOn0Xlu/8+y94ljP0JiIlYdqCBAgQIAAAQIECBAgQIBAUQLOylhU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBLQmIiVh2oIECBAgAABAgQIECBAgEBRAhoTRcVtsQQIECBAgAABAgQIECBAIJaAxkSsPFRDgAABAgQIECBAgAABAgSKEtCYKCpuiyVAgAABAgQIECBAgAABArEENCZi5aEaAgQIECBAgAABAgQIECBQlIDGRFFxWywBAgQIECBAgAABAgQIEIgloDERKw/VECBAgAABAgQIECBAgACBogQ0JoqK22IJECBAgAABAgQIECBAgEAsAY2JWHmohgABAgQIECBAgAABAgQIFCWgMVFU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBLQmIiVh2oIECBAgAABAgQIECBAgEBRAhoTRcVtsQQIECBAgAABAgQIECBAIJaAxkSsPFRDgAABAgQIECBAgAABAgSKEtCYKCpuiyVAgAABAgQIECBAgAABArEENCZi5aEaAgQIECBAgAABAgQIECBQlIDGRFFxWywBAgQIECBAgAABAgQIEIgloDERKw/VECBAgAABAgQIECBAgACBogQ0JoqK22IJECBAgAABAgQIECBAgEAsAY2JWHmohgABAgQIECBAgAABAgQIFCWgMVFU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBLQmIiVh2oIECBAgAABAgQIECBAgEBRAhoTRcVtsQQIECBAgAABAgQIECBAIJaAxkSsPFRDgAABAgQIECBAgAABAgSKEtCYKCpuiyVAgAABAgQIECBAgAABArEENCZi5aEaAgQIECBAgAABAgQIECBQlIDGRFFxWywBAgQIECBAgAABAgQIEIgloDERKw/VECBAgAABAgQIECBAgACBogQ0JoqK22IJECBAgAABAgQIECBAgEAsAY2JWHmohgABAgQIECBAgAABAgQIFCWgMVFU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBLQmIiVh2oIECBAgAABAgQIECBAgEBRAhoTRcVtsQQIECBAgAABAgQIECBAIJaAxkSsPFRDgAABAgQIECBAgAABAgSKEtCYKCpuiyVAgAABAgQIECBAgAABArEENCZi5aEaAgQIECBAgAABAgQIECBQlIDGRFFxWywBAgQIECBAgAABAgQIEIgloDERKw/VECBAgAABAgQIECBAgACBogQ0JoqK22IJECBAgAABAgQIECBAgEAsAY2JWHmohgABAgQIECBAgAABAgQIFCWgMVFU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBLQmIiVh2oIECBAgAABAgQIECBAgEBRAhoTRcVtsQQIECBAgAABAgQIECBAIJaAxkSsPFRDgAABAgQIECBAgAABAgSKEtCYKCpuiyVAgAABAgQIECBAgAABArEENCZi5aEaAgQIECBAgAABAgQIECBQlIDGRFFxWywBAgQIECBAgAABAgQIEIgloDERKw/VECBAgAABAgQIECBAgACBogQ0JoqK22IJECBAgAABAgQIECBAgEAsAY2JWHmohgABAgQIECBAgAABAgQIFCWgMVFU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBLQmIiVh2oIECBAgAABAgQIECBAgEBRAhoTRcVtsQQIECBAgAABAgQIECBAIJaAxkSsPFRDgAABAgQIECBAgAABAgSKEtCYKCpuiyVAgAABAgQIECBAgAABArEENCZi5aEaAgQIECBAgAABAgQIECBQlIDGRFFxWywBAgQIECBAgAABAgQIEIgloDERKw/VECBAgAABAgQIECBAgACBogQ0JoqK22IJECBAgAABAgQIECBAgEAsAY2JWHmohgABAgQIECBAgAABAgQIFCWgMVFU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBLQmIiVh2oIECBAgAABAgQIECBAgEBRAhoTRcVtsQQIECBAgAABAgQIECBAIJaAxkSsPFRDgAABAgQIECBAgAABAgSKEtCYKCpuiyVAgAABAgQIECBAgAABArEENCZi5aEaAgQIECBAgAABAgQIECBQlIDGRFFxWywBAgQIECBAgAABAgQIEIgloDERKw/VECBAgAABAgQIECBAgACBogQ0JoqK22IJECBAgAABAgQIECBAgEAsAY2JWHmohgABAgQIECBAgAABAgQIFCWgMVFU3BZLgAABAgQIECBAgAABAgRiCWhMxMpDNQQIECBAgAABAgQIECBAoCgBjYmi4rZYAgQIECBAgAABAgQIECAQS0BjIlYeqiFAgAABAgQIECBAgAABAkUJaEwUFbfFEiBAgAABAgQIECBAgACBWAIaE7HyUA0BAgQIECBAgAABAgQIEChKQGOiqLgtlgABAgQIECBAgAABAgQIxBL4f75SxuneZm8xAAAAAElFTkSuQmCC" /></p>
<section id="formulation">
<h3>3.1. Formulation<a class="headerlink" href="#formulation" title="Link to this heading">#</a></h3>
<p>Typically, the problem is formulated as an optimization problem (Wei Wu, 2011):
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-45d59ad3-4800-4413-a536-28247bc259f2">
<span class="eqno">(36)<a class="headerlink" href="#equation-45d59ad3-4800-4413-a536-28247bc259f2" title="Permalink to this equation">#</a></span>\[\begin{equation}
\theta_t^* = \arg \underset{\theta}{\min} \frac 1 t \sum_{i=1}^{t} \left(L(f_{\theta}(x_i), y_i) + R(\theta)\right)
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where $t$ is the number of data points, $\theta_t^*$ is the parameter that minimize the empirical cost, $(x_i, y_i)$ are the $i^{th}$ training example, $L(s, y)$ is a loss function which gives small value if $s$ is a good prediction for $y$, and $R(\theta)$ is a regularization function for $\theta$  which typically gives small value for small $\theta$. \\For stochastic gradient descent (SGD) method, let $d=(x, y)$ be one data sample, $l(\theta, d) = L(f_{\theta}(x), y) + R(\theta)$ be the cost of $\theta$ for $d$, $g(\theta, \xi) = \frac{\partial l(\theta, d)}{\partial \theta}$ be the gradient function, and $D_t = (d_1, \cdots, d_t)$ be all the training samples at $t^{th}$ step. The SGD method updates $\theta$ according to its  stochastic gradient:
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-939d59bd-9d7b-4c93-90a6-49b58316d314">
<span class="eqno">(37)<a class="headerlink" href="#equation-939d59bd-9d7b-4c93-90a6-49b58316d314" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \theta_t = \theta_{t-1} - \alpha g(\theta_{t-1}, d_t)
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[The formulation of gradient descent is very similar to the above equation but focusing on total data samples in each iteration:
\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-390c402b-8ed1-4656-a3c1-7bca4c298ec5">
<span class="eqno">(38)<a class="headerlink" href="#equation-390c402b-8ed1-4656-a3c1-7bca4c298ec5" title="Permalink to this equation">#</a></span>\[\begin{equation}
\theta_t = \theta_{t-1} - \alpha G(\theta_{t-1})
\end{equation}\]</div>
<p>$$</p>
<p>The algorithm involves three main steps:</p>
<ol class="arabic simple">
<li><p>Random selection of a sample or a set of samples from the data set.</p></li>
<li><p>Computation of the gradient of the objective function.</p></li>
<li><p>Updating the values of the variables in the optimization problem based on the
gradient and step length.</p></li>
</ol>
<p><strong>Pseudocode:</strong>
(<em>The different parts in pseudocode compared to gradient descent are in bold.</em>)</p>
<ul>
<li><p>Initialize the starting point <span class="math notranslate nohighlight">\(\theta^0\)</span> and the model hyperparameters (e.g., learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> where  <span class="math notranslate nohighlight">\(0 &lt; \alpha \leq 1\)</span> and minibatch size <span class="math notranslate nohighlight">\(n\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is an integer and <span class="math notranslate nohighlight">\(1\leq n \leq \)</span> number of data samples)</p></li>
<li><p>In each <strong>epoch</strong>:</p>
<ol class="arabic simple">
<li><p>Shuffle and split the training data set by size <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
<li><p>In the <span class="math notranslate nohighlight">\(t\)</span>âs <strong>batch</strong>:</p></li>
</ol>
<p>(a) compute the gradient <span class="math notranslate nohighlight">\(\nabla \theta^t\)</span> of the current batch</p>
<p>(b) update to get <span class="math notranslate nohighlight">\(\theta^{t+1} = \theta^t - \alpha \nabla \theta^t\)</span></p>
<p>(c) (Optional) Check convergence: if <span class="math notranslate nohighlight">\(\lvert \theta^{t+1} - \theta^t \rvert &lt; \epsilon\)</span>, then STOP.</p>
</li>
</ul>
<p><em>To be mentioned, in deep learning compared with iteration/step, âepochâ can be understood as the number of times the algorithm scans the entire data. Although we add the convergence checking procedure, in deep learning we usually take the epoch number or comparison to test set as stopping criterion.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sgd</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
        <span class="n">costfunc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_flag</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function of stochastic gradient descent algorithm</span>

<span class="sd">    Arguments:</span>
<span class="sd">        grad = the function or any Python callable object that </span>
<span class="sd">                  takes a vector and returns the gradient of the function to minimize</span>
<span class="sd">        x = arrays of inputs/features</span>
<span class="sd">        y = arrays of outputs/observations</span>
<span class="sd">        start = the points where the algorithm starts its search</span>
<span class="sd">        lr = the learning rate that controls the magnitude of the vector update</span>
<span class="sd">        batch_size = specify the number of observations in each minibatch</span>
<span class="sd">        n_iter = the number of iterations</span>
<span class="sd">        tol = tolerance to stop the algorithm</span>
<span class="sd">        dtype = the data type of NumPy array</span>
<span class="sd">        </span>
<span class="sd">    Arguments (optional):</span>
<span class="sd">        random_state = the seed of the random number generator for stochaisticity</span>
<span class="sd">        trajectory = boolean to determine if return trajectories or optima</span>
<span class="sd">        costfunc = function to compute cost</span>
<span class="sd">        verbose = boolean to determine if printing out</span>
<span class="sd">        check_flag = boolean to determine whether checking convergence</span>

<span class="sd">    Returns:</span>
<span class="sd">        vector = minimizer solution</span>
<span class="sd">        traj = optimization trajectories</span>
<span class="sd">        cost = trajectories of how cost changes</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Checking if the gradient is callable</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;&#39;grad&#39; must be callable&quot;</span><span class="p">)</span>

    <span class="c1"># Setting up the data type for NumPy arrays</span>
    <span class="n">dtype_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Converting x and y to NumPy arrays</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">n_obs</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;x&#39; and &#39;y&#39; lengths do not match&quot;</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="c1"># Initializing the random number generator</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Initializing the values of the variables</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">traj</span> <span class="o">=</span> <span class="n">vector</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># Setting up and checking the learning rate</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">lr</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;lr&#39; must be greater than zero&quot;</span><span class="p">)</span>

    <span class="c1"># Setting up and checking the size of minibatches</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="n">n_obs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;batch_size&#39; must be greater than zero and less than &quot;</span>
            <span class="s2">&quot;or equal to the number of observations&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Setting up and checking the maximal number of iterations</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n_iter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;n_iter&#39; must be greater than zero&quot;</span><span class="p">)</span>

    <span class="c1"># Setting up and checking the tolerance</span>
    <span class="n">tol</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tol</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tol</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;tol&#39; must be greater than zero&quot;</span><span class="p">)</span>

    <span class="c1"># Setting the difference to zero for the first iteration</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
    
    <span class="c1"># Setting the flag to stop the nested loops</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
    
    <span class="c1"># Performing the gradient descent loop</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="c1"># Shuffle x and y</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>

        <span class="c1"># Performing minibatch moves</span>
        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">stop</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">batch_size</span>
            <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">)</span>

            <span class="c1"># Compute the cost</span>
            <span class="k">if</span> <span class="n">costfunc</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cost</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">costfunc</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">vector</span><span class="p">))</span>
            
            <span class="c1"># Add your solution here</span>
        
        <span class="c1"># Break the nested loop</span>
        <span class="k">if</span> <span class="n">flag</span><span class="p">:</span> <span class="k">break</span>
    
    <span class="c1"># print out results</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>   
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> epochs, the current theta = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span> <span class="p">)</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trajectory</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">vector</span> <span class="k">if</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">vector</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">traj</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Just a quick test of <code class="docutils literal notranslate"><span class="pre">sgd</span></code> function on the previous OLS data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># SGD quick test on previous OLS example</span>
<span class="n">OLS_opt</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span> 
              <span class="n">lr</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> 
              <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check_flag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After 35317 epochs, the current theta = [5.62822349 0.54012867]
</pre></div>
</div>
</div>
</div>
</section>
<section id="parametric-study">
<h3>3.2. Parametric study<a class="headerlink" href="#parametric-study" title="Link to this heading">#</a></h3>
<p>In this part we use a synthetic dataset and fit a model to predict <span class="math notranslate nohighlight">\(y\)</span> from a given <span class="math notranslate nohighlight">\(x_0\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>.</p>
<p>Now the multivariate linear regression model could be regarded as:</p>
<div class="math notranslate nohighlight">
\[
y = b + w^Tx
\]</div>
<p>where <span class="math notranslate nohighlight">\(b\)</span> is scalar, <span class="math notranslate nohighlight">\(w = (w_0, w_1, \cdots, w_n)\)</span>,  <span class="math notranslate nohighlight">\(x = (x_0, x_1, \cdots, x_n)\)</span> and <span class="math notranslate nohighlight">\(y = (y_0, y_1, \cdots, y_n)\)</span></p>
<p>Or</p>
<div class="math notranslate nohighlight">
\[
y = \theta^Tx
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta = (\theta_0, \theta_1, \cdots, \theta_{n+1})\)</span> and <span class="math notranslate nohighlight">\(y = (y_0, y_1, \cdots, y_n)\)</span> but <span class="math notranslate nohighlight">\(x = (1, x_0, x_1, \cdots, x_n)\)</span></p>
<p>The objective function is:</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta} = \sum_{j=1}^n \mathcal{L}(y_j, f(x_j, \theta)) = \sum_{j=1}^n \mathcal{L}(y_j, \theta^Tx_j)
\]</div>
<p>The cost function is:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta) = \frac{1}{2n}\sum_{j=1}^n (\theta^T x_j - y_i)^2
\]</div>
<p>Therefore, the gradient of cost function is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \theta_i} = \frac{1}{n} \sum_{j} \left(\theta^Tx_j - y_j\right)x_i = \text{mean} \left(\left(\theta^Tx_j - y_j\right) x_i \right)
\]</div>
<p><strong>Question:</strong> Some people believe the <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> in cost function is only convenient to cancel when calculating the derivatives. However, there is a further reason, please try to answer it.</p>
<!-- https://math.stackexchange.com/questions/884887/why-divide-by-2m --><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">multivariate_normal</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>

<span class="c1"># Generate the dataset</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lg_generate</span><span class="p">():</span>
    <span class="n">NPoints</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Number of (x,y) pairs in synthetic dataset</span>
    <span class="n">noiseSD</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># True noise standard deviation</span>

    <span class="c1"># Get x points from a uniform distribution from [-1  to 1]</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">uniform</span><span class="p">()</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">NPoints</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">uniform</span><span class="p">()</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">NPoints</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">3</span>

    <span class="c1"># trasnform x1</span>
    <span class="n">x1</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>

    <span class="c1"># Random Error from a gaussian</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noiseSD</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">NPoints</span><span class="p">)</span>

    <span class="c1"># True regression parameters that we wish to recover</span>
    <span class="c1"># for the following visualization, we only activate parameters for w</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">w0</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>

    <span class="c1"># Get the y from given x using specified weights </span>
    <span class="c1"># and add noise from gaussian with true noise </span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="n">w0</span><span class="o">*</span><span class="n">x0</span> <span class="o">+</span> <span class="n">w1</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="n">epsilon</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x0&#39;</span><span class="p">:</span> <span class="n">x0</span><span class="p">,</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span> <span class="n">x1</span><span class="p">}</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># prepare the data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">lg_generate</span><span class="p">()</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-a63c7556-eed8-4f2e-9010-43aafc31cebe">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-13.321243</td>
      <td>0.740248</td>
      <td>7.840247</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-2.924226</td>
      <td>0.164554</td>
      <td>1.777122</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-12.102443</td>
      <td>-0.442322</td>
      <td>5.360247</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-2.333845</td>
      <td>-0.628178</td>
      <td>0.259101</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-12.753478</td>
      <td>-0.177800</td>
      <td>6.178104</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>495</th>
      <td>-3.950203</td>
      <td>-0.970749</td>
      <td>0.493724</td>
    </tr>
    <tr>
      <th>496</th>
      <td>-3.649303</td>
      <td>-0.905959</td>
      <td>0.452464</td>
    </tr>
    <tr>
      <th>497</th>
      <td>-5.681409</td>
      <td>-0.711615</td>
      <td>1.693122</td>
    </tr>
    <tr>
      <th>498</th>
      <td>-2.666745</td>
      <td>-0.911644</td>
      <td>0.002498</td>
    </tr>
    <tr>
      <th>499</th>
      <td>-12.825002</td>
      <td>0.677941</td>
      <td>7.429958</td>
    </tr>
  </tbody>
</table>
<p>500 rows Ã 3 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-a63c7556-eed8-4f2e-9010-43aafc31cebe')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-a63c7556-eed8-4f2e-9010-43aafc31cebe button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-a63c7556-eed8-4f2e-9010-43aafc31cebe');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  </div></div>
</div>
<section id="cost-function">
<h4>3.2.1. Cost function<a class="headerlink" href="#cost-function" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; </span>
<span class="sd">    Function to calculate cost function assuming a hypothesis of form h = theta.T*X</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        X = array of features</span>
<span class="sd">        y = array of training examples</span>
<span class="sd">        theta = array of parameters for hypothesis</span>

<span class="sd">    Returns:</span>
<span class="sd">        J = cost function</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c1"># Number of data points </span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Insert unit column to X feature matrix</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

    <span class="c1"># Compute predictions</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

    <span class="c1"># Compute cost</span>
    <span class="n">J</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">h</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">J</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">SSR gradient has been defined in the first section, here is just a backup.</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="c1"># # Compute the SSR of linear regression model</span>
<span class="c1"># def ssr_gradient(x, y, theta):</span>
<span class="c1">#     &#39;&#39;&#39;</span>
<span class="c1">#     Function to calculate the gradient of SSR</span>

<span class="c1">#     Arguments:</span>
<span class="c1">#         x = the given value for features</span>
<span class="c1">#         y = the observations</span>
<span class="c1">#         theta = parameter vector for linear model</span>
    
<span class="c1">#     Returns: the gradient of SSR</span>
<span class="c1">#     &#39;&#39;&#39;</span>
<span class="c1">#     # Number of data points</span>
<span class="c1">#     m = len(y)</span>

<span class="c1">#     # Insert unit column to X feature matrix</span>
<span class="c1">#     ones = np.ones(m)</span>
<span class="c1">#     x_new = np.vstack((ones,x.T)).T</span>

<span class="c1">#     # Compute residual between prediction and observation</span>
<span class="c1">#     res = np.dot(x_new, theta) - y</span>
    
<span class="c1">#     return np.dot(res, x_new)/m</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;\nSSR gradient has been defined in the first section, here is just a backup.\n&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the surface for all values of a0 and a1 between [-6 and 6]</span>
<span class="k">def</span><span class="w"> </span><span class="nf">costMaker</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Vectorize the cost field</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">out</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">):</span>     
        <span class="k">return</span> <span class="n">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># Set up the plot range</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">w0_grid</span> <span class="o">=</span> <span class="n">w1_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">G0</span><span class="p">,</span> <span class="n">G1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w0_grid</span><span class="p">,</span> <span class="n">w1_grid</span><span class="p">)</span>

<span class="c1"># Convert from pandas data frame to numpy array</span>
<span class="n">X</span>  <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Compute the contours of costs</span>
<span class="n">costfunc</span><span class="o">=</span> <span class="n">costMaker</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">cost_2d</span> <span class="o">=</span> <span class="n">costfunc</span><span class="p">(</span><span class="n">G0</span><span class="p">,</span> <span class="n">G1</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">G0</span><span class="p">,</span> <span class="n">G1</span><span class="p">,</span> <span class="n">cost_2d</span><span class="p">,</span>  <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;terrain&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_0$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;$w_1$&#39;)
</pre></div>
</div>
<img alt="../../_images/80616d59aa877794072121a24f8079f647940c66a3e2fc6f31b3adafe1bdb18f.png" src="../../_images/80616d59aa877794072121a24f8079f647940c66a3e2fc6f31b3adafe1bdb18f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Choose learning rate = 0.12</span>
<span class="n">traj_out</span><span class="p">,</span> <span class="n">cost_out</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.11</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">costfunc</span><span class="o">=</span><span class="n">cost</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Visualization</span>

<span class="c1"># Plot cost</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cost_out</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

<span class="c1"># Plot trajectory of w_0, w_1</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">G0</span><span class="p">,</span> <span class="n">G1</span><span class="p">,</span> <span class="n">cost_2d</span><span class="p">,</span>  <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;terrain&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_out</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">traj_out</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Parameters&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">traj_out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Starting points&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_0$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f67f020b220&gt;
</pre></div>
</div>
<img alt="../../_images/1cef976f2e76dd5a6d36d2e0cc63354f88b8f94f7ca710307a472dd54936c42f.png" src="../../_images/1cef976f2e76dd5a6d36d2e0cc63354f88b8f94f7ca710307a472dd54936c42f.png" />
</div>
</div>
</section>
<section id="change-values-of-alpha">
<h4>3.2.2. Change Values of <span class="math notranslate nohighlight">\(\alpha\)</span><a class="headerlink" href="#change-values-of-alpha" title="Link to this heading">#</a></h4>
<p>Similar to the section of gradient descent, we can change the hyperparameter learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> and explore its effect for stochastic gradient descent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare alpha list</span>
<span class="n">alpha_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.11</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.125</span><span class="p">]</span>

<span class="c1">## Visualization</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="c1"># Find trajectory for the current alpha</span>
    <span class="n">traj_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="n">alpha_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">costfunc</span><span class="o">=</span><span class="n">cost</span><span class="p">)</span>
    <span class="c1"># Plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">G0</span><span class="p">,</span> <span class="n">G1</span><span class="p">,</span> <span class="n">cost_2d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;terrain&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_out</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">traj_out</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Parameters&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">traj_out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Starting points&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;lr = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_0$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e633191e54fd064d91225664a4505c6b40a42bbaa91d341836f3bb6e57c3f033.png" src="../../_images/e633191e54fd064d91225664a4505c6b40a42bbaa91d341836f3bb6e57c3f033.png" />
</div>
</div>
<p><strong>Discussion</strong></p>
<p>Choosing the learning rate is pretty challenging. If the learning rate is too small, which may result in a long training process that could get stuck; whereas a learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, or make the training unstable with many big fluctuations.</p>
<p>From the above plots, I took different values of learning rate with the same iteration times (e.g. 100) and full batch. It is easy to see that when <span class="math notranslate nohighlight">\(\alpha = 0.12\)</span> (just slightly greater than the given value), the zig-zag behavior of the cost function becomes more unstable and harder to converge. Another small increment (e.g. <span class="math notranslate nohighlight">\(\alpha \geq 0.125\)</span>) would make the computation blow up. When <span class="math notranslate nohighlight">\(\alpha\)</span> is very small (e.g. <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, within the 100 iterations the algorithm cannot reach out the minimum, the rate is slowed down by small <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</section>
<section id="change-values-of-batch-size">
<h4>3.2.3. Change Values of Batch Size<a class="headerlink" href="#change-values-of-batch-size" title="Link to this heading">#</a></h4>
<p>Since we know the minibatch is the key of stochastic gradient descent, here we change the batch size and analyze its relation to training loss curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare minibatch size list</span>
<span class="n">batch_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="c1"># Plot the cost vs number of batch seen</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">cost_out</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">costfunc</span><span class="o">=</span><span class="n">cost</span><span class="p">)</span>   
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cost_out</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Batches Seen&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Batch = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">batch_list</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e965b4a9f482715e29b951878f81270ad3a705fc3cef542b8e196157e89c397c.png" src="../../_images/e965b4a9f482715e29b951878f81270ad3a705fc3cef542b8e196157e89c397c.png" />
</div>
</div>
<p><strong>Discussion:</strong> From the plots of cost versus batches, we can see after 100 epochs, minibatch size <span class="math notranslate nohighlight">\(n=1\)</span> seems to find lowest error; with <span class="math notranslate nohighlight">\(n\)</span> increasing, the fluctuations become smaller, but it also becomes harder to convergent.</p>
<p>However, is it reasonable only comparing the costs along batches instead of epochs?</p>
</section>
</section>
<section id="computational-cost-analysis">
<h3>3.3. Computational cost analysis<a class="headerlink" href="#computational-cost-analysis" title="Link to this heading">#</a></h3>
<p>Instead of showing the cost vs the number of batch seen, we need to average the costs over batches in each epoch for relevent comparison.</p>
<p>Since we need to shuffle the data set and split which involves efficiency of codes and all cases did not converge into the same error, so it is not reasonable if we directly record the wall time for each running. So we should count or use computational complexity for comparison. Consider the objective function is:</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta} = \sum_{j=1}^n \mathcal{L}(y_j, f_{\theta}(x_j))
\]</div>
<p>And the gradient of cost function is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \theta_i} = \frac{1}{n} \sum_{j} \left(f_{ \theta}(x_j) - y_j\right)x_i
\]</div>
<p>Assume the total data number is <span class="math notranslate nohighlight">\(N\)</span>, after each epoch we evaluate <span class="math notranslate nohighlight">\(f(\cdot)\)</span> by <span class="math notranslate nohighlight">\(N\)</span> times, and evaluate <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial \theta}\)</span> by <span class="math notranslate nohighlight">\(0\)</span> times. So in the above example varying batch size, we set all epochs <span class="math notranslate nohighlight">\( = 100\)</span>, theoretically all of them took the same total evaluation times as <span class="math notranslate nohighlight">\(100N\)</span>. Roughly speaking, all of them took the same computational cost.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot cost vs number of epochs</span>
<span class="k">for</span> <span class="n">i</span>  <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch_list</span><span class="p">)):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">cost_out</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">ssr_gradient</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">costfunc</span><span class="o">=</span><span class="n">cost</span><span class="p">)</span>
    <span class="c1"># compute the average cost in each epoch</span>
    <span class="c1"># Add your solution here</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cost_out_epoch</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;batch = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">batch_list</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f67bbc3bc10&gt;
</pre></div>
</div>
<img alt="../../_images/4a5faf0836e4e40316ec0ac9db534b22f3e6a4edc2acca2096ec9fdf788f046a.png" src="../../_images/4a5faf0836e4e40316ec0ac9db534b22f3e6a4edc2acca2096ec9fdf788f046a.png" />
</div>
</div>
<p><strong>Discussion:</strong></p>
<p>From the plot of cost versus epochs, the model cannot converge after 100 epochs if choosing batch size <span class="math notranslate nohighlight">\(n = 250\)</span> and <span class="math notranslate nohighlight">\(n = 500\)</span>. However, if choosing batch size too small, e.g. <span class="math notranslate nohighlight">\(n = 1\)</span>, there are so many big fluctuations and cost is hard to decrease. Usually, size of minibatch is from 50 to 256 but case-dependent.</p>
</section>
<section id="takeaway-conclusion">
<h3>3.4. Takeaway conclusion<a class="headerlink" href="#takeaway-conclusion" title="Link to this heading">#</a></h3>
<p><strong>Pros:</strong></p>
<ul class="simple">
<li><p>Since only some samples are used to calculate the gradient in each update, the training rate is fast and contains a certain degree of randomness, but from the perspective of expectation, the gradient calculated each time is basically the correct derivative. Although SGD sometimes fluctuates obviously and takes many detours, the requirements for gradients are very low (calculating gradients is fast for small batch). And for the introduction of noise, a lot of theoretical and practical work has proved that as long as the noise is not particularly large, SGD can converge well .</p></li>
<li><p>Training is fast when applied to large datasets. For example, take hundreds of data points from millions of data samples each time, calculate an minibatch gradient, and update the model parameters. Compared with traversing all samples of the standard gradient descent method, it is much faster to update the parameters every time a sampled minibatch as input.</p></li>
</ul>
<p><strong>Cons:</strong></p>
<ul class="simple">
<li><p>Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p></li>
<li><p>Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a datasetâs characteristics.</p></li>
<li><p>Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.</p></li>
<li><p>Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p></li>
</ul>
</section>
<section id="variants-of-sgd">
<h3>3.5. Variants of SGD<a class="headerlink" href="#variants-of-sgd" title="Link to this heading">#</a></h3>
<p>Due to the challenges for vanilla SGD, some effective variants of SGD become popular:</p>
<ol class="arabic simple">
<li><p><strong>Momentum:</strong> Instead of depending only on the current gradient to update the weight, gradient descent with momentum (Polyak, 1964) replaces the current gradient with m (âmomentumâ), which is an aggregate of gradients. This aggregate is the exponential moving average of current and past gradients (i.e. up to time <span class="math notranslate nohighlight">\(t\)</span>).</p></li>
<li><p><strong>AdaGrad:</strong> Adaptive gradient, or AdaGrad (Duchi et al., 2011), acts on the learning rate component by dividing the learning rate by the square root of <span class="math notranslate nohighlight">\(\nu\)</span>, which is the cumulative sum of current and past squared gradients (i.e. up to time <span class="math notranslate nohighlight">\(t\)</span>). Note that the gradient component remains unchanged like in SGD.</p></li>
<li><p><strong>RMSprop:</strong> Root mean square prop or RMSprop (Hinton et al., 2012) is another adaptive learning rate that tries to improve AdaGrad. Instead of taking the cumulative sum of squared gradients like in AdaGrad, it takes the exponential moving average of these gradients.</p></li>
<li><p><strong>Adadelta:</strong> Adadelta (Zeiler, 2012) is also another improvement from AdaGrad, focusing on the learning rate component. Adadelta is probably short for âadaptive deltaâ, where delta here refers to the difference between the current weight and the newly updated weight.</p></li>
<li><p><strong>Nesterov Accelerated Gradient (NAG):</strong> a similar update to momentum for was implemented using Nesterov Accelerated Gradient (Sutskever et al., 2013).</p></li>
<li><p><strong>Adam:</strong> Adaptive moment estimation, or Adam (Kingma &amp; Ba, 2014), is simply a combination of momentum and RMSprop. It acts upon the gradient component, the exponential moving average of gradients (like in momentum), and the learning rate component by dividing the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> by square root of <span class="math notranslate nohighlight">\(\nu\)</span>, the exponential moving average of squared gradients (like in RMSprop).</p></li>
<li><p><strong>AdaMax:</strong> AdaMax (Kingma &amp; Ba, 2015) is an adaptation of the Adam optimizer by the same authors using infinity norms (hence âmaxâ).</p></li>
<li><p><strong>Nadam:</strong> Nadam (Dozat, 2015) is an acronym for Nesterov and Adam optimizer. The Nesterov component here, is a more efficient modification.</p></li>
<li><p><strong>AMSGrad:</strong> Another variant of Adam is the AMSGrad (Reddi et al., 2018). This variant revisits the adaptive learning rate component in Adam and changes it to ensure that the current <span class="math notranslate nohighlight">\(\nu\)</span> is always larger than the <span class="math notranslate nohighlight">\(\nu\)</span> from the previous time step.</p></li>
</ol>
<p>The following table shows these optimizers differ: (1) adapt the âgradient componentâ and (2) adapt the âlearning rate componentâ. More details can be found at <a class="reference external" href="https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9">here</a>.</p>
<p><img alt="variantSGD.webp" src="data:image/webp;base64,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" /></p>
</section>
</section>
<section id="comparison-between-gd-and-newton-based-method">
<h2>4. Comparison between GD and Newton-based method<a class="headerlink" href="#comparison-between-gd-and-newton-based-method" title="Link to this heading">#</a></h2>
<p>After case study and parametric study on SGD and GD methods, we want to further compare the behavior of gradient descent and other Newton-based methods as <strong>Homework: Algorithm 3</strong>. As discussed in previous sections, shuffling and splitting data into batches then optimizing each minibatch by gradient descent, which is exactly the meaning of âstochasticâ. Theoretically, we can use any optimizer âstochasticallyâ by minibatches. In this section, we would not use data set anymore, however, we would use several functions to imitate the shape of trainable parameter space and explore the performance of these methods.</p>
<section id="newton-based-method">
<h3>4.1. Newton-based method<a class="headerlink" href="#newton-based-method" title="Link to this heading">#</a></h3>
<p>The following codes are obtained from <strong>Homework: Algorithm 3</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">linalg</span>

<span class="k">def</span><span class="w"> </span><span class="nf">check_nan</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>

<span class="c1">## Linear algebra calculation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">xxT</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Calculates u*u.T to circumvent limitation with SciPy</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    u - numpy 1D array</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    u*u.T</span>
<span class="sd">    </span>
<span class="sd">    Assume u is a nx1 vector.</span>
<span class="sd">    Recall: NumPy does not distinguish between row or column vectors</span>
<span class="sd">    </span>
<span class="sd">    u.dot(u) returns a scalar. This functon returns an nxn matrix.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
            <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">A</span>

<span class="k">def</span><span class="w"> </span><span class="nf">unconstrained_newton</span><span class="p">(</span><span class="n">calc_f</span><span class="p">,</span><span class="n">calc_grad</span><span class="p">,</span><span class="n">calc_hes</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                         <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;Newton&quot;</span><span class="p">,</span><span class="n">globalization</span><span class="o">=</span><span class="s2">&quot;Line-search&quot;</span><span class="p">,</span> <span class="c1"># specify algorithm</span>
                         <span class="n">eps_dx</span><span class="o">=</span><span class="mf">1E-6</span><span class="p">,</span><span class="n">eps_df</span><span class="o">=</span><span class="mf">1E-6</span><span class="p">,</span> <span class="c1"># Convergence tolerances (all)</span>
                         <span class="n">eta_ls</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">rho_ls</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">alpha_max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="c1"># line search parameters</span>
                         <span class="n">delta_max_tr</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span><span class="n">delta_0_tr</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="c1"># trust region parameters</span>
                         <span class="n">kappa_1_tr</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">kappa_2_tr</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">gamma_tr</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span> <span class="c1"># trust region parameters</span>
                         <span class="n">output</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># output iterative updates</span>
                        <span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Newton-Type Methods for Unconstrained Nonlinear Continuous Optimization</span>
<span class="sd">    </span>
<span class="sd">    Arguments (required):</span>
<span class="sd">        calc_f : function f(x) to minimize [scalar]</span>
<span class="sd">        calc_grad: gradient of f(x) [vector]</span>
<span class="sd">        calc_hes: Hessian of f(x) [matrix]</span>
<span class="sd">    </span>
<span class="sd">    Arguments (options):</span>
<span class="sd">        algorithm : specify main algorithm.</span>
<span class="sd">            choices: &quot;Newton&quot;, &quot;SR1&quot;, &quot;BFGS&quot;, &quot;Steepest-descent&quot;</span>
<span class="sd">        </span>
<span class="sd">        globalization : specify globalization strategy</span>
<span class="sd">            choices: &quot;none&quot;, &quot;Line-search&quot;, &quot;Trust-region&quot;</span>
<span class="sd">       </span>
<span class="sd">        eps_dx : tolerance for step size norm (convergence), eps1 in notes</span>
<span class="sd">        </span>
<span class="sd">        eps_df : tolerance for gradient norm (convergence), eps2 in notes</span>
<span class="sd">        </span>
<span class="sd">        eta_ls : parameter for Goldstein-Armijo conditions (line search only)</span>
<span class="sd">        </span>
<span class="sd">        rho_ls : parameter to shrink (backstep) line search</span>
<span class="sd">        </span>
<span class="sd">        alpha_max : initial step length scaling for line search and/or steepest-descent</span>
<span class="sd">    </span>
<span class="sd">        delta_max_tr : maximum trust region size</span>
<span class="sd">        </span>
<span class="sd">        delta_0_tr : initial trust region size</span>
<span class="sd">        </span>
<span class="sd">        kappa_1_tr : &#39;shrink&#39; tolerance for trust region</span>
<span class="sd">        </span>
<span class="sd">        kappa_2_tr : &#39;expand&#39; tolerance for trust region</span>
<span class="sd">        </span>
<span class="sd">        gamma_tr : &#39;accept step&#39; tolerance for trust region</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        x : iteration history for x (decision variables) [list of numpy arrays]</span>
<span class="sd">        f : iteration history for f(x) (objective values) [list of numpy arrays]</span>
<span class="sd">        p : iteration history for p (steps)</span>
<span class="sd">        B : iteration history for Hessian approximations [list of numpy arrays]</span>
<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="c1"># Allocate outputs as empty lists</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># decision variables</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># objective values</span>
    <span class="n">p</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># steps</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># gradients</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># line search / steepest descent step scalar</span>
    <span class="n">B</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Hessian approximation</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># trust region size</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># trust region actual/prediction ratio</span>
    <span class="n">step_accepted</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># step status for trust region. True means accepted. False means rejected.</span>
    
    <span class="c1"># Note: alpha, delta and rho will remain empty lists unless used in the algorithm below</span>
    
    <span class="c1"># Store starting point</span>
    <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">flag</span> <span class="o">=</span> <span class="kc">True</span>
    
    <span class="c1"># Print header for iteration information</span>
    <span class="k">if</span> <span class="n">output</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iter. </span><span class="se">\t</span><span class="s2">f(x) </span><span class="se">\t\t</span><span class="s2">||grad(x)|| </span><span class="se">\t</span><span class="s2">||p|| </span><span class="se">\t\t</span><span class="s2">min(eig(B)) </span><span class="se">\t</span><span class="s2">alpha </span><span class="se">\t\t</span><span class="s2">delta </span><span class="se">\t\t</span><span class="s2">step&quot;</span><span class="p">)</span>
    
    <span class="k">while</span> <span class="n">flag</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="c1"># Evaluate f(x) at current iteration</span>
        <span class="n">f</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        
        <span class="c1"># Evaluate gradient</span>
        <span class="n">grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_grad</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">check_nan</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">])):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: gradiant calculation returned NaN&quot;</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;k = &quot;</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;grad = &quot;</span><span class="p">,</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f = &quot;</span><span class="p">,</span><span class="n">f</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

        <span class="c1"># Calculate exact Hessian or update approximation</span>
        <span class="k">if</span><span class="p">(</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;Newton&quot;</span><span class="p">):</span>
            <span class="n">B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_hes</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        
        <span class="k">elif</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;Steepest-descent&quot;</span><span class="p">:</span>
            <span class="c1"># Initialize or set to identity</span>
            <span class="n">B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)))</span>
        
        <span class="k">elif</span> <span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;SR1&quot;</span> <span class="ow">or</span> <span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;BFGS&quot;</span><span class="p">:</span>
            <span class="c1"># Change in x</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># Change in gradient</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;s = &quot;</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y = &quot;</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;SR1&quot;</span><span class="p">:</span> <span class="c1"># Calculate SR1 update</span>
                <span class="n">dB</span> <span class="o">=</span> <span class="n">sr1_update</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
                <span class="n">B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">dB</span><span class="p">)</span>
                
            <span class="k">else</span><span class="p">:</span> <span class="c1"># Calculate BFGS update  </span>
                <span class="n">dB</span> <span class="o">=</span> <span class="n">bfgs_update</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
                <span class="n">B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">dB</span><span class="p">)</span> 

        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING. algorithm = &quot;</span><span class="p">,</span><span class="n">algorithm</span><span class="p">,</span><span class="s2">&quot; is not supported.&quot;</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B[k].shape = &quot;</span><span class="p">,</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            
        <span class="k">if</span><span class="p">(</span><span class="n">check_nan</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: Hessian update returned NaN&quot;</span><span class="p">)</span>
            <span class="k">break</span>
            
        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="mf">1E12</span><span class="p">:</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: Hessian approximation is near singular.&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B[k] = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        
        <span class="k">else</span><span class="p">:</span>
            
            <span class="c1"># Solve linear system to calculate step</span>
            <span class="n">pk</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">],</span><span class="o">-</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            
            <span class="k">if</span> <span class="n">globalization</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;Steepest-descent&quot;</span><span class="p">:</span>
                    <span class="c1"># Save step and scale by alpha_max</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pk</span><span class="o">*</span><span class="n">alpha_max</span><span class="p">)</span>
                    
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Take full step</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pk</span><span class="p">)</span>
                    
                <span class="c1"># Apply step and calculate x[k+1]</span>
                <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                    
            <span class="k">elif</span> <span class="n">globalization</span> <span class="o">==</span> <span class="s2">&quot;Line-search&quot;</span><span class="p">:</span>
                
                <span class="c1"># Line Search Function</span>
                <span class="n">update</span><span class="p">,</span> <span class="n">alphak</span> <span class="o">=</span> <span class="n">line_search</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">calc_f</span><span class="p">,</span> <span class="n">pk</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha_max</span><span class="p">,</span> <span class="n">eta_ls</span><span class="p">,</span> <span class="n">rho_ls</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
                
                <span class="c1"># Now the line search is complete, apply final value of alphak</span>
                <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
                
                <span class="c1"># Save alpha</span>
                <span class="n">alpha</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alphak</span><span class="p">)</span>
                
                <span class="c1"># Apply step and calculate x[k+1]</span>
                <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            
            <span class="k">elif</span> <span class="n">globalization</span> <span class="o">==</span> <span class="s2">&quot;Trust-region&quot;</span><span class="p">:</span>
                
                <span class="c1"># Trust region function</span>
                <span class="n">update</span> <span class="o">=</span> <span class="n">trust_region</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">pk</span><span class="p">,</span> <span class="n">delta_0_tr</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
                
                <span class="c1">### Trust region management</span>

                <span class="c1"># Actual reduction</span>
                <span class="n">ared</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">calc_f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

                <span class="c1"># Predicted reduction</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])))</span>

                <span class="c1"># Calculate rho</span>
                <span class="k">if</span> <span class="n">ared</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">pred</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># This occurs is the gradient is zero and Hessian is P.D.</span>
                    <span class="n">rho</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">1E4</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">rho</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ared</span><span class="o">/</span><span class="n">pred</span><span class="p">)</span>

                <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f[k] = &quot;</span><span class="p">,</span><span class="n">f</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p[k] = &quot;</span><span class="p">,</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f(x[k] + p[k]) = &quot;</span><span class="p">,</span><span class="n">calc_f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ared = &quot;</span><span class="p">,</span><span class="n">ared</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;pred = &quot;</span><span class="p">,</span><span class="n">pred</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rho = &quot;</span><span class="p">,</span><span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

                <span class="c1">## Check trust region shrink/expand logic</span>
                <span class="k">if</span> <span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">kappa_1_tr</span><span class="p">:</span>
                    <span class="c1"># Shrink trust region</span>
                    <span class="n">delta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kappa_1_tr</span><span class="o">*</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

                <span class="k">elif</span> <span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">kappa_2_tr</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">-</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1E-6</span><span class="p">:</span>
                    <span class="c1"># Expand trust region</span>
                    <span class="n">delta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">delta_max_tr</span><span class="p">]))</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Keep trust region same size</span>
                    <span class="n">delta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

                <span class="c1"># Compute step</span>
                <span class="k">if</span> <span class="n">rho</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">gamma_tr</span><span class="p">:</span>
                    <span class="c1"># Take step</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                    <span class="n">step_accepted</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Skip step</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                    <span class="n">step_accepted</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: globalization strategy not supported&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p = &quot;</span><span class="p">,</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            
            <span class="c1"># Calculate norms</span>
            <span class="n">norm_p</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="n">norm_g</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">output</span><span class="p">:</span>
                <span class="c1"># Calculate eigenvalues of Hessian (for display only)</span>
                <span class="n">min_ev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])))</span>

<span class="w">                </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">                print(&quot;f[k] = &quot;,f[k])</span>
<span class="sd">                print(&quot;norm_g = &quot;,norm_g)</span>
<span class="sd">                print(&quot;norm_g = &quot;,norm_p)</span>
<span class="sd">                print(&quot;min_ev = &quot;,min_ev)</span>
<span class="sd">                &#39;&#39;&#39;</span>
            
                <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="s1">&#39;  </span><span class="se">\t</span><span class="si">{0: 1.4e}</span><span class="s1"> </span><span class="se">\t</span><span class="si">{1:1.4e}</span><span class="s1"> </span><span class="se">\t</span><span class="si">{2:1.4e}</span><span class="s1"> </span><span class="se">\t</span><span class="si">{3: 1.4e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f</span><span class="p">[</span><span class="n">k</span><span class="p">],</span><span class="n">norm_g</span><span class="p">,</span><span class="n">norm_p</span><span class="p">,</span><span class="n">min_ev</span><span class="p">),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

                <span class="c1"># Python tip. The expression &#39;if my_list&#39; is false if &#39;my_list&#39; is empty</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">alpha</span><span class="p">:</span>
                    <span class="c1"># alpha is an empty list</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="se">\t</span><span class="s1"> -------&#39;</span><span class="p">,</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># otherwise print value of alpha</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="se">\t</span><span class="si">{0: 1.2e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            
                <span class="k">if</span> <span class="ow">not</span> <span class="n">delta</span><span class="p">:</span>
                    <span class="c1"># delta is an empty list</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="se">\t</span><span class="s1"> -------&#39;</span><span class="p">,</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># otherwise print value of alpha</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="se">\t</span><span class="si">{0: 1.2e}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="ow">not</span> <span class="n">step_accepted</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="se">\t</span><span class="s1"> -----&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">step_accepted</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="se">\t</span><span class="s1"> accept&#39;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="se">\t</span><span class="s1"> reject&#39;</span><span class="p">)</span>
            
            <span class="c1"># Check convergence criteria.</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">norm_p</span> <span class="o">&gt;</span> <span class="n">eps_dx</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">norm_g</span> <span class="o">&gt;</span> <span class="n">eps_df</span><span class="p">)</span>

            <span class="c1"># Update iteration counter</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>
    
    <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="n">max_iter</span> <span class="ow">and</span> <span class="n">flag</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reached maximum number of iterations.&quot;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">output</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x* = &quot;</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">p</span><span class="p">,</span><span class="n">B</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sr1_update</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Function that implements the sr1 optimization algorithm</span>
<span class="sd">    </span>
<span class="sd">    Inputs:</span>
<span class="sd">    s : Change in x</span>
<span class="sd">    y : Change in gradient</span>
<span class="sd">    k : Iteration number</span>
<span class="sd">    B : Hessian approximation</span>
<span class="sd">    verbose : toggles verbose output (True or False)</span>
<span class="sd">    </span>
<span class="sd">    Outputs:</span>
<span class="sd">    dB : Update to Hessian approximation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># SR1 formulation</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># Formula: dB = u * u.T / (u.T * s) if u is a column vector.</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">denom</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1E-10</span><span class="p">:</span>
        <span class="c1"># Skip update</span>
        <span class="n">dB</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Calculate update</span>
        <span class="n">dB</span> <span class="o">=</span> <span class="n">xxT</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">/</span><span class="n">denom</span>

    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SR1 update denominator, (y-B[k-1]*s).T*s = &quot;</span><span class="p">,</span><span class="n">denom</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SR1 update u = &quot;</span><span class="p">,</span><span class="n">u</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SR1 update u.T*u/(u.T*s) = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dB</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">dB</span> <span class="c1">#return the update to the Hessian approximation</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">bfgs_update</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Function that implements the BFGS optimization algorithm</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        s : Change in x</span>
<span class="sd">        y : Change in gradient</span>
<span class="sd">        k : Iteration number</span>
<span class="sd">        B : Hessian approximation</span>
<span class="sd">        verbose : toggles verbose output (True or False)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        dB : Update to Hessian approximation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Define constant used to check norm of the update</span>
    <span class="c1"># See Eq. (3.19) in Biegler (2010)</span>
    <span class="n">C2</span> <span class="o">=</span> <span class="mf">1E-4</span>
    <span class="c1"># sTy = s.dot(y)</span>
    
    <span class="c1"># # Add your solution here</span>
    <span class="c1"># if sTy &lt; C2*linalg.norm(s)**2:</span>
    <span class="c1">#     # Skip update</span>
    <span class="c1">#     dB = 0</span>
    <span class="c1"># elif s.dot(B[k-1]).dot(s) &lt; 1E-8:</span>
    <span class="c1">#       dB = 0</span>
    <span class="c1"># else:</span>
    <span class="c1">#     # Calculate update</span>
    <span class="c1">#     dB = xxT(y)/sTy - (B[k-1]).dot(xxT(s)).dot(B[k-1])/(s.dot(B[k-1]).dot(s))</span>

    <span class="c1"># Calculate intermediate</span>
    <span class="n">sy</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Calculate Term 2 denominator</span>
    <span class="c1"># s.T * Bk * s</span>
    <span class="n">d2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
    
    <span class="c1"># Condition check</span>
    <span class="n">C2norm</span> <span class="o">=</span> <span class="n">C2</span><span class="o">*</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">if</span> <span class="n">sy</span> <span class="o">&lt;=</span> <span class="n">C2norm</span> <span class="ow">or</span> <span class="n">d2</span> <span class="o">&lt;=</span> <span class="mf">1E-8</span><span class="p">:</span>
        <span class="n">skip</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">skip</span> <span class="o">=</span> <span class="kc">False</span>
    
    <span class="c1"># Add your solution here</span>
    <span class="k">if</span> <span class="n">skip</span><span class="p">:</span>
        <span class="n">dB</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dB</span> <span class="o">=</span> <span class="n">xxT</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">sy</span> <span class="o">-</span> <span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xxT</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">d2</span>
    
    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BFGS update denominator 1, (s.T * y) = &quot;</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BFGS update denominator 2, (s.T * B[k-1] * s) = &quot;</span><span class="p">,</span> <span class="n">d2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BFGS update = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">dB</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">dB</span> <span class="c1">#return the update to the Hessian approximation</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">line_search</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">calc_f</span><span class="p">,</span> <span class="n">pk</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha_max</span><span class="p">,</span> <span class="n">eta_ls</span><span class="p">,</span> <span class="n">rho_ls</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that implements the line search globalization strategy</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        x : decision variables</span>
<span class="sd">        f : objective values</span>
<span class="sd">        grad : gradients</span>
<span class="sd">        calc_f : function f(x) to minimize [scalar]</span>
<span class="sd">        pk : step</span>
<span class="sd">        k : Iteration number</span>
<span class="sd">        alpha_max : initial step length scaling for line search and/or steepest-descent</span>
<span class="sd">        eta_ls : parameter for Goldstein-Armijo conditions (line search only)</span>
<span class="sd">        rho_ls : parameter to shrink (backstep) line search</span>
<span class="sd">        verbose : toggles verbose output (True or False)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        update : update to p</span>
<span class="sd">        alphak : update to alpha</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Flag - continue line search?</span>
    <span class="n">ls</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># ls_arm = True</span>
    <span class="c1"># ls_gold = True</span>

    <span class="c1">## Initialize alpha</span>
    <span class="n">alphak</span> <span class="o">=</span> <span class="n">alpha_max</span>

    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2"> LINE SEARCH&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">ls</span><span class="p">:</span>

        <span class="c1"># Calculate test point (if step accepted)</span>
        <span class="n">xtest</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">alphak</span><span class="o">*</span><span class="n">pk</span>

        <span class="c1"># Evaluate f(x) at test point. This is used for Armijo and Goldstein conditions</span>
        <span class="n">ftest</span> <span class="o">=</span> <span class="n">calc_f</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>
        
        <span class="c1"># Add your solution here</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">alphak</span>
        <span class="c1"># Armijo condition as upper bound</span>
        <span class="n">armijo</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">eta_ls</span><span class="o">*</span><span class="n">alphak</span><span class="o">*</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pk</span><span class="p">)</span>
        <span class="c1"># Goldstein condition as lower bound </span>
        <span class="n">goldstein</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">eta_ls</span><span class="p">)</span><span class="o">*</span><span class="n">alphak</span><span class="o">*</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pk</span><span class="p">)</span>

        <span class="n">ls_arm</span> <span class="o">=</span> <span class="n">ftest</span> <span class="o">&lt;=</span> <span class="n">armijo</span>
        
        <span class="k">if</span> <span class="n">alphak</span> <span class="o">&lt;</span> <span class="n">alpha_max</span><span class="p">:</span>
            <span class="n">ls_gold</span> <span class="o">=</span> <span class="n">ftest</span> <span class="o">&gt;=</span> <span class="n">goldstein</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ls_gold</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">ls_arm</span> <span class="ow">and</span> <span class="n">ls_gold</span><span class="p">:</span>
            <span class="c1"># satisfied, stop shrink</span>
            <span class="n">ls</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">ls_gold</span><span class="p">:</span>
            <span class="c1"># failure, alphak is too short</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Line search failed. Alpha = </span><span class="si">{}</span><span class="s1"> is too short&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alphak</span><span class="p">))</span>
            <span class="k">break</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">ls_arm</span><span class="p">:</span>
            <span class="c1"># shrink alpha</span>
            <span class="n">alphak</span> <span class="o">=</span> <span class="n">alphak</span><span class="o">*</span><span class="n">rho_ls</span>

    <span class="c1"># Now the line search is complete, apply final value of alphak</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">alphak</span><span class="o">*</span><span class="n">pk</span>

    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Line search, alpha_max = &quot;</span><span class="p">,</span> <span class="n">alpha_max</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Line search, alphak = &quot;</span><span class="p">,</span> <span class="n">alphak</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Line search, Armijo condition = &quot;</span><span class="p">,</span> <span class="n">ls_arm</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Line search, Goldstein condition =  </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ls_gold</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">update</span><span class="p">,</span> <span class="n">alphak</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">trust_region</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">pk</span><span class="p">,</span> <span class="n">delta_0_tr</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Function that implements the trust region globalization strategy</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        x : decision variables</span>
<span class="sd">        grad : gradients</span>
<span class="sd">        B : Hessian approximation</span>
<span class="sd">        delta : trust region size</span>
<span class="sd">        k : Iteration number</span>
<span class="sd">        pk : step</span>
<span class="sd">        delta_0_tr : initial trust region size</span>
<span class="sd">        verbose : toggles verbose output (True or False)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        update : update to p</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1">### Initialize trust region radius</span>
    <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">delta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta_0_tr</span><span class="p">)</span>

    <span class="n">grad_zero</span> <span class="o">=</span> <span class="p">(</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1E-14</span><span class="p">)</span>

    <span class="c1">### Powell dogleg step</span>

    <span class="c1"># Calculate Cauchy step (pC)</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
    <span class="c1"># denom = grad[k].T @ B[k] @ grad[k]</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TR: Cauchy step. grad.T*B*grad = &quot;</span><span class="p">,</span><span class="n">denom</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">denom</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Term in ( ) is a scalar</span>
        <span class="n">pC</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">])</span><span class="o">/</span><span class="n">denom</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="c1"># pC = -(xxT(grad[k])/denom)*grad[k]</span>
    <span class="k">elif</span> <span class="n">grad_zero</span><span class="p">:</span>
        <span class="n">pC</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pC</span> <span class="o">=</span> <span class="o">-</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">/</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">])</span><span class="o">*</span><span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

    <span class="c1"># Use Newton step (calculate above)</span>
    <span class="n">pN</span> <span class="o">=</span> <span class="n">pk</span>

    <span class="c1"># Determine step</span>
    <span class="k">if</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pN</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span>
        <span class="c1"># Take Newton step. pN is inside the trust region.</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">pN</span>
        
    <span class="c1"># Add your solution here</span>
    <span class="k">elif</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pC</span><span class="p">):</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">pC</span> <span class="o">/</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pC</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">pN</span> <span class="o">-</span> <span class="n">pC</span>
        <span class="n">diff_norm2</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">pN</span> <span class="o">-</span> <span class="n">pC</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">diffTpC</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pC</span><span class="p">)</span>
        <span class="c1"># diffTpC = diff.T @ pC</span>
        <span class="c1"># compute eta</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">diffTpC</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">diffTpC</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pC</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">diff_norm2</span><span class="p">))</span> <span class="o">/</span> <span class="n">diff_norm2</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">eta</span><span class="o">*</span><span class="n">pN</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">eta</span><span class="p">)</span><span class="o">*</span><span class="n">pC</span>
    
    <span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trust region, delta[k] = &quot;</span><span class="p">,</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trust region, pN =  &quot;</span><span class="p">,</span> <span class="n">pN</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trust region, pC =  &quot;</span><span class="p">,</span> <span class="n">pC</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trust region, eta =  &quot;</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trust region, update =  </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">update</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">update</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="rosenbrock-function">
<h3>4.2. Rosenbrock function<a class="headerlink" href="#rosenbrock-function" title="Link to this heading">#</a></h3>
<p>In mathematical optimization, the <a class="reference external" href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock function</a> is a non-convex function, introduced by <em>Howard H. Rosenbrock</em> in 1960, which is used as a performance test problem for optimization algorithms. It is also known as Rosenbrockâs valley or Rosenbrockâs banana function.</p>
<p>The global minimum is inside a long, narrow, parabolic shaped flat valley. To find the valley is trivial. To converge to the global minimum, however, is difficult.</p>
<p><strong>The function is defined by:</strong></p>
<p><span class="math notranslate nohighlight">\(f(x, y) = (a-x)^2 + b(y-x^2)^2\)</span> and has a global minimum at <span class="math notranslate nohighlight">\((x, y) = (a, a^2)\)</span>. Usually we choose parameters <span class="math notranslate nohighlight">\((a, b) = (1, 100)\)</span> giving:</p>
<p><span class="math notranslate nohighlight">\(f(x, y) = (1-x)^2 + 100(y-x^2)^2\)</span> with minimum at <span class="math notranslate nohighlight">\((1, 1)\)</span></p>
<p><strong>Gradient:</strong></p>
<p><span class="math notranslate nohighlight">\(\nabla f = \left[-400xy + 400x^3 + 2x -2, 200y - 200x^2 \right]\)</span></p>
<p><strong>Hessian:</strong></p>
<p><span class="math notranslate nohighlight">\(H = \left[\begin{array}{ccc}
-400y+1200x^2+2 &amp; -400x\\
-400x &amp; 200
\end{array}\right]\)</span></p>
<!-- https://xavierbourretsicotte.github.io/Intro_optimization.html#Comparing-Newton-and-Gradient-Descent-in-presence-of-a-single-saddle-point --><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">Rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function of Rosenbrock</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">Grad_Rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function of Rosenbrock gradient</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">g1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span><span class="mi">2</span>
    <span class="n">g2</span> <span class="o">=</span> <span class="mi">200</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span><span class="mi">200</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">g1</span><span class="p">,</span><span class="n">g2</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">Hessian_Rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function of Rosenbrock Hessian matrix</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">h11</span> <span class="o">=</span> <span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1200</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="n">h12</span> <span class="o">=</span> <span class="o">-</span><span class="mi">400</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">h21</span> <span class="o">=</span> <span class="o">-</span><span class="mi">400</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">h22</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">h11</span><span class="p">,</span><span class="n">h12</span><span class="p">],[</span><span class="n">h21</span><span class="p">,</span><span class="n">h22</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">Rosenbrock_comparison</span><span class="p">(</span><span class="n">traj_1</span><span class="p">,</span> <span class="n">traj_2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Visualization to compare the performance of GD and Newton method on Rosenbrock function</span>

<span class="sd">    Arguments:</span>
<span class="sd">        traj_1: trajectory of gradient descent</span>
<span class="sd">        traj_2: trajectory of Newton method</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 3D plot and A 2D contour plot</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Print out the steps of convergence</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, gradient descent found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_1</span><span class="p">),</span> <span class="n">traj_1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, Newton method found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_2</span><span class="p">),</span> <span class="n">traj_2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># Set the range and function values over space</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">250</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">350</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Rosenbrock</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>

    <span class="c1"># Split data for x and y</span>
    <span class="n">iter_x_gd</span><span class="p">,</span> <span class="n">iter_y_gd</span> <span class="o">=</span> <span class="n">traj_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">traj_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">traj_2</span><span class="p">)</span>
    <span class="n">iter_x_nr</span><span class="p">,</span> <span class="n">iter_y_nr</span> <span class="o">=</span> <span class="n">traj_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="c1">#Angles needed for quiver plot</span>
    <span class="n">anglesx</span> <span class="o">=</span> <span class="n">iter_x_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_x_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesy</span> <span class="o">=</span> <span class="n">iter_y_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_y_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesx_nr</span> <span class="o">=</span> <span class="n">iter_x_nr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_x_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesy_nr</span> <span class="o">=</span> <span class="n">iter_y_nr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_y_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1">## Plot </span>
    <span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

    <span class="c1">#Surface plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">Z</span><span class="p">,</span><span class="n">rstride</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">cstride</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;jet&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span> <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">,</span> <span class="n">Rosenbrock</span><span class="p">((</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">)),</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">,</span> <span class="n">Rosenbrock</span><span class="p">((</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">)),</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Newton method&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1">#Rotate the initialization to help viewing the graph</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="mi">280</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

    <span class="c1">#Contour plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">Z</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;jet&#39;</span><span class="p">)</span>

    <span class="c1">#Plotting the iterations and intermediate values</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">iter_y_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">anglesx</span><span class="p">,</span> <span class="n">anglesy</span><span class="p">,</span> <span class="n">scale_units</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span>  <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Newton method&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">iter_y_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">anglesx_nr</span><span class="p">,</span> <span class="n">anglesy_nr</span><span class="p">,</span> <span class="n">scale_units</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Comparing Gradient descent and Newton method&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run gradient descent</span>
<span class="n">traj_gd</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="n">Grad_Rosenbrock</span><span class="p">,</span> 
                 <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.00125</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Run Newton&#39;s method</span>
<span class="n">traj_nt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">=</span> <span class="n">unconstrained_newton</span><span class="p">(</span><span class="n">Rosenbrock</span><span class="p">,</span><span class="n">Grad_Rosenbrock</span><span class="p">,</span><span class="n">Hessian_Rosenbrock</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]),</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;Newton&quot;</span><span class="p">,</span><span class="n">globalization</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
<span class="c1"># Plot </span>
<span class="n">Rosenbrock_comparison</span><span class="p">(</span><span class="n">traj_gd</span><span class="p">,</span> <span class="n">traj_nt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After 11222 steps, gradient descent found minimizer x* = [0.99900133 0.99799966]
After 7 steps, Newton method found minimizer x* = [1. 1.]
</pre></div>
</div>
<img alt="../../_images/56e7f389b3ba83de9eb89474ac9b00130a8fe2665cff46d59d7b7cfcea6dafd5.png" src="../../_images/56e7f389b3ba83de9eb89474ac9b00130a8fe2665cff46d59d7b7cfcea6dafd5.png" />
</div>
</div>
<p><strong>Discussion:</strong></p>
<p>As expected, the GD algorithm finds the âvalleyâ - however it struggles to reach the global minimum and gets stuck in a zigzag behavior. The minimum is found after thousand of steps. Trying different initial values leads to vastly different results, many of which are not close to the global minimum, and in many cases leading to infinite values (i.e. algorithm does not converge)</p>
<p>The Newton-Raphson method converges extremely rapidly to the global solution, despite the significant challenge posed by the Rosenbrock function. Irrespective of the starting point (within the scope of the graph above) the minimum is found within 4 - 6 iterations. Compare this to the several thousand iterations required by the gradient descent approach.</p>
<p><strong>Question:</strong> What will happen if we change the learning rate (<span class="math notranslate nohighlight">\(\alpha\)</span>) of the gradient descent method? Try to plot it out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Add your solution</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="single-saddle-point">
<h3>4.3. Single saddle point<a class="headerlink" href="#single-saddle-point" title="Link to this heading">#</a></h3>
<p>Saddle points are much more common than local minima when it comes to optimization problems in a high number of dimensions (which is always the case when it comes to building deep learning models). Consider the following problem:</p>
<div class="math notranslate nohighlight">
\[f(x, y) = 0.01x^2 - 0.1y^2\]</div>
<div class="math notranslate nohighlight">
\[f'(x, y) = 0.02x - 0.2y\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">saddle_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function of single saddle point case</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="mf">.01</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">.1</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">saddle_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Gradient function of single saddle point case</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">g1</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="mf">.01</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">g2</span> <span class="o">=</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="mf">.1</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">g1</span><span class="p">,</span><span class="n">g2</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">saddle_hes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Hessian matrix of single saddle point case</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">.02</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mf">.2</span><span class="p">]])</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">saddle_comparison</span><span class="p">(</span><span class="n">traj_1</span><span class="p">,</span> <span class="n">traj_2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Visualization to compare the performance of GD and Newton method on space with a single saddle point</span>

<span class="sd">    Arguments:</span>
<span class="sd">        traj_1: trajectory of gradient descent</span>
<span class="sd">        traj_2: trajectory of Newton method</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 3D plot and A 2D contour plot</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Print out the steps of convergence</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, gradient descent found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_1</span><span class="p">),</span> <span class="n">traj_1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, Newton method found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_2</span><span class="p">),</span> <span class="n">traj_2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># Set the range and function values over space</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">saddle_func</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>

    <span class="c1"># Split data for x and y</span>
    <span class="n">iter_x_gd</span><span class="p">,</span> <span class="n">iter_y_gd</span> <span class="o">=</span> <span class="n">traj_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">traj_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">traj_2</span><span class="p">)</span>
    <span class="n">iter_x_nr</span><span class="p">,</span> <span class="n">iter_y_nr</span> <span class="o">=</span> <span class="n">traj_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Angles needed for quiver plot</span>
    <span class="n">anglesx</span> <span class="o">=</span> <span class="n">iter_x_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_x_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesy</span> <span class="o">=</span> <span class="n">iter_y_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_y_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesx_nr</span> <span class="o">=</span> <span class="n">iter_x_nr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_x_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesy_nr</span> <span class="o">=</span> <span class="n">iter_y_nr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_y_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

    <span class="c1">#Surface plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">Z</span><span class="p">,</span><span class="n">rstride</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">cstride</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;jet&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span> <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">,</span> <span class="n">saddle_func</span><span class="p">((</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">)),</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">,</span> <span class="n">saddle_func</span><span class="p">((</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">)),</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Newton method&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1">#Rotate the initialization to help viewing the graph</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

    <span class="c1">#Contour plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">Z</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;jet&#39;</span><span class="p">)</span>

    <span class="c1">#Plotting the iterations and intermediate values</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">iter_y_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">anglesx</span><span class="p">,</span> <span class="n">anglesy</span><span class="p">,</span> <span class="n">scale_units</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span>  <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Newton method&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">iter_y_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">anglesx_nr</span><span class="p">,</span> <span class="n">anglesy_nr</span><span class="p">,</span> <span class="n">scale_units</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Comparing Gradient descent and Newton method&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Comparison with starting point = [-2.0, -0.01]</span>
<span class="c1"># Run gradient descent</span>
<span class="n">traj_gd</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="n">saddle_grad</span><span class="p">,</span>
                 <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">45</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Run Newton&#39;s method</span>
<span class="n">traj_nt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">=</span> <span class="n">unconstrained_newton</span><span class="p">(</span><span class="n">saddle_func</span><span class="p">,</span> <span class="n">saddle_grad</span><span class="p">,</span><span class="n">saddle_hes</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">]),</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;Newton&quot;</span><span class="p">,</span><span class="n">globalization</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="c1"># Plot </span>
<span class="n">saddle_comparison</span><span class="p">(</span><span class="n">traj_gd</span><span class="p">,</span> <span class="n">traj_nt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After 46 steps, gradient descent found minimizer x* = [-1.27237097 -0.72890484]
After 3 steps, Newton method found minimizer x* = [0. 0.]
</pre></div>
</div>
<img alt="../../_images/a6a57ab69422ce349c00a142ae2489972f842a31489bddf4c1d8cc1d7826f1c7.png" src="../../_images/a6a57ab69422ce349c00a142ae2489972f842a31489bddf4c1d8cc1d7826f1c7.png" />
</div>
</div>
<p>In single saddle points case, it seems gradient descent is more inclined to escape from saddle points compared to Newtonâs method. But what if facing multiple saddle points?</p>
</section>
<section id="multiple-saddle-points">
<h3>4.4. Multiple saddle points<a class="headerlink" href="#multiple-saddle-points" title="Link to this heading">#</a></h3>
<p>Consider <a class="reference external" href="https://en.wikipedia.org/wiki/Himmelblau%27s_function">Himmelblauâs function</a>, which is a multi-modal function, used to test the performance of optimization algorithms. The function is defined by:</p>
<div class="math notranslate nohighlight">
\[f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\]</div>
<p>It has one local maximum at <span class="math notranslate nohighlight">\(x = -0.270845\)</span> and <span class="math notranslate nohighlight">\(y = -0.923039\)</span> where <span class="math notranslate nohighlight">\(f(x,y) = 181.617\)</span>.</p>
<p>And it has four different local minima:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(3.0, 2.0) = 0.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(-2.805118, 3.131312) = 0.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(-3.779310, -3.283186) = 0.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(3.584428, -1.848126) = 0.0\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">Himmer</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Himmelblau&#39;s function</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">11</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">7</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">Grad_Himmer</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Himmelblau&#39;s gradient function</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">7</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">11</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">11</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">7</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">))])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">Hessian_Himmer</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Himmelblau&#39;s Hessian matrix</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">h11</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">11</span><span class="p">)</span> <span class="o">+</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="n">h12</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">h21</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">h22</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">7</span><span class="p">)</span> <span class="o">+</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">h11</span><span class="p">,</span><span class="n">h12</span><span class="p">],[</span><span class="n">h21</span><span class="p">,</span><span class="n">h22</span><span class="p">]])</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Comparison with starting point = [0.5, -2.0]</span>
<span class="c1"># Run gradient descent</span>
<span class="n">traj_gd</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="n">Grad_Himmer</span><span class="p">,</span> 
                 <span class="n">start</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-02</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Run Newton&#39;s method</span>
<span class="n">traj_nt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">=</span> <span class="n">unconstrained_newton</span><span class="p">(</span><span class="n">Himmer</span><span class="p">,</span> <span class="n">Grad_Himmer</span><span class="p">,</span> <span class="n">Hessian_Himmer</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">]),</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;Newton&quot;</span><span class="p">,</span><span class="n">globalization</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, gradient descent found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_gd</span><span class="p">),</span> <span class="n">traj_gd</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, Newton method found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_nt</span><span class="p">),</span> <span class="n">traj_nt</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After 63 steps, gradient descent found minimizer x* = [ 3.48561945 -1.77701988]
After 5 steps, Newton method found minimizer x* = [-0.12796135 -1.95371498]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">comparison_plot</span><span class="p">(</span><span class="n">traj_1</span><span class="p">,</span> <span class="n">traj_2</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Visualization to compare the performance of GD and Newton method on Himmelblau&#39;s function</span>

<span class="sd">    Arguments:</span>
<span class="sd">        traj_1: trajectory of gradient descent</span>
<span class="sd">        traj_2: trajectory of Newton method</span>
<span class="sd">        name: the name of Newton-based method for printout</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        A 3D plot and A 2D contour plot</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Prepare the range and function values over space</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Himmer</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>

    <span class="c1"># Local minima</span>
    <span class="n">local_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.805118</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.779310</span><span class="p">,</span> <span class="mf">3.584428</span><span class="p">])</span>
    <span class="n">local_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.131312</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.283186</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.848126</span><span class="p">])</span>

    <span class="c1"># Split data for x and y</span>
    <span class="n">iter_x_gd</span><span class="p">,</span> <span class="n">iter_y_gd</span> <span class="o">=</span> <span class="n">traj_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">traj_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">traj_2</span><span class="p">)</span>
    <span class="n">iter_x_nr</span><span class="p">,</span> <span class="n">iter_y_nr</span> <span class="o">=</span> <span class="n">traj_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Check the width of trajectories </span>
    <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">10</span> <span class="ow">or</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">iter_y_nr</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Out of printable area!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># Angles needed for quiver plot</span>
    <span class="n">anglesx</span> <span class="o">=</span> <span class="n">iter_x_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_x_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesy</span> <span class="o">=</span> <span class="n">iter_y_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_y_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesx_nr</span> <span class="o">=</span> <span class="n">iter_x_nr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_x_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesy_nr</span> <span class="o">=</span> <span class="n">iter_y_nr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_y_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

    <span class="c1">#Surface plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">Z</span><span class="p">,</span><span class="n">rstride</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">cstride</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;jet&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span> <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">,</span> <span class="n">Himmer</span><span class="p">((</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">)),</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">,</span> <span class="n">Himmer</span><span class="p">((</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">)),</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.4</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">local_x</span><span class="p">,</span> <span class="n">local_y</span><span class="p">,</span> <span class="n">Himmer</span><span class="p">((</span><span class="n">local_x</span><span class="p">,</span> <span class="n">local_y</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Local minima&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1">#Rotate the initialization to help viewing the graph</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

    <span class="c1">#Contour plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">Z</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;jet&#39;</span><span class="p">)</span>

    <span class="c1">#Plotting the iterations and intermediate values</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">,</span><span class="n">iter_y_gd</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Gradient descent&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">iter_x_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">iter_y_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">anglesx</span><span class="p">,</span> <span class="n">anglesy</span><span class="p">,</span> <span class="n">scale_units</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">,</span><span class="n">iter_y_nr</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span>  <span class="n">label</span> <span class="o">=</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">iter_x_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">iter_y_nr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">anglesx_nr</span><span class="p">,</span> <span class="n">anglesy_nr</span><span class="p">,</span> <span class="n">scale_units</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">angles</span> <span class="o">=</span> <span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">local_x</span><span class="p">,</span> <span class="n">local_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Local minima&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Comparing Gradient descent and &#39;</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">Himmelblau_comparison</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function to compare the gradient descent and other Newton-based method on Himmelblau&#39;s optimization</span>

<span class="sd">    Arguments: None</span>

<span class="sd">    Returns: </span>
<span class="sd">        Optima, steps, and visualizations</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Comparison with starting point = [0.5, -2.0]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">])</span>
    
    <span class="c1"># Run gradient descent</span>
    <span class="n">traj_gd</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="n">Grad_Himmer</span><span class="p">,</span> 
                 <span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-02</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Set name of methods and globalizations</span>
    <span class="n">alg</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Newton&quot;</span><span class="p">,</span> <span class="s2">&quot;SR1&quot;</span><span class="p">,</span> <span class="s2">&quot;BFGS&quot;</span><span class="p">]</span>
    <span class="n">glb</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;Line-search&quot;</span><span class="p">,</span> <span class="s2">&quot;Trust-region&quot;</span><span class="p">]</span>

    <span class="c1"># Prepare the range and function values over space</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Himmer</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
    
    <span class="c1"># Angles needed for quiver plot</span>
    <span class="n">iter_x_gd</span><span class="p">,</span> <span class="n">iter_y_gd</span> <span class="o">=</span> <span class="n">traj_gd</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_gd</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesx</span> <span class="o">=</span> <span class="n">iter_x_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_x_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">anglesy</span> <span class="o">=</span> <span class="n">iter_y_gd</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">iter_y_gd</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Set case number</span>
    <span class="n">case_num</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Loop over methods</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alg</span><span class="p">)):</span>
        <span class="c1"># Loop over globalizations</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">glb</span><span class="p">)):</span>
            <span class="c1"># Set name for printout</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">alg</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39; with &#39;</span><span class="o">+</span><span class="n">glb</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">if</span> <span class="n">glb</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span> <span class="k">else</span> <span class="n">alg</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; without globalization&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Case </span><span class="si">{}</span><span class="s2">: compare GD to &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">case_num</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span>

            <span class="c1"># Run Newton-based method</span>
            <span class="n">traj_nt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">=</span> <span class="n">unconstrained_newton</span><span class="p">(</span><span class="n">Himmer</span><span class="p">,</span> <span class="n">Grad_Himmer</span><span class="p">,</span> <span class="n">Hessian_Himmer</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">algorithm</span><span class="o">=</span><span class="n">alg</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">globalization</span><span class="o">=</span><span class="n">glb</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            
            <span class="c1"># Print out the steps of convergence</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, GD found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_gd</span><span class="p">),</span> <span class="n">traj_gd</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, </span><span class="si">{}</span><span class="s2"> found minimizer x* = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_nt</span><span class="p">),</span> <span class="n">name</span><span class="p">,</span> <span class="n">traj_nt</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            
            <span class="c1"># Run visualziation</span>
            <span class="n">comparison_plot</span><span class="p">(</span><span class="n">traj_gd</span><span class="p">,</span> <span class="n">traj_nt</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
            
            <span class="c1"># Update case number</span>
            <span class="n">case_num</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="c1"># For the last case: Steepest-descent with Line-search (steepest-descent = gradient-descent)</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Steepest-descent with Line-search&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Case </span><span class="si">{}</span><span class="s2">: compare GD to &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">case_num</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># Run Steepest-descent with line search</span>
    <span class="n">traj_nt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="o">=</span> <span class="n">unconstrained_newton</span><span class="p">(</span><span class="n">Himmer</span><span class="p">,</span> <span class="n">Grad_Himmer</span><span class="p">,</span> <span class="n">Hessian_Himmer</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;Steepest-descent&quot;</span><span class="p">,</span><span class="n">globalization</span><span class="o">=</span><span class="s2">&quot;Line-search&quot;</span><span class="p">)</span>
    
    <span class="c1"># Print out the steps of convergence</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, GD found minimizer = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_gd</span><span class="p">),</span> <span class="n">traj_gd</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After </span><span class="si">{}</span><span class="s2"> steps, </span><span class="si">{}</span><span class="s2"> found minimizer = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traj_nt</span><span class="p">),</span> <span class="n">name</span><span class="p">,</span> <span class="n">traj_nt</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    
    <span class="c1"># Run visualziation</span>
    <span class="n">comparison_plot</span><span class="p">(</span><span class="n">traj_gd</span><span class="p">,</span> <span class="n">traj_nt</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Himmelblau_comparison</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 1: compare GD to Newton without globalization
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 5 steps, Newton without globalization found minimizer x* = [-0.12796135 -1.95371498]
</pre></div>
</div>
<img alt="../../_images/a454edade264fa65fc091f3312ae704f7bc45805ecaea6a6c355e2cb3197da7a.png" src="../../_images/a454edade264fa65fc091f3312ae704f7bc45805ecaea6a6c355e2cb3197da7a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 2: compare GD to Newton with Line-search
Line search failed. Alpha = 0.9 is too short
Line search failed. Alpha = 0.9 is too short
Line search failed. Alpha = 0.9 is too short
Line search failed. Alpha = 0.9 is too short
Line search failed. Alpha = 0.9 is too short
Line search failed. Alpha = 0.9 is too short
Line search failed. Alpha = 0.9 is too short
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 8 steps, Newton with Line-search found minimizer x* = [-0.12796131 -1.95371497]
</pre></div>
</div>
<img alt="../../_images/97b4cb22a0a3d16ff2fee3b40f3b8b788a988fc42cb1de525e9dacf8a4483017.png" src="../../_images/97b4cb22a0a3d16ff2fee3b40f3b8b788a988fc42cb1de525e9dacf8a4483017.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 3: compare GD to Newton with Trust-region
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 5 steps, Newton with Trust-region found minimizer x* = [-0.12796135 -1.95371498]
</pre></div>
</div>
<img alt="../../_images/ea82895eabe7b3630f2fef8abff3568db174cd26f76c38704603799f2952f45c.png" src="../../_images/ea82895eabe7b3630f2fef8abff3568db174cd26f76c38704603799f2952f45c.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 4: compare GD to SR1 without globalization
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 20 steps, SR1 without globalization found minimizer x* = [-0.27084459 -0.92303856]
Out of printable area!





Case 5: compare GD to SR1 with Line-search
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 13 steps, SR1 with Line-search found minimizer x* = [ 3.58442834 -1.84812653]
</pre></div>
</div>
<img alt="../../_images/42d92fde51a44e69c7cceaef02981c64f826eb1bcb0828f4893abddf34954cea.png" src="../../_images/42d92fde51a44e69c7cceaef02981c64f826eb1bcb0828f4893abddf34954cea.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 6: compare GD to SR1 with Trust-region
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 13 steps, SR1 with Trust-region found minimizer x* = [3. 2.]
</pre></div>
</div>
<img alt="../../_images/44500a96a000e90620823b9a4ce09aabf1dd71bad2a128fe13b16f159fcc759b.png" src="../../_images/44500a96a000e90620823b9a4ce09aabf1dd71bad2a128fe13b16f159fcc759b.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 7: compare GD to BFGS without globalization
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 48 steps, BFGS without globalization found minimizer x* = [3. 2.]
Out of printable area!





Case 8: compare GD to BFGS with Line-search
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 11 steps, BFGS with Line-search found minimizer x* = [ 3.58442834 -1.84812652]
</pre></div>
</div>
<img alt="../../_images/c3a5df823006c052fe323207de1c6b06910e0c30d05498e220049b2186c15059.png" src="../../_images/c3a5df823006c052fe323207de1c6b06910e0c30d05498e220049b2186c15059.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 9: compare GD to BFGS with Trust-region
After 11 steps, GD found minimizer x* = [ 3.58173398 -1.81859973]
After 20 steps, BFGS with Trust-region found minimizer x* = [ 3.58442834 -1.84812652]
</pre></div>
</div>
<img alt="../../_images/d1bd3b006308820ccc08fb2444dda5360e4d0d5a3ce16cb5d237672d7961158a.png" src="../../_images/d1bd3b006308820ccc08fb2444dda5360e4d0d5a3ce16cb5d237672d7961158a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Case 10: compare GD to Steepest-descent with Line-search
After 11 steps, GD found minimizer = [ 3.58173398 -1.81859973]
After 29 steps, Steepest-descent with Line-search found minimizer = [ 3.58442806 -1.84812621]
</pre></div>
</div>
<img alt="../../_images/100c991bb1971855851d985b748e5c9ddd070df6b8ea3f9810a9b3bf54158e71.png" src="../../_images/100c991bb1971855851d985b748e5c9ddd070df6b8ea3f9810a9b3bf54158e71.png" />
</div>
</div>
<p><strong>Discussion:</strong></p>
<p>In first four cases, Newton-based methods (vanila Newton w/ and w/o globalization and SR1 w/o globalization) did not found any local minima. Rest of other cases found a minimum successfully. Gradient descent (<span class="math notranslate nohighlight">\(\alpha=0.01\)</span>) took 11 steps until convergent; BFGS took 48 steps instead. SR1 with globalization converged by 13 steps; BFGS with line search took 11 steps; BFGS with trust region took 20 steps.</p>
<p>If only considering the iteration steps, the cost of vanilla gradient descent is better than others. Moreover, Newton method attracts to saddle points and saddle points are common in machine learning, or in fact any multivariable optimization. That is the reason why we use a lot stochastic gradient descent in machine learning, rather than stochastic Newton-Raphson method.</p>
<p><strong>Paperâs point of view:</strong></p>
<p>Indeed the ratio of the number of saddle points to local minima increases exponentially with the dimensionality <span class="math notranslate nohighlight">\(N\)</span>. While gradient descent dynamics are repelled away from a saddle point to lower error by following directions of negative curvature, the Newton method does not treat saddle points appropriately; as argued below, saddle-points instead become attractive under the Newton dynamics. (We can check further details in <a class="reference external" href="https://arxiv.org/pdf/1406.2572.pdf">https://arxiv.org/pdf/1406.2572.pdf</a>)</p>
<p><strong>Question:</strong> Steepest-descent is exactly gradient descent, so why they took different iteration steps until convergence?</p>
<p><strong>Answer:</strong></p>
<p>Because steepest-descent chooses <span class="math notranslate nohighlight">\(\alpha = 1\)</span>, but gradient descent in this case chose <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>. If we run gradient descent with <span class="math notranslate nohighlight">\(\alpha = 1\)</span> again, it would not converge. So thanks to line search that improves the stability of  steepest-descent.</p>
<p><strong>Question:</strong> In â<em>Case 4: compare GD to SR1 without globalization</em>â and â<em>Case 7: compare GD to BFGS</em>â, the trajectories of BFGS method without globalization were out of printable area for the current codes, please fix the plot and interpret why this search area is so wide.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Add your solution</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3>4.5. Takeaway conclusion<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p><strong>Computational cost analysis:</strong></p>
<p>To compare the computational cost, we have discussed the time complexity of gradient descent is <span class="math notranslate nohighlight">\(\mathcal{O}(N)\)</span>.</p>
<p>Newtonâs Method takes a long time per iteration and is memory-intensive Usually Newtonâs Method requires computing the second derivative, <span class="math notranslate nohighlight">\(H\)</span>, which is <span class="math notranslate nohighlight">\(O(N^2)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of features. While computing the gradient, <span class="math notranslate nohighlight">\(g\)</span>, is only <span class="math notranslate nohighlight">\(O(N)\)</span>. But the next step is <span class="math notranslate nohighlight">\(H^{â1} g\)</span>, which is <span class="math notranslate nohighlight">\(O(N^3)\)</span> to compute. So while computing the Hessian is expensive, inverting it or solving least squares is often even worse. (If you have sparse features, the asymptotics look better, but other methods also perform better, so sparsity doesnât make Newton <strong>relatively</strong> more appealing.) If using LU decomposition, some papers argue that it is <span class="math notranslate nohighlight">\(O(N^2)\)</span>.</p>
<p>For globalization methods, to evaluate time complexity is much more complicated, but many papers believe it should be yield to <span class="math notranslate nohighlight">\(\mathcal{O}(N)\)</span> or <span class="math notranslate nohighlight">\(\mathcal{O}(N^2)\)</span>.</p>
<p>Hence, in the multiple saddle point case, we should square steps for all quasi-Newton no matter with or without globalization. In this rough comparison, we can clearly see the advantages of gradient descent (1) quite low computational cost and (2) not easily attracted by saddle points.</p>
<p><strong>How to improve Newtonâs method:</strong></p>
<p>We can approximate a Newton step at a lower computational cost per step but take more iterations to converge. Some examples:</p>
<ul class="simple">
<li><p>Because of the expense of inverting the Hessian, quasi-Newton methods like BFGS approximate the inverse Hessian, <span class="math notranslate nohighlight">\(H^{â1}\)</span>, by looking at how the gradient has changed over the last few steps.</p></li>
<li><p>BFGS is still very memory-intensive in high-dimensional settings because it requires storing the entire <span class="math notranslate nohighlight">\(O(N^2)\)</span> approximate inverse Hessian. Limited memory BFGS (L-BFGS) calculates the next step direction as the approximate inverse Hessian times the gradient, but it only requires storing the last several gradient updates; it doesnât explicitly store the approximate inverse Hessian.</p></li>
<li><p>When you donât want to deal with approximating second derivatives at all, gradient descent is appealing because it only uses only first-order information. Gradient descent is implicitly approximating the inverse Hessian as the learning rate times the identity matrix.</p></li>
<li><p>Sometimes you have a very large number of observations (data points), but you could learn almost as well from a smaller number of observations. When that is the case, you can use âbatch methodsâ, like stochastic gradient descent, that cycle through using subsets of the observations.</p></li>
</ul>
<!-- **More professional discussion:**

**Pros:** 

Indeed the ratio of the number of saddle points to local minima increases exponentially with the dimensionality $N$. While gradient descent dynamics are repelled away from a saddle point to lower error by following directions of negative curvature, the Newton method does not treat saddle points appropriately; as argued below, saddle-points instead become attractive under the Newton dynamics. (We can check further details in https://arxiv.org/pdf/1406.2572.pdf)


**Cons:**

Newton-based methods could be used more in machine learning by the following reasons. 

* According to the above paper, using line-search with the Wolfe conditions or using or trust regions prevents convergence to saddle points. A proper gradient descent implementation should be doing this too. 

* Using Newton's method does not require constructing the whole (dense) Hessian; you can apply the inverse of the Hessian to a vector with iterative methods that only use matrix-vector products (e.g., Krylov methods like conjugate gradient). See, for example, the CG-Steihaug trust region method.

* You can compute Hessian matrix-vector products efficiently by solving two higher order adjoint equations of the same form as the adjoint equation that is already used to compute the gradient (e.g., the work of two backpropagation steps in neural network training).

* Ill conditioning slows the convergence of iterative linear solvers, but it also slows gradient descent equally or worse. Using Newton's method instead of gradient descent shifts the difficulty from the nonlinear optimization stage (where not much can be done to improve the situation) to the linear algebra stage (where we can attack it with the entire arsenal of numerical linear algebra preconditioning techniques).

* Also, the computation shifts from "many many cheap steps" to "a few costly steps", opening up more opportunities for parallelism at the sub-step (linear algebra) level.


*To be metioned, Newton's method will not help you with L1 or other similar compressed sensing/sparsity promoting penalty functions, since they lack the required smoothness.* --></section>
</section>
<section id="implement-sgd-on-glm-with-real-dataset">
<h2>5. Implement SGD on GLM with real dataset<a class="headerlink" href="#implement-sgd-on-glm-with-real-dataset" title="Link to this heading">#</a></h2>
<p>In ordinary linear regression, we treat our outcome variable as a linear combination of several input variables plus some random noise, typically assumed to be Normally distributed. While this modeling approach is easily interpreted, efficiently implemented, and capable of accurately capturing many linear relationships, it does come with several significant limitations. A simple extension of linear models, a Generalized Linear Model (GLM) is able to relax some of most strict assumptions in linear regression. These assumptions include:</p>
<ul class="simple">
<li><p>Linearity between the outcome and input variables</p></li>
<li><p>Normal distribution of error terms</p></li>
<li><p>Constant variance of error terms.</p></li>
</ul>
<p>Relaxing these assumptions allows us to fit much more flexible models to much broader data types.</p>
<section id="glm-structure">
<h3>5.1. GLM Structure<a class="headerlink" href="#glm-structure" title="Link to this heading">#</a></h3>
<p>Fitting a GLM first requires specifying two components: a random distribution for our outcome variable and a link function between the distributionâs mean parameter and its âlinear predictorâ.</p>
<p><strong>The Random Component</strong></p>
<p>The first step to building our GLM is identifying the distribution of the outcome variable. If the data has a binary response, we might want to use the Bernoulli or Binomial distributions. If we are working with count data, a Poisson model might be more useful. This distribution is typically assumed to come from the Exponential Family of distributions, which includes the Binomial, Poisson, Negative Binomial, Gamma, and Normal.</p>
<p>Each of these models can be expressed in terms of its mean parameter, <span class="math notranslate nohighlight">\(\mu = E(Y)\)</span>. For instance, we specify a binomial model as <span class="math notranslate nohighlight">\(Y \sim \text{Bin}(n, \mu/n)\)</span>, which can also be written as <span class="math notranslate nohighlight">\(Y \sim \text{Bin}(n, \mu/n)\)</span>. Once we estimate <span class="math notranslate nohighlight">\(\mu\)</span>, we model <span class="math notranslate nohighlight">\(Y\)</span> as coming from a distribution indexed by <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and our predicted value of <span class="math notranslate nohighlight">\(Y\)</span> is simply <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>.</p>
<p><strong>The Link Function</strong></p>
<p>In a GLM, we estimate <span class="math notranslate nohighlight">\(\mu\)</span> as a non-linear function of a âlinear predictorâ <span class="math notranslate nohighlight">\(\eta\)</span>, which itself is a linear function of the data. The non-linear function connecting <span class="math notranslate nohighlight">\(\mu\)</span> to <span class="math notranslate nohighlight">\(\eta\)</span> is called the link function, and we determine it before model-fitting. The link function is written as a function of <span class="math notranslate nohighlight">\(\mu\)</span>, e.g., <span class="math notranslate nohighlight">\(\eta = g(\mu)\)</span>.</p>
<p>Suppose we have the following training data where each <span class="math notranslate nohighlight">\(x\)</span> is a D-dimensional vector:</p>
<div class="math notranslate nohighlight">
\[
\{\left(x_1, y_1\right), \left(x_2, y_2\right), \cdots, \left(x_N, y_N\right) \}
\]</div>
<p>We first write <span class="math notranslate nohighlight">\(\eta\)</span> as a linear function of <span class="math notranslate nohighlight">\(x\)</span> for each observation <span class="math notranslate nohighlight">\(n = 1, \cdots , N\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\eta_n = \theta_0 + \theta_1 x_{n,2} + \theta_1 x_{n,2} + \cdots + \theta_D x_{n,d}
\]</div>
<p>Then we connect <span class="math notranslate nohighlight">\(\eta\)</span> to <span class="math notranslate nohighlight">\(\mu\)</span> with the link function:</p>
<div class="math notranslate nohighlight">
\[
\eta_n = g(\mu_n)
\]</div>
<p>Finally, we get the loss function:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_n = \sum_{n=1}^N \mathcal{L} (\hat{y}_n, y_n) = \sum_{n=1}^N \mathcal{L} (\hat{\mu}_n, y_n) = \sum_{n=1}^N \mathcal{L} (g^{-1}(\eta_n), y_n)
\]</div>
<p><strong>Logistic Regression â A GLM for Binary Data</strong></p>
<p>In logistic regression, we model our outputs as independent Bernoulli trials, i.e., we assume:</p>
<div class="math notranslate nohighlight">
\[
Y_n \sim \text{Bern}(p_n)
\]</div>
<p>Then we need to find a way to relate our linear predictor <span class="math notranslate nohighlight">\(\eta\)</span> to our parameter <span class="math notranslate nohighlight">\(p\)</span>. Since <span class="math notranslate nohighlight">\(p\)</span> is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(\eta\)</span> can be any real number, a natural choice is the log-odds. I.e.,</p>
<div class="math notranslate nohighlight">
\[
\eta_n = g(p_n) = \text{log}\left(\frac{p_n}{1-p_n}\right)
\]</div>
<p>Inversely, we use the sigmoid function to get from <span class="math notranslate nohighlight">\(\eta\)</span> to <span class="math notranslate nohighlight">\(p\)</span> (which will be called as <span class="math notranslate nohighlight">\(S\)</span> then):</p>
<div class="math notranslate nohighlight">
\[
p_n = g^{-1}(\eta_n) = \frac{1}{1+\text{exp}(-\eta_n)} = S(\eta_n)
\]</div>
<p>Then, find the negative log likelihood:</p>
<div class="math notranslate nohighlight">
\[
L(p; X, y) = \prod_{n=1}^N p_n^{y_n} (1-p_n)^{1-y_n}
\]</div>
<div class="math notranslate nohighlight">
\[
-\text{log}(L(p; X, y)) = - \sum_{n=1}^N y_n \text{log}(p_n) + (1-y_n)\text{log}(1-p_n)
\]</div>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\text{log}(L(\theta ; X, y)) = -\sum_{n=1}^N y_n \text{log}(S(\beta^T x_n)) + (1-y_n)\text{log}(1-S(\beta^T x_n))
\]</div>
<p>Then we find the values of <span class="math notranslate nohighlight">\(\theta\)</span> to minimize this loss. Unfortunately, in the logistic regression case, there is no closed-form solution, so we must use gradient descent. So, letâs find the derivative of the loss function with respect to <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="math notranslate nohighlight">
\[
S'(x) = \frac{\text{exp}(-x)}{(1+\text{exp}(-x))^2} = S(x)(1-S(x))
\]</div>
<p>Then we can derive the entire gradient:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \theta} = - \sum_{n=1}^N y_n (1-S(\theta^T x_n))x_n - (1-y_n)S(\theta^T x_n)x_n
\]</div>
</section>
<section id="data-wrangling">
<h3>5.2. Data Wrangling<a class="headerlink" href="#data-wrangling" title="Link to this heading">#</a></h3>
<p>While the aim of this section is to build a logistic regression model, the dataset is known to have 3 different classes. As mentioned above, logistic regress is named for binary class problems. The <a class="reference external" href="https://www.kaggle.com/datasets/lucidlenn/sloan-digital-sky-survey">data</a> produced by <a class="reference external" href="https://www.kaggle.com/lucidlenn">Lennart Grosser</a> consists of <strong>10,000</strong> observations of space taken by the SDSS. Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># data load</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Skyserver_SQL2_27_2018 6_51_39 PM.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-3655a06c-210f-4fdb-9f62-397fd9f08961">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>objid</th>
      <th>ra</th>
      <th>dec</th>
      <th>u</th>
      <th>g</th>
      <th>r</th>
      <th>i</th>
      <th>z</th>
      <th>run</th>
      <th>rerun</th>
      <th>camcol</th>
      <th>field</th>
      <th>specobjid</th>
      <th>class</th>
      <th>redshift</th>
      <th>plate</th>
      <th>mjd</th>
      <th>fiberid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.237650e+18</td>
      <td>183.531326</td>
      <td>0.089693</td>
      <td>19.47406</td>
      <td>17.04240</td>
      <td>15.94699</td>
      <td>15.50342</td>
      <td>15.22531</td>
      <td>752</td>
      <td>301</td>
      <td>4</td>
      <td>267</td>
      <td>3.722360e+18</td>
      <td>STAR</td>
      <td>-0.000009</td>
      <td>3306</td>
      <td>54922</td>
      <td>491</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.237650e+18</td>
      <td>183.598370</td>
      <td>0.135285</td>
      <td>18.66280</td>
      <td>17.21449</td>
      <td>16.67637</td>
      <td>16.48922</td>
      <td>16.39150</td>
      <td>752</td>
      <td>301</td>
      <td>4</td>
      <td>267</td>
      <td>3.638140e+17</td>
      <td>STAR</td>
      <td>-0.000055</td>
      <td>323</td>
      <td>51615</td>
      <td>541</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.237650e+18</td>
      <td>183.680207</td>
      <td>0.126185</td>
      <td>19.38298</td>
      <td>18.19169</td>
      <td>17.47428</td>
      <td>17.08732</td>
      <td>16.80125</td>
      <td>752</td>
      <td>301</td>
      <td>4</td>
      <td>268</td>
      <td>3.232740e+17</td>
      <td>GALAXY</td>
      <td>0.123111</td>
      <td>287</td>
      <td>52023</td>
      <td>513</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.237650e+18</td>
      <td>183.870529</td>
      <td>0.049911</td>
      <td>17.76536</td>
      <td>16.60272</td>
      <td>16.16116</td>
      <td>15.98233</td>
      <td>15.90438</td>
      <td>752</td>
      <td>301</td>
      <td>4</td>
      <td>269</td>
      <td>3.722370e+18</td>
      <td>STAR</td>
      <td>-0.000111</td>
      <td>3306</td>
      <td>54922</td>
      <td>510</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.237650e+18</td>
      <td>183.883288</td>
      <td>0.102557</td>
      <td>17.55025</td>
      <td>16.26342</td>
      <td>16.43869</td>
      <td>16.55492</td>
      <td>16.61326</td>
      <td>752</td>
      <td>301</td>
      <td>4</td>
      <td>269</td>
      <td>3.722370e+18</td>
      <td>STAR</td>
      <td>0.000590</td>
      <td>3306</td>
      <td>54922</td>
      <td>512</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-3655a06c-210f-4fdb-9f62-397fd9f08961')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-3655a06c-210f-4fdb-9f62-397fd9f08961 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-3655a06c-210f-4fdb-9f62-397fd9f08961');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># check data analysis</span>
<span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-2f8e6747-5153-4eab-acc2-1f9dfc913e7f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>objid</th>
      <th>ra</th>
      <th>dec</th>
      <th>u</th>
      <th>g</th>
      <th>r</th>
      <th>i</th>
      <th>z</th>
      <th>run</th>
      <th>rerun</th>
      <th>camcol</th>
      <th>field</th>
      <th>specobjid</th>
      <th>redshift</th>
      <th>plate</th>
      <th>mjd</th>
      <th>fiberid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1.000000e+04</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.0</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>1.000000e+04</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
      <td>10000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.237650e+18</td>
      <td>175.529987</td>
      <td>14.836148</td>
      <td>18.619355</td>
      <td>17.371931</td>
      <td>16.840963</td>
      <td>16.583579</td>
      <td>16.422833</td>
      <td>981.034800</td>
      <td>301.0</td>
      <td>3.648700</td>
      <td>302.380100</td>
      <td>1.645022e+18</td>
      <td>0.143726</td>
      <td>1460.986400</td>
      <td>52943.533300</td>
      <td>353.069400</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.000000e+00</td>
      <td>47.783439</td>
      <td>25.212207</td>
      <td>0.828656</td>
      <td>0.945457</td>
      <td>1.067764</td>
      <td>1.141805</td>
      <td>1.203188</td>
      <td>273.305024</td>
      <td>0.0</td>
      <td>1.666183</td>
      <td>162.577763</td>
      <td>2.013998e+18</td>
      <td>0.388774</td>
      <td>1788.778371</td>
      <td>1511.150651</td>
      <td>206.298149</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.237650e+18</td>
      <td>8.235100</td>
      <td>-5.382632</td>
      <td>12.988970</td>
      <td>12.799550</td>
      <td>12.431600</td>
      <td>11.947210</td>
      <td>11.610410</td>
      <td>308.000000</td>
      <td>301.0</td>
      <td>1.000000</td>
      <td>11.000000</td>
      <td>2.995780e+17</td>
      <td>-0.004136</td>
      <td>266.000000</td>
      <td>51578.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.237650e+18</td>
      <td>157.370946</td>
      <td>-0.539035</td>
      <td>18.178035</td>
      <td>16.815100</td>
      <td>16.173333</td>
      <td>15.853705</td>
      <td>15.618285</td>
      <td>752.000000</td>
      <td>301.0</td>
      <td>2.000000</td>
      <td>184.000000</td>
      <td>3.389248e+17</td>
      <td>0.000081</td>
      <td>301.000000</td>
      <td>51900.000000</td>
      <td>186.750000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.237650e+18</td>
      <td>180.394514</td>
      <td>0.404166</td>
      <td>18.853095</td>
      <td>17.495135</td>
      <td>16.858770</td>
      <td>16.554985</td>
      <td>16.389945</td>
      <td>756.000000</td>
      <td>301.0</td>
      <td>4.000000</td>
      <td>299.000000</td>
      <td>4.966580e+17</td>
      <td>0.042591</td>
      <td>441.000000</td>
      <td>51997.000000</td>
      <td>351.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.237650e+18</td>
      <td>201.547279</td>
      <td>35.649397</td>
      <td>19.259232</td>
      <td>18.010145</td>
      <td>17.512675</td>
      <td>17.258550</td>
      <td>17.141447</td>
      <td>1331.000000</td>
      <td>301.0</td>
      <td>5.000000</td>
      <td>414.000000</td>
      <td>2.881300e+18</td>
      <td>0.092579</td>
      <td>2559.000000</td>
      <td>54468.000000</td>
      <td>510.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.237650e+18</td>
      <td>260.884382</td>
      <td>68.542265</td>
      <td>19.599900</td>
      <td>19.918970</td>
      <td>24.802040</td>
      <td>28.179630</td>
      <td>22.833060</td>
      <td>1412.000000</td>
      <td>301.0</td>
      <td>6.000000</td>
      <td>768.000000</td>
      <td>9.468830e+18</td>
      <td>5.353854</td>
      <td>8410.000000</td>
      <td>57481.000000</td>
      <td>1000.000000</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-2f8e6747-5153-4eab-acc2-1f9dfc913e7f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-2f8e6747-5153-4eab-acc2-1f9dfc913e7f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-2f8e6747-5153-4eab-acc2-1f9dfc913e7f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  </div></div>
</div>
<p>Except for the ârerunâ column, there are no missing values in the dataset. Except the âdecâ and âspaceobjidâ, the mean and medians (50 percentile) of all features are quite similar.</p>
<p>With understanding of the data spread and features, we can prepare the data needed to develop the model.</p>
<p>Since we do not have to deal with missing values except for the ârerunâ column, let us find out the number of unique values in each column.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">cols</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span><span class="s1">&#39;:&#39;</span><span class="p">,</span><span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span><span class="o">.</span><span class="n">nunique</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>objid : 1
ra : 10000
dec : 10000
u : 9730
g : 9817
r : 9852
i : 9890
z : 9896
run : 23
rerun : 1
camcol : 6
field : 703
specobjid : 6349
class : 3
redshift : 9637
plate : 487
mjd : 355
fiberid : 892
</pre></div>
</div>
</div>
</div>
<p>As provided in the data description, âclassâ is the only feature that isnât continuous which has to be predicted. While the dataset has 10000 data points, it can be noticed that there are columns like âobjidâ and ârerunâ that only have one value. These columns can be removed. The âspaceobjidâ feature is an object identifier with fairly large values. Similarly, the âplateâ feature is a number identifying the plate used in the telescope. These feature being IDs cannot be scaled and thus it can be removed as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Delete columns</span>
<span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;objid&#39;</span><span class="p">,</span> <span class="s1">&#39;rerun&#39;</span><span class="p">,</span> <span class="s1">&#39;specobjid&#39;</span><span class="p">,</span> <span class="s1">&#39;plate&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Analyze the correlation among the features in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize Pearson correlation coefficient</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="c1"># compute the coefficient and plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">fignum</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
<span class="c1"># change ticks</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">([</span><span class="s1">&#39;number&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">([</span><span class="s1">&#39;number&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">([</span><span class="s1">&#39;number&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">([</span><span class="s1">&#39;number&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">cb</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">cb</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Correlation Matrix&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-51-ee9f42d61847&gt;:4: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
  plt.matshow(data.corr(), fignum=f.number)
</pre></div>
</div>
<img alt="../../_images/49d28a2b78b5353827a05931d236e9b0ba5ccc5903f7afbf8928b13ee1bc6269.png" src="../../_images/49d28a2b78b5353827a05931d236e9b0ba5ccc5903f7afbf8928b13ee1bc6269.png" />
</div>
</div>
<p>The correlation matrix above shows that features g, r , i , z are strongly correlated with each other. While the feature u is only correlated to g, it seems independent from the others. Since we are dealing with a simple logistic regression model, we shall eliminate the correlation by removing columns g, r and i. Feature u is not highly correlated with other features and among g, r, i, z, the feature z is least correlated to u. Thus we retain these two columns among the 5 bands of the telescope.</p>
<p>Feature âmjdâ is also highly correlated with âplateâ, although they do not have any meaningful data in common. Since mjd is the âModified Julian Dateâ that identifies the date the SDSS image was taken, we can eliminate it along with the others while retaining the âplateâ feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;mjd&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now analyze all the remaining columns. We start with the features that describe a field within an image taken by the SDSS.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. &#39;run&#39; column identifies specific scan </span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;run&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>756     3060
752     2086
1345     915
1350     540
1140     527
745      453
1035     396
1412     347
1302     246
1231     245
1331     245
1334     212
1239     197
1336     182
1404     137
1045     112
1402      49
308       31
1411      10
727        4
1356       4
1332       1
1119       1
Name: run, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2. &#39;camcol&#39; column identifies scanline with &#39;run&#39; </span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;camcol&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4    1834
5    1827
6    1769
2    1712
3    1560
1    1298
Name: camcol, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Since this feature is of an integer type and only has 6 values, it can be treated as a categorical variable. Dummy variables can be created to accommodate all the factors in this features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># replace &#39;camcol&#39; with dummy varaibles</span>
<span class="n">dummy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;camcol&#39;</span><span class="p">])</span>
<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">data</span><span class="p">,</span> <span class="n">dummy_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">new_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;camcol&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">new_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-95d21ae7-2b15-409d-9df7-38b2df5f791f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ra</th>
      <th>dec</th>
      <th>u</th>
      <th>z</th>
      <th>run</th>
      <th>field</th>
      <th>class</th>
      <th>redshift</th>
      <th>fiberid</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>183.531326</td>
      <td>0.089693</td>
      <td>19.47406</td>
      <td>15.22531</td>
      <td>752</td>
      <td>267</td>
      <td>STAR</td>
      <td>-0.000009</td>
      <td>491</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>183.598370</td>
      <td>0.135285</td>
      <td>18.66280</td>
      <td>16.39150</td>
      <td>752</td>
      <td>267</td>
      <td>STAR</td>
      <td>-0.000055</td>
      <td>541</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>183.680207</td>
      <td>0.126185</td>
      <td>19.38298</td>
      <td>16.80125</td>
      <td>752</td>
      <td>268</td>
      <td>GALAXY</td>
      <td>0.123111</td>
      <td>513</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>183.870529</td>
      <td>0.049911</td>
      <td>17.76536</td>
      <td>15.90438</td>
      <td>752</td>
      <td>269</td>
      <td>STAR</td>
      <td>-0.000111</td>
      <td>510</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>183.883288</td>
      <td>0.102557</td>
      <td>17.55025</td>
      <td>16.61326</td>
      <td>752</td>
      <td>269</td>
      <td>STAR</td>
      <td>0.000590</td>
      <td>512</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-95d21ae7-2b15-409d-9df7-38b2df5f791f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-95d21ae7-2b15-409d-9df7-38b2df5f791f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-95d21ae7-2b15-409d-9df7-38b2df5f791f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Analyse class column</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GALAXY    49.98
STAR      41.52
QSO        8.50
Name: class, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Almost 50% of the dataset contains data identifying galaxies while 41.5% are stars with the remaining 8% as quasars. We shall be eliminating quasars (âQSOâ) and develop a binary classification model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create subset of data</span>
<span class="n">data1</span> <span class="o">=</span> <span class="n">new_data</span><span class="p">[</span><span class="n">new_data</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">!=</span><span class="s1">&#39;QSO&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Analyze the mean values of the features w.r.t. the âclassâ factor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Analyse the mean values of the features w.r.t. the &#39;class&#39; factor</span>
<span class="n">data1</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;class&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-5805a055-9c81-4388-84a2-ab0743a2252b">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ra</th>
      <th>dec</th>
      <th>u</th>
      <th>z</th>
      <th>run</th>
      <th>field</th>
      <th>redshift</th>
      <th>fiberid</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
    </tr>
    <tr>
      <th>class</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GALAXY</th>
      <td>177.333570</td>
      <td>15.764372</td>
      <td>18.804339</td>
      <td>16.017923</td>
      <td>996.711685</td>
      <td>300.963585</td>
      <td>0.080325</td>
      <td>340.108844</td>
      <td>0.137455</td>
      <td>0.163665</td>
      <td>0.150460</td>
      <td>0.180272</td>
      <td>0.191477</td>
      <td>0.176671</td>
    </tr>
    <tr>
      <th>STAR</th>
      <td>172.962158</td>
      <td>12.544824</td>
      <td>18.330439</td>
      <td>16.531119</td>
      <td>950.886561</td>
      <td>303.552264</td>
      <td>0.000043</td>
      <td>362.838391</td>
      <td>0.119701</td>
      <td>0.182563</td>
      <td>0.166185</td>
      <td>0.185934</td>
      <td>0.168593</td>
      <td>0.177023</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-5805a055-9c81-4388-84a2-ab0743a2252b')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-5805a055-9c81-4388-84a2-ab0743a2252b button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-5805a055-9c81-4388-84a2-ab0743a2252b');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  </div></div>
</div>
<p>While most of the feature mean values are similar for either class, there are a few with noteworthy differences. The right ascension (ra), declination (dec) and âredshiftâ have higher means for the GALAXY class, suggesting that higher values of these 3 features indicate the object to be a galaxy.</p>
</section>
<section id="developing-the-model">
<h3>5.3. Developing the Model<a class="headerlink" href="#developing-the-model" title="Link to this heading">#</a></h3>
<p>The data has been cleaned and prepared to now develop a model. In short, the logistic regression model calculates the probabilities of a data point belonging to a certain class. The model first calculates a linear regression model and then uses the sigmoid function to calculate the probability. The final step is using a decision boundary to decide which class the data point belongs when compared to the decision boundary values.</p>
<p>To create and test the model, we will need train and test data sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># import function for data split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># build input matrix X and output vector y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;class&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data1</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span>

<span class="c1"># Train and test set split</span>
<span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1551</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># check the first couple of rows of inputs for training dataset</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-948753d4-a103-45ee-bb2b-adf4d82f5bec">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ra</th>
      <th>dec</th>
      <th>u</th>
      <th>z</th>
      <th>run</th>
      <th>field</th>
      <th>redshift</th>
      <th>fiberid</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4621</th>
      <td>244.870089</td>
      <td>0.513505</td>
      <td>19.51077</td>
      <td>16.42123</td>
      <td>752</td>
      <td>677</td>
      <td>0.132896</td>
      <td>552</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7446</th>
      <td>199.748564</td>
      <td>67.118753</td>
      <td>17.57432</td>
      <td>14.69458</td>
      <td>1412</td>
      <td>207</td>
      <td>-0.000109</td>
      <td>332</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1217</th>
      <td>146.462979</td>
      <td>0.638698</td>
      <td>17.70856</td>
      <td>14.98198</td>
      <td>756</td>
      <td>205</td>
      <td>0.030323</td>
      <td>506</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2213</th>
      <td>27.837241</td>
      <td>13.106702</td>
      <td>19.03296</td>
      <td>16.73755</td>
      <td>1035</td>
      <td>146</td>
      <td>0.062437</td>
      <td>258</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6313</th>
      <td>190.379369</td>
      <td>68.460756</td>
      <td>19.58128</td>
      <td>15.76584</td>
      <td>1350</td>
      <td>382</td>
      <td>0.109400</td>
      <td>576</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-948753d4-a103-45ee-bb2b-adf4d82f5bec')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-948753d4-a103-45ee-bb2b-adf4d82f5bec button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-948753d4-a103-45ee-bb2b-adf4d82f5bec');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  </div></div>
</div>
<p>Notice the values of the features above. All columns excluding the dummy variables for the âcamcolâ feature are of varying scales. In order for the logistic regression model to calculate the distances between the points, the data should be across a similar range. Generating a model with varying data scales result in an unstable prediction. To avoid that, we use MinMax scaler (among others) to scale the features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="c1"># implement scaler on training set</span>
<span class="n">train_scale</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s1">&#39;ra&#39;</span><span class="p">,</span> <span class="s1">&#39;dec&#39;</span><span class="p">,</span> <span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="s1">&#39;run&#39;</span><span class="p">,</span> <span class="s1">&#39;field&#39;</span><span class="p">,</span> <span class="s1">&#39;redshift&#39;</span><span class="p">,</span> <span class="s1">&#39;fiberid&#39;</span><span class="p">]])</span>
<span class="n">train_trans_data</span> <span class="o">=</span> <span class="n">train_scale</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s1">&#39;ra&#39;</span><span class="p">,</span> <span class="s1">&#39;dec&#39;</span><span class="p">,</span> <span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="s1">&#39;run&#39;</span><span class="p">,</span> <span class="s1">&#39;field&#39;</span><span class="p">,</span> <span class="s1">&#39;redshift&#39;</span><span class="p">,</span> <span class="s1">&#39;fiberid&#39;</span><span class="p">]])</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">train_trans_data</span><span class="p">,</span><span class="n">X_train</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]]])</span>

<span class="c1"># scale the test set as well</span>
<span class="n">test_trans_data</span> <span class="o">=</span> <span class="n">train_scale</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="s1">&#39;ra&#39;</span><span class="p">,</span> <span class="s1">&#39;dec&#39;</span><span class="p">,</span> <span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="s1">&#39;run&#39;</span><span class="p">,</span> <span class="s1">&#39;field&#39;</span><span class="p">,</span> <span class="s1">&#39;redshift&#39;</span><span class="p">,</span> <span class="s1">&#39;fiberid&#39;</span><span class="p">]])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">test_trans_data</span><span class="p">,</span><span class="n">X_test</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]]])</span>
</pre></div>
</div>
</div>
</div>
<p>Use Label Encoder to convert the classes (âGALAXYâ, âSTARâ) to numeric values to be used in the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># implement LabelEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">lb</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>

<span class="c1"># transform on train and test output set</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Question:</strong> try to print out a couple of lines before and after <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code>, and answer what the effect of this function?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Add your solution</span>
</pre></div>
</div>
</div>
</div>
<p>Then we need to code the cost and gradient function for logistic regression. Recall the given formulas in âGLM Structureâ and complete the following codes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sigmoid function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    The sigmoid function</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        val = the given value to sigmoid</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        y = output after sigmoid</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Add your solution here</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lg_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; </span>
<span class="sd">    Function to calculate cost function for logistic regression </span>
<span class="sd">    by assuming a hypothesis of form h = theta.T*X</span>

<span class="sd">    Arguments:</span>
<span class="sd">        X = array of features</span>
<span class="sd">        y = array of training examples</span>
<span class="sd">        theta = array of parameters for hypothesis</span>

<span class="sd">    Returns:</span>
<span class="sd">        J = cost function</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Number of data points </span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Insert unit column to X feature matrix</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

    <span class="c1"># Add your solution here</span>

    <span class="k">return</span> <span class="n">J</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lg_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Function to calculate gradient for logistic regression </span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        X = array of features</span>
<span class="sd">        y = array of training examples</span>
<span class="sd">        theta = array of parameters for hypothesis</span>

<span class="sd">    Returns:</span>
<span class="sd">        grad = gradient</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Number of data points</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Insert unit column to X feature matrix</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
    
    <span class="c1"># Add your solution here</span>

    <span class="k">return</span> <span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># give a starting vector</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># optimize the logistic regression model by SGD</span>
<span class="n">traj_out</span><span class="p">,</span> <span class="n">cost_out</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">lg_gradient</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">costfunc</span><span class="o">=</span><span class="n">lg_cost</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the cost change</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cost_out</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Cost&#39;)
</pre></div>
</div>
<img alt="../../_images/c71bbe3e8ec1dd81cfec83190ca1ab31f754d1997fc84e442f2daa487642286d.png" src="../../_images/c71bbe3e8ec1dd81cfec83190ca1ab31f754d1997fc84e442f2daa487642286d.png" />
</div>
</div>
<p><strong>Question:</strong> Describe how cost changes along iterations. And can we say it is already convergent? What else we could do to validate the convergence?</p>
<hr class="docutils" />
<p><strong>Answer:</strong> The plot above shows that the error/cost function values sharply decline in the first few iterations and proceed to reduce slowly over the rest of the iterations. it can be assumed that the local minima was achieved at the end. This shows that the logistic regression model has converged with optimum values for the coefficients. Using these new values, we can predict the values for the test set and check the goodness of fit using model metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## predict on test set</span>
<span class="c1"># Add your solution here</span>


<span class="c1">## Calculate accuracy and model fit metrics</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">precision_recall_fscore_support</span>
<span class="c1"># compute the accuracy score</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># compute the confusion matrix</span>
<span class="n">conf_mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy of the model is :&quot;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="s2">&quot;%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">conf_mat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy of the model is : 96.7 %
Confusion Matrix:
 [[1196   74]
 [   1 1017]]
</pre></div>
</div>
</div>
</div>
<p>Based on the values of the confusion matrix, we can calculate the precision, recall and F-score to estimate the goodness of the model fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate the precision, recall and F-score</span>
<span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">fscore</span><span class="p">,</span> <span class="n">support</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision = &#39;</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Recall = &#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">F-Score = &#39;</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">fscore</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Precision =  0.9322 
Recall =  0.999 
F-Score =  0.9644
</pre></div>
</div>
</div>
</div>
<p>While the precision is a little low, the recall and F-score are comparatively higher. A higher recall value indicates how many of the actual true values were correctly predicted by the model and precision indicates how many of the values predicted true were, in fact, correct. A 0.9644 F-score indicates a good model fit.</p>
<p>The final test of the goodness of fit of the model is analyzing the AUC plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## plot AUC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="c1"># ROC curve</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>  <span class="n">y_prob</span><span class="p">)</span>
<span class="c1"># Calculate AUC score</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="n">tpr</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;data, auc=&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">auc</span><span class="p">,</span><span class="mi">4</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;False Positive Rate&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Positive Rate&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;ROC Curve for Model from Sratch&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/f64a23a3a2807efbda04a90408d479b0e450a9bc9353ad93098144814d37035e.png" src="../../_images/f64a23a3a2807efbda04a90408d479b0e450a9bc9353ad93098144814d37035e.png" />
</div>
</div>
<p>The AUC score for the developed model is 0.9911. With its close proximity to 1, it can be stated that the model is good at distinguishing between the two classes.</p>
</section>
<section id="using-other-package">
<h3>5.4. Using other package<a class="headerlink" href="#using-other-package" title="Link to this heading">#</a></h3>
<p>We have created the model from scratch and produced one that classifies the data well. To compare the goodness of fit of our model, we use the predefined <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> package to create a logistic regression model using stochastic gradient descent (SGD) on the same training and test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;log&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># create a classifier using logistic regression using SGD</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># fit the model on the training data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># predict values on test set</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss &#39;log&#39; was deprecated in v1.1 and will be removed in version 1.3. Use `loss=&#39;log_loss&#39;` which is equivalent.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Deteremine model accuracy and goodness of fit</span>
<span class="c1"># Calculate accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Calculate confusion matrix</span>
<span class="n">conf_mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Print out</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy of the model is :&quot;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="s2">&quot;%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">conf_mat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy of the model is : 96.0 %
Confusion Matrix:
 [[1196   74]
 [   8 1010]]
</pre></div>
</div>
</div>
</div>
<p>While the accuracy of the model is almost the same as our original model, notice the slight difference in the confusion matrix. The metrics can tell the story further.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate precision, recall, F-score</span>
<span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">fscore</span><span class="p">,</span> <span class="n">support</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision = &#39;</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Recall = &#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">F-Score = &#39;</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">fscore</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Precision =  0.9317 
Recall =  0.9921 
F-Score =  0.961
</pre></div>
</div>
</div>
</div>
<p>Here we have an almost-perfect recall with the in-built model and a relatively higher score for the precision and F-score than our model, indicating that the in-built model is better than ours.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[::,</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># get probabilities of the model prediction</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>  <span class="n">y_pred_proba</span><span class="p">)</span> <span class="c1"># calculate ROC curve</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span> <span class="c1"># calculate AUC</span>

<span class="c1"># Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="n">tpr</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;data, auc=&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">auc</span><span class="p">,</span><span class="mi">4</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;False Positive Rate&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Positive Rate&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;ROC Curve for scikit-learn Model&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8fb00f3a68a5d71e9a3cc2dd753bd88554b2c16466836f978a89da004b5309cf.png" src="../../_images/8fb00f3a68a5d71e9a3cc2dd753bd88554b2c16466836f978a89da004b5309cf.png" />
</div>
</div>
<p>The AUC score for the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> model is close to our model as well. This goes to show that our model could be as good as the in-built model for classifying data.</p>
</section>
</section>
<section id="summary">
<h2>6. Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>This notebook introduces and implements (stochastic) gradient descent methods on several examples. By parametric study and comparison with Newtonâs method, this work aims to help readers to understand:</p>
<ul class="simple">
<li><p>Stochastic gradient descent can be challenging to optimize due to the difficulty in selecting an appropriate learning rate. A learning rate that is too small leads to slow convergence, whereas a learning rate that is too large can hinder convergence and cause the loss function to oscillate or even diverge. However, the performance of SGD can be improved by using adaptive learning or momentum methods.</p></li>
<li><p>Large-scaled machine learning problems are hard to train and their loss functions are costly to compute. However, using the stochastic approach of minibatches does not take higher cost but can yield better solutions in fewer epochs. In a sense, the noise in the direction enables jumping across the trenches, whereas a full-batch approach will converge in the trench it started with.</p></li>
<li><p>Newtonâs method is faster than gradient descent in moderately difficult cases. As the dimensionality <span class="math notranslate nohighlight">\(N\)</span> increases, the number of saddle points grows exponentially, gradient descent dynamics can escape saddle points by following directions of negative curvature, leading to a lower error. However, the Newton method is not effective at handling saddle points, which can hinder the optimization process.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p><strong>(Stochastic) Gradient Descent:</strong></p>
<p>[1] Goodfellow, Ian, et al. âDeep learning, volume 1.â (2016). <a class="reference external" href="https://www.deeplearningbook.org/contents/ml.html">https://www.deeplearningbook.org/contents/ml.html</a></p>
<p>[2] Xu, Wei. âTowards optimal one pass large scale learning with averaged stochastic gradient descent.â arXiv preprint arXiv:1107.2490 (2011).</p>
<p>[3] Dauphin, Yann N., et al. âIdentifying and attacking the saddle point problem in high-dimensional non-convex optimization.â Advances in neural information processing systems 27 (2014).</p>
<p>[4] Sebastian Ruder: <a class="reference external" href="https://www.ruder.io/optimizing-gradient-descent/">https://www.ruder.io/optimizing-gradient-descent/</a></p>
<p>[5] Mirko StojiljkoviÄ: <a class="reference external" href="https://realpython.com/gradient-descent-algorithm-python/">https://realpython.com/gradient-descent-algorithm-python/</a></p>
<p>[6] Jeremy Jordan: <a class="reference external" href="https://www.jeremyjordan.me/gradient-descent/">https://www.jeremyjordan.me/gradient-descent/</a></p>
<p>[7] Raimi Karim: <a class="reference external" href="https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9">https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9</a></p>
<hr class="docutils" />
<p><strong>(Quasi-)Newtonâs Method:</strong></p>
<p>[8] Jorge, Nocedal, and J. Wright Stephen. âNumerical optimization.â (2006)</p>
<p>[9] Class website: <a class="reference external" href="https://ndcbe.github.io/optimization/notebooks/3/Newton-Methods.html">https://ndcbe.github.io/optimization/notebooks/3/Newton-Methods.html</a></p>
<p>[10] Class assignment: <a class="reference external" href="https://ndcbe.github.io/optimization/notebooks/assignments/Algorithms3.html">https://ndcbe.github.io/optimization/notebooks/assignments/Algorithms3.html</a></p>
<hr class="docutils" />
<p><strong>Case Study:</strong></p>
<p>[11] Xavier Bourret Sicotte: <a class="reference external" href="https://xavierbourretsicotte.github.io/Intro_optimization.html#Comparing-Newton-and-Gradient-Descent-in-presence-of-a-single-saddle-point">https://xavierbourretsicotte.github.io/Intro_optimization.html#Comparing-Newton-and-Gradient-Descent-in-presence-of-a-single-saddle-point</a></p>
<p>[12] Wikipedia: Test functions for optimization. <a class="reference external" href="https://en.wikipedia.org/wiki/Test_functions_for_optimization">https://en.wikipedia.org/wiki/Test_functions_for_optimization</a></p>
<hr class="docutils" />
<p><strong>Generalized Linear Model:</strong></p>
<p>[13] Daniel Friedman: <a class="reference external" href="https://towardsdatascience.com/fitting-glms-by-hand-189c02af33a8">https://towardsdatascience.com/fitting-glms-by-hand-189c02af33a8</a></p>
<p>[14] Marissa Fernandes: <a class="reference external" href="https://www.kaggle.com/code/marissafernandes/logistic-regression-sgd-in-python-from-scratch/notebook">https://www.kaggle.com/code/marissafernandes/logistic-regression-sgd-in-python-from-scratch/notebook</a></p>
<p>[15] Lennart Grosser: <a class="reference external" href="https://www.kaggle.com/datasets/lucidlenn/sloan-digital-sky-survey">https://www.kaggle.com/datasets/lucidlenn/sloan-digital-sky-survey</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/contrib"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Stochastic-Gradient-Descent-2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Stochastic Gradient Descent Tutorial 2</p>
      </div>
    </a>
    <a class="right-next"
       href="data.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Learning and Applied Statistics</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">2. Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-algorithm">2.1. Gradient Descent Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-visualization">2.2. Learning rate and visualization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization">2.2.1. Visualization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#change-learning-rate">2.2.2. Change learning rate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nonconvex-function">2.2.3. Nonconvex function</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-applications">2.3. More applications</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate">2.3.1. Univariate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bivariate">2.3.2. Bivariate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares">2.3.3. Ordinary Least Squares</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-algorithms">3. Stochastic Gradient Descent Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formulation">3.1. Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-study">3.2. Parametric study</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">3.2.1. Cost function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#change-values-of-alpha">3.2.2. Change Values of <span class="math notranslate nohighlight">\(\alpha\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#change-values-of-batch-size">3.2.3. Change Values of Batch Size</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-cost-analysis">3.3. Computational cost analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#takeaway-conclusion">3.4. Takeaway conclusion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variants-of-sgd">3.5. Variants of SGD</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-between-gd-and-newton-based-method">4. Comparison between GD and Newton-based method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-based-method">4.1. Newton-based method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rosenbrock-function">4.2. Rosenbrock function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-saddle-point">4.3. Single saddle point</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-saddle-points">4.4. Multiple saddle points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">4.5. Takeaway conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-sgd-on-glm-with-real-dataset">5. Implement SGD on GLM with real dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glm-structure">5.1. GLM Structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-wrangling">5.2. Data Wrangling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#developing-the-model">5.3. Developing the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-other-package">5.4. Using other package</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">6. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander Dowling, et al.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>